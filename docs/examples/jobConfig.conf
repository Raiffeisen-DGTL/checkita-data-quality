jobId: "sample_job"

messageBrokers: {
  kafka: [
    {
      id: "kafka1",
      servers: ["kafka-broker1:9092", "kafka-broker2:9092", "kafka-broker3:9092"]
      jaasConfigFile: "configuration/jaas.conf"
      parameters: [
        "security.protocol=SASL_PLAINTEXT",
        "sasl.mechanism=GSSAPI",
        "sasl.kerberos.service.name=kafka-service"
      ]
    }
  ]
}

databases: {
  oracle: [ 
    {id: "oracle_db1", url: "oracle.db.com:1521/public", user: "db-user", password: "db-password"}
  ]
  postgresql: [
    {id: "postgre_db1", url: "postgre1.db.com:5432/public", user: "db-user", password: "db-password"},
    {
      id: "postgre_db2",
      url: "postgre2.db.com:5432/public",
      user: "another-db-user",
      password: "another-db-password",
      schema: "some_schema"
    }
  ]
  sqlite: [
    {id: "sqlite_db", url: "/data/sqlite/some_database.db"}
  ]
}

sources: {
  kafka: [
    {
      id: "topic1",
      brokerId: "kafka1",
      topics: ["some.topic"]
      startingOffsets: """{"some.topic":{"0":35590000,"1":1234,"2":1432}}"""
      options: ["kafkaConsumer.pollTimeoutMs=300000"]
      format: "json"
    }
  ]
  table: [
    {id: "table1", database: "postgre_db1", table: "db_table1"},
    {id: "table2", database: "postgre_db1", table: "db_table2"}
  ]
  hive: [
    {id: "hive_table1", query: "select * from schema.table1"},
    {id: "hive_table2", query: "select * from schema.table2"}
  ]
  hdfs: {
    fixed: [
      {
        id: "fixed_file1",
        path: "/data/fixed_file1.txt",
        schema: [{name: "a", type: "string", length: 6}, {name: "b", type: "integer", length: 5}]
      },
      {id: "fixed_file2", path: "/data/fixed_file2.txt", shortSchema: ["id:6", "name:12", "value:8"]}
    ]
    delimited: [
      {id: "csv_file1", path: "/data/delimited_file1.csv", header: true}
      {
        id: "csv_file2",
        path: "/data/delimited_file2.csv",
        delimiter: ":",
        quote: "'",
        escape: "|",
        header: false,
        schema: [{name: "a", type: "string"}, {name: "b", type: "integer"}, {name: "c", type: "string"}]
      }
    ]
    avro: [
      {id: "avro_file1", path: "/data/some_file.avro", schema: "/data/some_schema.avsc"}
    ]
    parquet: [
      {id: "parquet_file", path: "/data/some_file.parquet"}
    ]
    orc: [
      {id: "orc_data", path: "/data/some_folder_with_orc_files/"}
    ]
  }
}

virtualSources: {
  filterSql: [
    {
      id: "vSource1",
      parentSources: ["table1"],
      sql: "select distinct client_name as name from table1 where client_id is null",
      persist: "MEMORY_AND_DISK"
    }
  ]
  joinSql: [
    {
      id: "vSource2",
      parentSources: ["table1", "table2"],
      sql: "select * from table1 left join table2 on client_name=supplier_name",
      save: {path: "/some/path", fileFormat: "orc"}
    }
  ]
  join: [
    {id: "vSource3", parentSources: ["hive_table1", "hive_table2"], joinColumns: ["ACCOUNT","CURRENCY"], joinType: "ider"}
  ]
}

loadChecks: {
  exist: [
    {id: "loadCheck1", source: "fixed_file1", option: true}
  ]
  encoding: [
    {id: "loadCheck2", source: "csv_file1", option: "UTF-8"}
  ]
  fileType: [
    {id: "loadCheck3", source: "avro_file1", option: "avro"},
    {id: "loadCheck4", source: "csv_file2", option: "delimited"}
  ]
  exactColumnNum: [
    {id: "loadCheck5", source: "csv_file2", option: 3}
  ]
  minColumnNum: [
    {id: "loadCheck6", source: "fixed_file2", option: 2}
  ]
}

metrics: {
  file: {
    rowCount: [
      {id: "hive_table1_row_cnt", description: "Row count in hive_table1", source: "hive_table1"},
      {id: "csv_file1_row_cnt", description: "Row count in csv_file1", source: "csv_file1"}
    ]
  }
  column: {
    distinctValues: [
      {
        id: "fixed_file1_dist_name", description: "Distinct names in fiexed_file1",
        source: "fixed_file1", columns: ["name"]
      }
    ]
    nullValues: [
      {id: "hive_table1_nulls", description: "Null values in columns id and name", source: "hive_table1", columns: ["id", "name"]}
    ]
    completeness: [
      {id: "orc_data_compl", description: "Completness of column id", source: "orc_data", columns: ["id"]}
    ]
    avgNumber: [
      {id: "avro_file1_avg_bal", description: "Avg number of column balance", source: "avro_file1", columns: ["balance"]}
    ]
    regexMatch: [
      {
        id: "parquet_file_id_regex", description: "Regex match for id column", source: "parquet_file",
        columns: ["id"], params: {regex: """^\d{10}$"""}
      }
    ]
    stringInDomain: [
      {
        id: "orc_data_currency_domain", description: "Currencies should be in domain", source: "orc_data",
        columns: ["currency"], params: {domain: ["USD", "EUR", "RUB", "CNY", "SGD", "JPY"]}
      }
    ]
    topN: [
      {
        id: "table1_top3_currency", description: "Top 3 currency in table1", source: "table1",
        columns: ["currency"], params: {targetNumber: 3, maxCapacity: 10}
      }
    ]
  }
  composed: [
    {
      id: "pct_of_null", description: "Percent of null values in hive_table1",
      formula: "100 * $hive_table1_nulls / $hive_table1_row_cnt"
    }
  ]
}

checks: {
  trend: {
    averageBoundFullCheck: [
      {
        id: "avg_bal_check",
        description: "Check that average balance stays within +/-25% of the week average"
        metric: "avro_file1_avg_bal",
        rule: "date"
        timeWindow: 8,
        threshold: 0.25
      }
    ]
    averageBoundUpperCheck: [
      {id: "avg_pct_null", metric: "pct_of_null", rule: "date", timeWindow: 15, threshold: 0.5}
    ]
    averageBoundLowerCheck: [
      {id: "avg_distinct", metric: "fixed_file1_dist_name", rule: "record", timeWindow: 31, threshold: 0.3}
    ]
    averageBoundRangeCheck: [
      {
        id: "avg_id_match",
        metric: "parquet_file_id_regex",
        rule: "date",
        timeWindow: 8,
        thresholdLower: 0.2
        thresholdUpper: 0.4
      }
    ]
    topNRankCheck: [
      {id: "top2_curr_match", metric: "table1_top3_currency", rule: "record", timeWindow: 2, targetNumber: 2, threshold: 0.1}
    ]
  }
  snapshot: {
    differByLT: [
      {
        id: "row_cnt_diff",
        description: "Number of rows in two tables should not differ on more than 5%.",
        metric: "hive_table1_row_cnt"
        compareMetric: "csv_file1_row_cnt"
        threshold: 0.05
      }
    ]
    equalTo: [
      {id: "zero_nulls", description: "Hive Table1 mustn't contain nulls", metric: "hive_table1_nulls", threshold: 0}
    ]
    greaterThan: [
      {id: "completeness_check", metric: "orc_data_compl", threshold: 0.99}
    ]
    lessThan: [
      {id: "null_threshold", metric: "pct_of_null", threshold: 0.01}
    ]
  }
  sql: {
    countEqZero: [
      {id: "NaN_names", source: "table1", query: "select count(1) from table1 where name = 'NaN'"}
    ]
  }
}

targets: {
  hive: {
    columnMetrics: {schema: "checkita_backup", table: "DQ_COLUMNAR_METRICS", partitionColumn: "load_date"},
    fileMetrics: {schema: "checkita_backup", table: "DQ_FILE_METRICS", partitionColumn: "load_date"},
    composedMetrics: {schema: "checkita_backup", table: "DQ_COMPOSED_METRICS", partitionColumn: "load_date"}
  }
  hdfs: {
    checks: {
      fileFormat: "csv"
      path: "/tmp/checkita/results"
      delimiter: ":",
      quote: "'",
      escape: "|",
    },
    loadChecks: {fileFormat: "orc", path: "/tmp/checkita/results"}
  }
  results: {
    kafka: {
      results: ["columnMetrics", "fileMetrics", "composedMetrics", "loadChecks", "checks"],
      brokerId: "kafka1"
      topic: "some.topic"
    }
  }
  errorCollection: {
    hdfs: {
      dumpSize: 100
      fileFormat: "csv"
      path: "tmp/checkita/errors"
      delimiter: ";"
      quote: "\""
      escape: "\\"
      quoted: false
    }
    kafka: {
      metrics: ["hive_table1_nulls", "fixed_file1_dist_name", "parquet_file_id_regex"]
      dumpSize: 25
      brokerId: "kafka1"
      topic: "some.topic"
    }
  }
  summary: {
    email: {
      attachMetricErrors: true
      metrics: ["hive_table1_nulls", "fixed_file1_dist_name", "parquet_file_id_regex"]
      dumpSize: 10
      mailingList: ["some.person@some.damin"]
    }
    mattermost: {
      attachMetricErrors: true
      metrics: ["hive_table1_nulls", "fixed_file1_dist_name", "parquet_file_id_regex"]
      dumpSize: 10
      recipients: ["@someUser", "#someChannel"]
    }
    kafka: {
      brokerId: "kafka1"
      topic: "some.topic"
    }
  }
  checkAlerts: {
    email: [
      {
        id: "alert1"
        checks: ["avg_bal_check", "zero_nulls"]
        mailingList: ["some.peron@some.domain"]
      }
      {
        id: "alert2"
        checks: ["top2_curr_match", "completeness_check"]
        mailingList: ["some.peron@some.domain", "another.person@some.domain"]
      }
    ]
    mattermost: [
      {
        id: "alert3"
        checks: ["avg_bal_check", "zero_nulls"]
        recipients: ["@someUser"]
      }
      {
        id: "alert4"
        checks: ["top2_curr_match", "completeness_check"]
        recipients: ["#someChannel"]
      }
    ]
    kafka: [
      {
        id: "alert5"
        checks: ["avg_bal_check", "zero_nulls"]
        brokerId: "kafka1"
        topic: "some.topic"
      }
      {
        id: "alert6"
        checks: ["top2_curr_match", "completeness_check"]
        brokerId: "kafka1"
        topic: "some.topic"
      }
    ]
  }
}