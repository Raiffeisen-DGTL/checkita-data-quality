# Конфигурация подключений (Connections)

Фреймворк Checkita позволяет создавать читать данные из внешних систем, таких как реляционные СУБД или брокеры
сообщений (Kafka). Для того чтобы читать данные из внешних систем, нужно настроить подключение к ним.

Так, конфигурации подключений к внешним системам описываются в разделе `connections` файла с настройками Data Quality
пайплайна. На текущий момент поддерживаются подключения к следующим системам:

* Подключения к СУБД посредством JDBC драйвера:
    * PostgreSQL (также этот тип подключения может быть использован для чтения данных из GreenPlum);
    * Oracle
    * SQLite
* Подключения к брокерам сообщений:
    * Kafka

Все подключения должны иметь уникальный идентификатор `id`, а также могут иметь опциональный список Spark-параметров.
Эти параметры указываются в поле `parameters` и используются для предоставления дополнительных параметров подключения
которые используются Spark'ом, чтобы читать данные из данной системы.

Конфигурация всех подключений описывается следующими общими параметрами:

* `id` - Идентификатор подключения;
* `description` - Опциональное описание подключения;
* `parameters` - Опциональный список Spark-параметров, которые могут быть указаны, чтобы обеспечить дополнительную
  конфигурацию подключения как этого требует Spark.
* `metadata` - Опциональный список произвольных параметров, определяемых пользователем.

Специфичные параметры указаны ниже отдельно для каждого из поддерживаемых типов подключений.

Пример заполненного раздела `connections` представлен ниже в главе [Пример Описания Подключений](#_1).

## Конфигурация подключения к SQLite

Настройка подключения к БД SQLite достаточно проста. Достаточно указать только два параметра:

* `id` - *Обязательно*. ID подключения;
* `description` - *Опционально*. Описание подключения;
* `url` - *Обязательно*. Путь до файла с базой SQLite.
* `parameters` - *Опционально*. Список Spark-параметров (если требуется), где каждый параметр - это строка в формате:
  `spark.param.name=spark.param.value`.
* `metadata` - *Опционально*. Произвольный список параметров, определяемых пользователем для этого подключения
  в формате: `param.name=param.value`.

## Конфигурация подключения к PostgreSQL

Настройка подключения к PostgreSQL может быть описана с помощью следующих параметров:

* `id` - *Обязательно*. ID подключения;
* `description` - *Опционально*. Описание подключения;
* `url` - *Обязательно*. URL для подключения к серверу PostgreSQL. URL должен содержать адрес сервера, порт и имя БД
  для подключения. Дополнительные параметры также могут быть указаны в URL, если требуется, в соответствии с 
  документацией PostgreSQL. *URL не должен содержать протокол подключения.*
* `username` - *Опционально*. Имя пользователя для подключения к базе данных (если требуется).
* `password` - *Опционально*. Пароль для подключения к базе данных (если требуется).
* `parameters` - *Опционально*. Список Spark-параметров (если требуется), где каждый параметр - это строка в формате:
  `spark.param.name=spark.param.value`.
* `metadata` - *Опционально*. Произвольный список параметров, определяемых пользователем для этого подключения
  в формате: `param.name=param.value`.

## Конфигурация подключения к Oracle

Настройка подключения к Oracle описывается так же как и для PostgreSQL, с помощью следующих параметров:

* `id` - *Обязательно*. ID подключения;
* `description` - *Опционально*. Описание подключения;
* `url` - *Обязательно*. URL для подключения к серверу Oracle. URL должен содержать адрес сервера, порт и имя БД
  для подключения. Дополнительные параметры также могут быть указаны в URL, если требуется, в соответствии с
  документацией Oracle. *URL не должен содержать протокол подключения.*
* `username` - *Опционально*. Имя пользователя для подключения к базе данных (если требуется).
* `password` - *Опционально*. Пароль для подключения к базе данных (если требуется).
* `parameters` - *Опционально*. Список Spark-параметров (если требуется), где каждый параметр - это строка в формате:
  `spark.param.name=spark.param.value`.
* `metadata` - *Опционально*. Произвольный список параметров, определяемых пользователем для этого подключения
  в формате: `param.name=param.value`.

##  Конфигурация подключения к Kafka

Для того чтобы настроить подключение к кластеру Kafka, необходимо указать следующие параметры:

* `id` - *Обязательно*. ID подключения;
* `description` - *Опционально*. Описание подключения;
* `servers` - *Обязательно*. Список серверов (брокеров сообщений) для подключения.
* `parameters` - *Optional*. Список Spark-параметров (если требуется), где каждый параметр - это строка в формате:
  `spark.param.name=spark.param.value`. Обычно, настройки авторизации в Kafka предоставляются как список
  Spark-параметров.
* `metadata` - *Опционально*. Произвольный список параметров, определяемых пользователем для этого подключения
  в формате: `param.name=param.value`.

Если подключение к кластеру Kafka требует предоставления конфигурационного файла JAAS, то его расположение должно быть
указано в переменных окружения Java. Важно заметить, что эти переменные должны быть установлены до того, как JVM
будет запущена. Поэтому они должны быть определены в команде `spark-submit` как указано ниже:

* Если приложение запускается в `cluster` режиме:
  ```bash
  --deploy-mode cluster \
  --conf 'spark.driver.extraJavaOptions="-Djava.security.auth.login.config=./jaas.conf"' \
  --conf 'spark.executor.extraJavaOptions="-Djava.security.auth.login.config=./jaas.conf"' \
  --files /path/to/your/jaas.conf,<other files required for DQ>
  ```
* Если приложение запускается в `client` режиме, то JVM на клиенте (драйвер) стартует до того, как считываются
  конфигурации Spark-приложения. Поэтому, переменные окружения Java для драйвера должны быть заданы посредством
  аргумента `--driver-java-options`:
  ```bash
  --deploy-mode client \
  --driver-java-options "-Djava.security.auth.login.config=.jaas.conf" \
  --conf 'spark.executor.extraJavaOptions="-Djava.security.auth.login.config=./jaas.conf"' \
  --files file.keytab,jaas.conf,<other files required for DQ>
  ```

## Пример Описания Подключений

Как показано в примере ниже, подключения одного типа сгруппированы в подразделы, которые именуются в соответствии с
типом подключения. Эти разделы содержат список подключений только данного типа.

```hocon
jobConfig: {
  connections: {
    postgres: [
      {id: "postgre_db1", url: "postgre1.db.com:5432/public", username: "dq-user", password: "dq-password"}
      {
        id: "postgre_db2",
        url: "postgre2.db.com:5432/public",
        username: "dq-user",
        password: "dq-password",
        schema: "dataquality"
      }
    ]
    oracle: [
      {id: "oracle_db1", url: "oracle.db.com:1521/public", username: "db-user", password: "dq-password"}
    ]
    sqlite: [
      {id: "sqlite_db", url: "some/path/to/db.sqlite"}
    ],
    kafka: [
      {id: "kafka_cluster_1", servers: ["server1:9092", "server2:9092"]}
      {
        id: "kafka_cluster_2",
        servers: ["kafka-broker1:9092", "kafka-broker2:9092", "kafka-broker3:9092"]
        parameters: [
          "security.protocol=SASL_PLAINTEXT",
          "sasl.mechanism=GSSAPI",
          "sasl.kerberos.service.name=kafka-service"
        ]
      }
    ]
  }
}
```