# Sources

Sources - раздел конфигурационного файла, посвященный описанию источников данных.
Это первый этап на пути описания пайплайна расчета.

На текущий момент поддерживаются следующие типы источников:

* **table**: Таблицы из реляционных БД. Таблица будет загружена целиком,
  но есть возможность изменить ее создав виртуальный источник посредством
  SQL запроса (см. [Virtual Sources](VirtualSources.md)). Для загрузки таблицы необходимо задать
  параметры подключения к БД в разделе [Databases](Databases.md).
* **hive**: Hive таблица загруженная посредством SQL запроса с помощь Spark SQL API.
* **kafka**: Источники, сформированные на основе сообщений из топиков Kafka.
* **hdfs**: Файлы загруженные из HDFS. Поддерживаются следующие типы фалов:
    * ***fixed***: текстовые файлы без разделителя с фиксированным количеством символов на колонку;
    * ***delimited***: текстовые файлы с разделителем;
    * ***avro***: файлы .avro с поддержкой внешних схем в .avsc файлах;
    * ***parquet***: файлы .parquet;
    * ***orc***: файлы .orc.
    * ***delta***: чтение файлов delta-tables от Databricks

Раздел **sources** в конфигурационном файле выглядит следующим образом:

```hocon
sources: {
  table: [ // список источников на основе таблиц БД
  ]
  hive: [ // список источников на основе Hive таблиц
  ]
  kafka: [ // список источников на основе топиков Kafka
  ]
  hdfs: { // hdfs источники сгруппированные по под-типам
    fixed: [ // список текстовых файлов без разделителя с фиксированным количеством символов на колонку
    ]
    delimited: [ // список текстовых файлов с разделителем
    ]
    avro: [ // список .avro файлов c указанием схемы .avsc
    ]
    parquet: [ // список .parquet файлов
    ]
    orc: [ // список .orc файлов
    ]
  }
}
```

## Table

Для чтения данных из таблицы БД нужно указать следующие параметры (***все из них обязательны***):

* `id` - идентификатор источника
* `database` - идентификатор БД из раздела [databases](Databases.md)
* `table` - имя таблицы (схема указывается в параметрах подключения к БД, при необходимости)
* `keyFields [optional]` - список колонок-ключей, которые идентифицируют строку в источнике.
  Используются для составления отчетов с ошибками при вычислении метрик.

Пример:

```hocon
table: [
  {id: "table1", database: "postgre_db1", table: "db_table1", keyFields: ["id"]}
]
```

## Hive

Для чтения Hive таблиц необходимо указать идентификатор источника и HQL запрос (обязательные параметры).

* `id` - идентификтор источника
* `query` - HQL запрос
* `keyFields [optional]` - список колонок-ключей, которые идентифицируют строку в источнике.
  Используются для составления отчетов с ошибками при вычислении метрик.

Пример:

```hocon
hive: [
  {id: "hive_table1", query: "select * from schema.table1", keyFields: ["cnum", "deal_id"]},
  {id: "hive_table2", query: "select * from schema.table2"}
]
```

## Kafka

Для чтения данных из топиков Kafka необходимо указать следующие параметры:

* `id` - идентификатор источника
* `brokerId` - ID брокера Kafka, описанного в разделе [messageBrokers](MessageBrokers.md).
* `topics [optional]` - список топиков для чтения. Можно указывать топики в двух форматах:
    * Топики без указания партиций (читать из всех партиций): `["topic1", "topic2"]`
    * Топики с указанием партиций для чтения: `["topic1@[0, 1]", "topic2@[2, 4]"]`
    * **Все топики должны быть указаны в одном из двух форматов.**
* `topicPattern [optional]`
  > **Важно:** Топики для чтения должны быть указаны в одном из вышеуказанных форматов:
  > либо в `topics`, либо в `topicPattern`.
* `startingOffsets [optional]` - Json-строка с указанием стартовых оффсетов для чтения.
  Default: `earliest` (читаем весь топик)
* `endingOffsets [optional]` - Json-строка с указанием конечных оффсетов для чтения.
  Default: `latest` (читаем топик до конца)
  > **Формат для указания оффсетов:** Json-строка следующего формата (задаем в тройных кавычках):
  >
  >`"""{"topic1":{"0":1234,"1":2345},"topic2":{"0":3456,"1":4567}}"""`
* `format` - формат сообщения в топике.
  > На данный момент поддерживаются только два формата: `xml` и `json`.
* `options [optional]` - дополнительные параметры чтения из следующего списка
  (см. [Spark Kafka Integration Guide](https://spark.apache.org/docs/2.3.2/structured-streaming-kafka-integration.html)):
    * `failOnDataLoss, kafkaConsumer.pollTimeoutMs, fetchOffset.numRetries, fetchOffset.retryIntervalMs, maxOffsetsPerTrigger`
* `keyFields [optional]` - список колонок-ключей, которые идентифицируют строку в источнике.
  Используются для составления отчетов с ошибками при вычислении метрик.

Пример:
```hocon
  kafka: [
  {
    id: "topic1",
    brokerId: "kafka1",
    topics: ["some.topic"]
    startingOffsets: """{"some.topic":{"0":35590000,"1":1234,"2":1432}}"""
    options: ["kafkaConsumer.pollTimeoutMs=300000"]
    format: "json"
  }
]
```

## HDFS

Для чтения данных из HDFS, вне зависимости от типа файла, обязательно указывается идентификатор источника
и путь до файла/папки с файлами:

* `id` - идентификатор источника;
* `path` - путь до источника в HDFS.
* `keyFields [optional]` - список колонок-ключей, которые идентифицируют строку в источнике.
  Используются для составления отчетов с ошибками при вычислении метрик.

### Fixed

Для чтения текстовых файлов без разделителя, нужно обязательно указать схему данных
в одном из следующих форматов:

* `schema` - список колонок описанных словарем со следующими ключами:
    * `name` - имя колонки;
    * `type` - тип колонки;
    * `length` - ширина колонки (количество символов);
* `shortSchema` - список колонок в формате "name:size". Все колонки будут иметь текстовый тип данных.

Пример:

```hocon
fixed: [
  {
    id: "fiexed_file1",
    path: "/data/fixed_file1.txt",
    schema: [{name: "a", type: "string", length: 6}, {name: "b", type: "integer", length: 5}]
  },
  {id: "fixed_file2", path: "/data/fixed_file2.txt", shortSchema: ["id:6", "name:12", "value:8"], keyFields: ["id"]}
]
```

### Delimited

Для чтения текстовых файлов с разделителем необходимо дополнительно указать откуда
считывается схема данных (из заголовка файла или указана в конфигурационном файле):

* `header [optional]` - считываем (`true`) или нет (`false`) имена колонок из заголовка файла. По умолчанию: `false`
* `schema` - список колонок описанных словарем с ключами `name` (имя колонки) и `type` (тип колонки)
* `keyFields [optional]` - список колонок-ключей, которые идентифицируют строку в источнике.
  Используются для составления отчетов с ошибками при вычислении метрик.

Если параметр `header` отсутствует или `false`, то обязательно наличие схемы (`schema`).
Если `header = true`, то схема должна отсутствовать.

Дополнительные параметры (все опциональные):

* `delimiter` - разделитель (по умолчанию: `,`)
* `quote` - обрамляющие кавычки (по умолчанию `"`)
* `escape` - символ экранирования (по умолчанию `` \ ``)

Пример:

```hocon
delimited: [
  {id: "csv_file1", path: "/data/delimited_file1.csv", header: true}
  {
    id: "csv_file2",
    path: "/data/delimited_file2.csv",
    delimiter: ":",
    quote: "'",
    escape: "|",
    header: false,
    schema: [{name: "a", type: "string"}, {name: "b", type: "integer"}, {name: "c", type: "string"}],
    keyFields: ["a", "b"]
  }
]
```

> **Поддерживаемые типы данных в схемах текстовых файлов:**
>
> * string
> * boolean
> * date
> * timestamp
> * integer (32-bit integer)
> * long (64-bit integer)
> * short (16-bit integer)
> * byte (signed integer in a single byte)
> * double
> * float
> * decimal(precision, scale) _(precision <= 38; scale <= precision)_

### Avro

Обязательные параметры только `id` и `path`. Дополнительно можно указать путь до файла
со схемой данных (.avsc), используя параметр `schema [optional]`. Также можно указать список
колонок-ключей: `keyFields [optional]`, которые идентифицируют строку в источнике и будут
использоваться для составления отчетов с ошибками при вычислении метрик.

Пример:

```hocon
avro: [
  {id: "avro_file1", path: "/data/some_file.avro", schema: "/data/some_schema.avsc", keyFields: ["id", "name"]}
]
```

### Parquet

Дополнительно можно указать список колонок-ключей: `keyFields [optional]`, которые идентифицируют строку в источнике
и будут использоваться для составления отчетов с ошибками при вычислении метрик.

Пример:

```hocon
parquet: [
  {id: "parquet_file", path: "/data/some_file.parquet", keyFields: ["id", "name"]}
]
```

### Orc

Дополнительно можно указать список колонок-ключей: `keyFields [optional]`, которые идентифицируют строку в источнике
и будут использоваться для составления отчетов с ошибками при вычислении метрик.

Пример:

```hocon
orc: [
  {id: "orc_data", path: "/data/some_folder_with_orc_files/", keyFields: ["id", "name"]}
]
```

### Delta

Дополнительно можно указать список колонок-ключей: `keyFields [optional]`, которые идентифицируют строку в источнике
и будут использоваться для составления отчетов с ошибками при вычислении метрик.

Пример:

```hocon
delta: [
  {id: "delta_file", path: "/data/some_folder_with_orc_files/", keyFields: ["id", "name"]}
]
```