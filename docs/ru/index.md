# Checkita Data Quality Framework

**Latest Release: 0.3.0**

---

Для обеспечения качества данных, необходимо выполнять расчеты большого количества метрик и проверок над большими
данными, что в свою очередь является сложной задачей. 

**Checkita** - это Data Quality Framework, который решает эту задачу, позволяя формализовать и упростить процесс
подключения и чтения данных из различных источников, описания метрик и проверок над данными в этих источниках,
а также отправку результатов и уведомлений по различным каналам.

> Данный фреймворк является глубоко переработанным форком фреймворка 
> [Agile Lab Data Quality](https://github.com/agile-lab-dev/DataQuality).

Итак, **Checkita** позволяет выполнять расчет различных метрик и проверок над данными (как структурированными,
так и неструктурированными). Фреймворк способен выполнять распределенные вычисления над данными за "один проход",
используя Spark в качестве вычислительного ядра. В качестве описания пайплайна используются Hocon конфигурационные
файлы, а результаты расчетов сохраняются как в выделенную базу фреймворка, а также могут быть отправлены пользователям
по различным каналам: HDFS, Email, Mattermost, Kafka.

Использование Spark в качестве вычислительного ядра позволяет выполнять расчеты метрик и проверок
на уровне "сырых" данных, не требуя каких-либо SQL абстракций над данными (таких, как Hive или Impala),
которые в свою очередь могут скрывать некоторые ошибки в данных
(по типу плохого форматирования или несоответствия схемы).

**Checkita** позволяет выполнять следующее:

* Читать данные из различных источников (HDFS, Hive, Jdbc, Kafka) и в различных форматах (text, orc, parquet, avro).
* Приметь SQL запросы к данным, таким образом формируя производные "виртуальные источники" данных.
  Данный функционал осуществляется посредством Spark DataFrame API.
* Выполнять расчет широкого спектра метрик над данными, а также выполнять композицию метрик.
* Проводить проверки над данными на основе рассчитанных метрик.
* Выполнять проверки основанные на предыдущих результатах расчета (обнаружение аномалий в данных).
* Сохранять результаты расчета в базу данных фреймворка, а отправлять их по другим каналам
  (HDFS, Hive, Kafka, Email, Mattermost).

## Prerequisites

Фреймворк написан на Scala 2.12 и использует Spark 2.4+ в качестве вычислительного ядра.
В проекте настроена параметризуемая сборка, которая позволяет собирать фреймворк под определенную версию Spark,
публиковать проект в заданный репозиторий, а также собирать Uber-jar, как с зависимостями Spark, так и без них.
