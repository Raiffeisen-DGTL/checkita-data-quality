# Home

**Актуальная версия: 1.7.0**

> Документация на русском языке находится в стадии разработки. Пожалуйста, пользуйтесь документацией на английском.

Для обеспечения качества больших данных, необходимо выполнять расчеты большого количества метрик и проверок
над огромными датасетами, что в свою очередь является сложной задачей.

Checkita - это Data Quality фреймворк, который решает эту задачу, позволяя формализовать и упростить процесс
подключения и чтения данных из различных источников, описания метрик и проверок над данным в этих источниках,
а также отправку результатов и уведомлений по различным каналам.

Итак, **Checkita** позволяет выполнять расчет различных метрик и проверок над данными (как структурированными,
так и неструктурированными). Фреймворк способен выполнять распределенные вычисления над данными за "один проход",
используя Spark в качестве вычислительного ядра. Конфигурационные Hocon файлы используются как для описания
настроек приложения и, так и для описания пайплайна вычисления метрик и проверок. Результаты расчетов сохраняются в
выделенную базу фреймворка, а также могут быть отправлены пользователям по различным каналам:
файл (локальная FS, HDFS, S3), Email, Mattermost, Kafka.

Использование Spark в качестве вычислительного ядра позволяет выполнять расчеты метрик и проверок
на уровне "сырых" данных, не требуя каких-либо SQL абстракций над данными (таких, как Hive или Impala),
которые в свою очередь могут скрывать некоторые ошибки в данных
(например, плохое форматирования или несоответствия схемы).

Checkita позволяет выполнять следующее:

* Читать данные из различных источников (HDFS, S3, Hive, Jdbc, Kafka) и в различных форматах (text, orc, parquet, avro).
* Применять SQL запросы к данным, таким образом формируя производные "виртуальные источники" данных.
  Данный функционал осуществляется посредством Spark DataFrame API.
* Выполнять расчет широкого спектра метрик над данными, а также выполнять композицию метрик.
* Проводить проверки над данными на основе рассчитанных метрик.
* Выполнять проверки основанные на предыдущих результатах расчета (обнаружение аномалий в данных).
* Сохранять результаты расчета в базу данных фреймворка, а отправлять их по другим каналам
  (HDFS, S3, Hive, Kafka, Email, Mattermost).

Checkita разрабатывается с фокусом на интеграцию в ETL пайплайны и системы каталогов данных:

* Фреймворк использует Spark и может быть запущен как обычное Spark-приложение. Spark, в свою очередь,
  является наиболее широко распространенным решением для распределенной обработки больших данных.
* No-code конфигурация пайплайнов посредством Hocon файлов, которые могут быть легко составлены и версионированы в VSC.
* Выделенная база данных для хранения результатов может быть использована для последующего представления
  этих результатов посредством деш-бордов или простых UI.
* Встроенная поддержка нотификаций (Email, Mattermost) позволяет информировать пользователей
  о проблемах с качеством данных.
* Альтернативные каналы отправки результатов, такие как Kafka, могут быть использованы для интеграции с другими сервисами.

Еще одной ключевой особенностью фреймворка Checkita является возможность обрабатывать как статичные (batch), так и
потоковые источники данных. Так, поддерживается запуск двух типов приложений: для пакетной и потоковой проверки
источников данных. ***Потоковый режим работы фреймворка в данных момент находится в экспериментальной стадии, 
и поэтому в этой части работы фреймворка возможны изменения.***

Фреймворк написан на Scala 2.12 и использует Spark 2.4+ в качестве вычислительного ядра.
В проекте настроена параметризуемая сборка, которая позволяет собирать фреймворк под определенную версию Spark,
публиковать проект в заданный репозиторий, а также собирать Uber-jar, как с зависимостями Spark, так и без них.

**Лицензия**

Фреймворк Checkita распространяется под лицензией [GNU LGPL](../LICENSE.txt).

---

Данный проект - это переосмысление [Data Quality фреймворка](https://github.com/agile-lab-dev/DataQuality) 
разработанного компанией Agile Lab, Италия.
