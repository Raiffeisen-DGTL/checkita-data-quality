{"config":{"lang":["en","ru"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"changelog/CHANGELOG/","title":"1.0.0 (2023-11-01)","text":""},{"location":"changelog/CHANGELOG/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>fixes related to project build for different version of Spark (#10) (c7218bd), closes #7</li> </ul>"},{"location":"changelog/CHANGELOG/#features","title":"Features","text":"<ul> <li>added template support for email subject (#9) (9246f7f), closes #6</li> <li>adding duplicateValues metric (#8) (da77daa), closes #5</li> <li>Incorporating latest updates (1fee747)</li> <li>Publishing major update of Checkita DQ (09af3a2), closes #3</li> </ul>"},{"location":"changelog/CHANGELOG/#035-2023-09-19","title":"0.3.5 (2023-09-19)","text":""},{"location":"changelog/CHANGELOG/#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>Ensure that JDBC connection is alive prior saving results.   There could be situation when Application runs quite long and DB server can close idle connection due to   inactivity. In such cases it is required to open it again before saving results.</li> </ul>"},{"location":"changelog/CHANGELOG/#034-2023-09-14","title":"0.3.4 (2023-09-14)","text":""},{"location":"changelog/CHANGELOG/#features_1","title":"Features","text":"<ul> <li>Added support of customisable email subject templates.</li> </ul>"},{"location":"changelog/CHANGELOG/#bug-fixes_2","title":"Bug Fixes","text":"<ul> <li>Fixed duplicateValues metric: some duplicate values can only be determined during metric calculator merge.</li> <li>Fix sender name for check Alert. It was hardcoded but need to refer the name configured in application configuration file.</li> <li>Fixed email encoding: changed to UTF-8</li> </ul>"},{"location":"changelog/CHANGELOG/#033-2023-09-04","title":"0.3.3 (2023-09-04)","text":""},{"location":"changelog/CHANGELOG/#features_2","title":"Features","text":"<ul> <li>Added new metric: duplicateValues.</li> <li>Enhanced joinSql virtual source by allowing to supply it with arbitrary number of parent sources.</li> <li>Enhanced table source by allowing to supply it with query to execute (on the DB side) and thus, read only query results.</li> <li>Docs update</li> </ul>"},{"location":"changelog/CHANGELOG/#032-2023-08-25","title":"0.3.2 (2023-08-25)","text":""},{"location":"changelog/CHANGELOG/#features_3","title":"Features","text":"<ul> <li>Added email sender name customization</li> <li>Added functionality to provide html and markdown templates to build body of check alerts and    summary report messages when sending them to either email or Mattermost.</li> <li>Added custom source</li> <li>Docs update</li> </ul>"},{"location":"changelog/CHANGELOG/#bug-fixes_3","title":"Bug Fixes","text":"<ul> <li>Pass both source and virtual sources to target processors</li> <li>Fix json serialization bugs.</li> </ul>"},{"location":"changelog/CHANGELOG/#031-2023-08-17","title":"0.3.1 (2023-08-17)","text":""},{"location":"changelog/CHANGELOG/#bug-fixes_4","title":"Bug Fixes","text":"<ul> <li>Fix params json when sending results to kafka</li> <li>Add sourceId and sourceKeyFields to errorCollection reports (or kafka messages)</li> <li>Fix CSV headers for checkAlert attachments</li> <li>Fix column metric result representation when saving to DB (sourceId and metric description were mixed)</li> <li>Add MD5 hash to message key when sending results to Kafka in order to ensure idempotent message consumption.</li> </ul>"},{"location":"changelog/CHANGELOG/#030-2023-07-31","title":"0.3.0 (2023-07-31)","text":""},{"location":"changelog/CHANGELOG/#features_4","title":"Features","text":"<ul> <li>Change DB model:</li> <li>Added referenceDateTime and executionDateTime</li> <li>Type in DB - timestamp with tz</li> <li>Render format can be setup in application.conf</li> <li>Job-conf variables are changed to referenceDateTime and executionDateTime</li> <li>Changed init sql script and also added alter sql script</li> <li>Added option to sent aggregated messaged to Kafka: one per each target type</li> <li>Added option to run DQ in Shared Spark Context</li> <li>Added new types of history DB: Hive and File (both managed by spark without extra services)</li> </ul>"},{"location":"changelog/CHANGELOG/#bug-fixes_5","title":"Bug Fixes","text":"<ul> <li>Fixed SQL checks</li> <li>Made DQ case-insensitive in terms of column names</li> <li>Docs updates</li> </ul>"},{"location":"changelog/CHANGELOG/#020-2023-06-21","title":"0.2.0 (2023-06-21)","text":""},{"location":"changelog/CHANGELOG/#features_5","title":"Features","text":"<ul> <li>Adding support of Spark 2.4+ and Spark 3+</li> <li>Project is rebuild for Scala 2.12.18</li> <li>Added test for HdfsReader</li> </ul>"},{"location":"changelog/CHANGELOG/#bug-fixes_6","title":"Bug Fixes","text":"<ul> <li>Fixed HdfsReader in terms of loading fixed-width files: added casting to requested column types</li> <li>HBase source is temporarily turned off (due to connector is incompatible with newer versions of Scala and Spark)</li> </ul>"},{"location":"changelog/CHANGELOG/#0110-2023-06-15","title":"0.1.10 (2023-06-15)","text":""},{"location":"changelog/CHANGELOG/#features_6","title":"Features","text":"<ul> <li>Adding Kafka support:</li> <li>New section in run configuration to describe connection to Kafka Brokers</li> <li>New type of source to read from Kafka topic</li> <li>Output of all targets to Kafka topic</li> <li>Adding Mattermost notifications:</li> <li>New section added to application configuration to describe connection to Mattermost API.</li> <li>CheckAlerts and summary reports can be sent to Mattermost.</li> <li>Notifications can be sent to both channels and user direct messages.</li> <li>Added new DQ application argument -v to pass extra variables to be prepended to application configuration file. Can be used to pass secrets for email, mattermost and storage DB on startup.</li> <li>Documentation is updated according to new features.</li> </ul>"},{"location":"changelog/CHANGELOG/#bug-fixes_7","title":"Bug Fixes","text":"<ul> <li>Fixed email configuration for cases when smpt support anonymous connection (user and password are undefined)</li> </ul>"},{"location":"changelog/CHANGELOG/#019-2023-05-17","title":"0.1.9 (2023-05-17)","text":""},{"location":"changelog/CHANGELOG/#features_7","title":"Features","text":"<ul> <li>Enable mailing notifications</li> <li>Added summary section to targets to set up summary reports sending via email</li> <li>Added checkAlerts section to targets to set up critical check alerts via email</li> <li>Added errorCollection section to targets to set up error collection (stored to HDFS only for now)</li> <li>Refactored metric error collection: single file with unified format will be written</li> <li>Modified config model for virtualSources to allow set the saving options directly when declaring virtual source.</li> <li>Update application config files with default production settings for mailing</li> <li>Update documentation</li> </ul>"},{"location":"changelog/CHANGELOG/#bug-fixes_8","title":"Bug Fixes","text":"<ul> <li>Update spark accumulator for metric error collection</li> <li>Add TLS Support to mailer</li> <li>Change mail attachments to ByteArrayDataSource in order to provide attachment as a byte stream instead of file.</li> <li>Minor documentation fixes</li> <li>Prevent empty file creation when errors accumulator is empty (this is also the case when keyFields are not set)</li> </ul>"},{"location":"changelog/CHANGELOG/#018-2023-04-20","title":"0.1.8 (2023-04-20)","text":""},{"location":"changelog/CHANGELOG/#features_8","title":"Features","text":"<ul> <li>Enable metric error collection.</li> <li>Add keyFields to source for purpose of metric error collection.</li> <li>Update metrics to make them Statusable whenever possible.</li> <li>Enhance DQ command line arguments to allow passing arbitrary number of variables on startup which will be added to   DQ configuration file in runtime and may be references within it.</li> <li>Updated documentation to reflect aforementioned changes.</li> </ul>"},{"location":"changelog/CHANGELOG/#bug-fixes_9","title":"Bug Fixes","text":"<ul> <li>Updated metric tests to cover Statusable calculation.</li> <li>Refactored configuration file parsing: now the variables are prepended to configuration file by means of streams   and no temporary file is created.</li> </ul>"},{"location":"changelog/CHANGELOG/#017-2023-04-04","title":"0.1.7 (2023-04-04)","text":""},{"location":"changelog/CHANGELOG/#features_9","title":"Features","text":"<ul> <li>Added numberNutBetween metric</li> <li>Added new results view to database init sql</li> <li>Added jobId to metric results model</li> </ul>"},{"location":"changelog/CHANGELOG/#bug-fixes_10","title":"Bug Fixes","text":"<ul> <li>Fixed composed metric calculator: increased power operation priority</li> <li>Fixed database config reading</li> <li>Fixed RDBMS source reading</li> <li>Fixed HistoryDBManager to additionally filter query results by jobId</li> </ul>"},{"location":"changelog/CHANGELOG/#016-2022-08-23","title":"0.1.6 (2022-08-23)","text":""},{"location":"changelog/CHANGELOG/#features_10","title":"Features","text":"<ul> <li>Added unit tests for column and file metric calculators;</li> <li>Refactored date-related metrics to make them work with timestamp column type correctly;</li> </ul>"},{"location":"changelog/CHANGELOG/#bug-fixes_11","title":"Bug Fixes","text":"<ul> <li>Fix build.sbt for production environment</li> <li>Add hive meta schema initialisation (script for schema initialisation in DQ database, not part of DQ application);</li> <li>Fix schema creation in db-init.sql (script for schema initialisation in DQ database, not part of DQ application);</li> <li>Other minor bug fixes. </li> </ul>"},{"location":"changelog/CHANGELOG/#015-2022-07-01","title":"0.1.5 (2022-07-01)","text":""},{"location":"changelog/CHANGELOG/#features_11","title":"Features","text":"<ul> <li>Added documentation</li> <li>SBT build is updated in terms of assembling uber-jars.</li> </ul>"},{"location":"changelog/CHANGELOG/#014-2022-06-22","title":"0.1.4 (2022-06-22)","text":""},{"location":"changelog/CHANGELOG/#features_12","title":"Features","text":"<ul> <li>New ConfigReader is added</li> <li>Format of metrics configuration file should be set within application.conf (0.x for old format 1.x for new format)</li> <li>Documentation on how to fill various sections of job configuration file is added.</li> </ul>"},{"location":"contribution/","title":"Contribution","text":"<p>Thank you for considering contributing to our project! We welcome contributions from everyone. By participating in this project, you agree to abide by our Code of Conduct.</p> <p>Please take a moment to review our Contribution Guide in order to make the contribution process as smooth as possible.</p>"},{"location":"contribution/code-of-conduct/","title":"Code of Conduct","text":""},{"location":"contribution/code-of-conduct/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and inclusive environment, we as contributors and maintainers pledge to make participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, education, socioeconomic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"contribution/code-of-conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Being respectful and inclusive of differing viewpoints and experiences.</li> <li>Giving and receiving constructive feedback gracefully.</li> <li>Using welcoming and inclusive language.</li> <li>Focusing on what is best for the community.</li> <li>Showing empathy towards other community members.</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or advances.</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks.</li> <li>Public or private harassment.</li> <li>Publishing others' private information, such as physical or electronic addresses, without explicit permission.</li> <li>Other conduct which could reasonably be considered inappropriate in a professional setting.</li> </ul>"},{"location":"contribution/code-of-conduct/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"contribution/code-of-conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at GitHub. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"contribution/code-of-conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.</p>"},{"location":"contribution/code-of-conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"contribution/code-of-conduct/#acknowledgements","title":"Acknowledgements","text":"<p>We thank the open-source community for providing inspiration and examples for creating a welcoming and inclusive Code of Conduct. Your efforts make the tech community a better place for everyone.</p>"},{"location":"contribution/contribution/","title":"Contribution Guide","text":""},{"location":"contribution/contribution/#types-of-contributions","title":"Types of Contributions","text":"<p>We value all kinds of contributions, including:</p> <ul> <li>Bug Reports: Help us identify and resolve issues by submitting detailed bug reports.</li> <li>Feature Requests: Suggest new features or improvements to enhance the project.</li> <li>Code Contributions: Submit code changes via pull requests to fix bugs or implement new features.</li> <li>Documentation: Improve the project's documentation to help others understand and use it better.</li> <li>Translations: Contribute translations to make the project more accessible to a wider audience.</li> <li>Community Engagement: Participate in discussions, help answer questions, and provide support to others.</li> </ul>"},{"location":"contribution/contribution/#getting-started","title":"Getting Started","text":"<ul> <li>Fork the repository and clone it locally.</li> <li>Create a new branch for your contribution: git checkout -b your-branch-name.</li> <li>Make your changes and test thoroughly.</li> <li>Commit your changes with a descriptive commit message: git commit -m \"Your message here\".</li> <li>Push your changes to your fork: git push origin your-branch-name.</li> <li>Open a pull request against the main branch of this repository.</li> </ul>"},{"location":"contribution/contribution/#code-style-and-guidelines","title":"Code Style and Guidelines","text":"<ul> <li>Follow the existing code style and naming conventions.</li> <li>Write clear and concise code and comments to enhance readability.</li> <li>Ensure your code is well-tested, especially for bug fixes and new features.</li> </ul>"},{"location":"contribution/contribution/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<ul> <li>Provide a clear title and description for your pull request, explaining the purpose of your changes.</li> <li>Reference any related issues or pull requests in your description (e.g., \"Fixes #123\").</li> <li>Include screenshots or GIFs if your change affects the visual appearance.</li> <li>Ensure all tests pass successfully.</li> <li>Keep your pull request focused. If you're addressing multiple issues or features, consider submitting separate pull requests.</li> <li>Be responsive to feedback and requests for changes during the review process.</li> </ul>"},{"location":"contribution/contribution/#review-process","title":"Review Process","text":"<p>All contributions will be reviewed by the maintainers of the project. Feedback or suggestions for improvement may be provided. Once everything is approved, your contribution will be merged.</p>"},{"location":"contribution/contribution/#code-of-conduct","title":"Code of Conduct","text":"<p>Please read and adhere to our Code of Conduct in all your interactions with the project.</p>"},{"location":"contribution/contribution/#attribution","title":"Attribution","text":"<p>A huge thanks to all contributors who help make this project better!</p> <p>If you're unsure about anything, feel free to ask for clarification. We appreciate your efforts to make our project better and look forward to your contributions!</p>"},{"location":"","title":"Home","text":"<p>Latest Version: 1.0.0</p> <p>To ensure quality of big data, it is necessary to perform calculations of a large number of metrics and checks on huge datasets, which in turn is a difficult task.</p> <p>Checkita is a Data Quality Framework that solves this problem by formalizing and simplifying the process connecting and reading data from various sources, describing metrics and checks on data from these sources,  as well as sending results and notifications via various channels.</p> <p>Thus, Checkita allows calculating various metrics and checks on data (both structured, and unstructured). The framework is able to perform distributed computing on data in a \"single pass\", using Spark as a computation core. Hocon configurations are used to describe application configurations and job pipelines. Job results are saved in a dedicated framework database, and can also be sent to users via various channels such as File (to local FS, HDFS, S3), Email, Mattermost and Kafka.</p> <p>Using Spark as a computation engine allows performing metrics and checks calculations at the level of \"raw\" data, without requiring any SQL abstractions over the data (such as Hive or Impala), which in turn can hide some errors in the data (e.g. bad formatting or schema mismatch).</p> <p>Summarizing, Checkita is able to do following:</p> <ul> <li>Read data from various sources (HDFS, S3, Hive, Jdbc, Kafka) and in various formats (text, orc, parquet, avro).</li> <li>Accept SQL queries on data, thus forming derived \"virtual sources\" of data.   This functionality is implemented with use of the Spark DataFrame API.</li> <li>Perform calculation of a wide range of metrics on data, as well as to perform composition of metrics.</li> <li>Perform checks on data based on calculated metrics.</li> <li>Perform checks based on previous calculation results (anomaly detection in data).</li> <li>Save calculation results to the dedicated framework database and also send them via other channels  (HDFS, S3, Hive, Kafka, Email, Mattermost).</li> </ul> <p>Checkita is designed with focus on integration into ETL pipelines and data catalogues:</p> <ul> <li>Uses Spark as core and can be run as an ordinary spark application.   Spark, in its turn, is the most widely used solution for distributed data processing. </li> <li>No-code configuration via Hocon files which easy to set up and manage via VSC.</li> <li>Dedicated databased is used to store calculation results and can be used to serve this results   to end users by means of dashboards or simple UI.</li> <li>Built-in support for notification (Email, Mattermost) to inform about any issues with quality of data.</li> <li>Alternative output channels such as Kafka can be used for integration with other services.</li> </ul> <p>The framework is written in Scala 2.12 and uses Spark 2.4+ as the computation core. The project is configured with a parameterized SBT build that allows building the framework for a specific version of Spark, publish the project to a given repository, and also build Uber-jar, both with and without Spark dependencies.</p> <p>License</p> <p>Checkita Data Quality framework is GNU LGPL licensed.</p> <p>This project is a reimagination of Data Quality Framework developed by Agile Lab, Italy.</p>"},{"location":"01-application-setup/","title":"General Information","text":"<p>Checkita runs as a Spark Application. Thus, it can be run in the same way as any other Spark application:</p> <ul> <li>locally, on the client machine;</li> <li>in a dedicated Spark cluster;</li> <li>via resource manager (YARN, Mesos);</li> <li>in a Kubernetes cluster.</li> </ul> <p>Both application spark-submit modes are also supported: <code>client</code> and <code>cluster</code>.</p> <p>The framework was developed primarily for batch data processing and currently supports only this mode of operation. A typical architecture for working with Checkita Data Quality is shown in the diagram below:</p> <ul> <li>The Uber-jar of the framework is built   (usually without the dependencies of the Spark itself, since they are already available on the cluster).</li> <li>Application configuration file is prepared: defines general Checkita Data Quality settings.</li> <li>A configuration file describing Data Quality job pipeline is prepared in accordance with the documentation.</li> <li>Spark Application is started.</li> <li>Spark Application loads the sources described in the configuration file (HDFS, S3, Hive, external databases),   calculates metrics, performs checks and saves the results:</li> <li>The main results are saved in the framework database.</li> <li>Additionally, results and notifications are sent via channels configured in the pipeline.</li> <li>Based on the results, dashboards are formed to monitor data quality   (not included in the functionality of this framework).</li> </ul> <p>Also, the Data Quality Framework can be used for streaming data processing, however, this functionality is under development.</p> <p></p>"},{"location":"01-application-setup/01-ApplicationSettings/","title":"Application Settings","text":"<p>General Checkita Data Quality settings are configured in Hocon file <code>application.conf</code> which is supplied to the application on the startup. All configurations are set within <code>appConfig</code> section.</p> <p>There is only one parameter that is set at the top level and this is <code>applicationName</code> - name of the Spark application. This parameter is optional and if not set, then <code>Checkita Data Quality</code> application name is used by default.</p> <p>The rest of the parameters are defined in the subsections that are described below.</p>"},{"location":"01-application-setup/01-ApplicationSettings/#datetime-settings","title":"DateTime Settings","text":"<p>DateTime configurations are set in the <code>dateTimeOptions</code> section.  Please, see Working with Date and Time  section for more details on working with date and time in Checkita Framework.</p> <p>DateTime settings include following:</p> <ul> <li><code>timeZone</code> - Time zone in which string representation of reference date and execution date are parsed and rendered.   Optional, default is <code>\"UTC\"</code>.</li> <li><code>referenceDateFormat</code> - datetime format used to parse and render reference date.   Optional, default is <code>\"yyyy-MM-dd'T'HH:mm:ss.SSS\"</code>.</li> <li><code>executionDateFormat</code> - datetime format used to parse and render execution date.   Optional, default is <code>\"yyyy-MM-dd'T'HH:mm:ss.SSS\"</code></li> </ul> <p>If <code>dateTimeOptions</code> section is missing then default values are used for all parameters above.</p>"},{"location":"01-application-setup/01-ApplicationSettings/#enablers","title":"Enablers","text":"<p>Section <code>enablers</code> of application configuration file defines various boolean switchers is single-value parameters that controls various aspects of data quality job execution:</p> <ul> <li><code>allowSqlQueries</code> - Enables usage arbitrary SQL queries in data quality job configuration.   Optional, default is <code>false</code></li> <li><code>allowNotifications</code> - Enables notifications to be sent from DQ application.    Optional, default is <code>false</code></li> <li><code>aggregatedKafkaOutput</code> - Enables sending aggregates messages for Kafka Targets (one per each target type).   By default, kafka messages are sent per each result entity.   Optional, default is <code>false</code></li> <li><code>enableCaseSensitivity</code> - Enable columns case sensitivity. Controls column names comparison and lookup.   Optional, default is <code>false</code></li> <li><code>errorDumpSize</code> - Maximum number of errors to be collected per single metric. Framework is able to collect source    data rows where metric evaluation yielded some errors. But in order to prevent OOM the number of collected errors   have to be limited to a reasonable value. Thus, maximum allowable number of errors per metric is <code>10000</code>.   It is possible to lower this number by setting this parameter. Optional, default is <code>10000</code></li> <li><code>outputRepartition</code> - Sets the number of partitions when writing outputs. By default, writes single file.   Optional, default is <code>1</code></li> </ul> <p>If <code>enablers</code> section is missing then default values are used for all parameters above.</p>"},{"location":"01-application-setup/01-ApplicationSettings/#storage-configuration","title":"Storage Configuration","text":"<p>Parameters for connecting to Data Quality results storage are defined in <code>storage</code> section of application configuration.</p> <p>For more information on results storage refer to Data Quality Results Storage chapter of the documentation.</p> <p>Thus, connection to storage is configured using following parameters:</p> <ul> <li><code>dbType</code> - Type of database used to store Data Quality results. Required.</li> <li><code>url</code> - Database connection URL (without protocol identifiers). Required.</li> <li><code>username</code> - Username to connect to database with (if required). Optional.</li> <li><code>password</code> - Password to connect to database with (if required). Optional.</li> <li><code>schema</code> - Schema where data quality tables are located (if required). Optional.</li> </ul> <p>IMPORTANT If <code>storage</code> section is missing then application will run without usage of results storage:</p> <ul> <li>results won't be saved (only targets can be sent);</li> <li>trend checks (used for anomaly detection in data) won't be performed as they require historical data.</li> </ul>"},{"location":"01-application-setup/01-ApplicationSettings/#email-configuration","title":"Email Configuration","text":"<p>In order to send notification via email it is necessary to configure connection to SMTP server which should be defined in <code>email</code> section of application configuration with following parameters:</p> <ul> <li><code>host</code> - SMTP server host. Required.</li> <li><code>port</code> - SMTP server port. Required.</li> <li><code>address</code> - Email address to sent notification from. Required.</li> <li><code>name</code> - Name of the sender. Required.</li> <li><code>sslOnConnect</code> - Boolean parameter indicating whether to use SSL on connect. Optional, default is <code>false</code>.</li> <li><code>tlsEnabled</code> - Boolean parameter indicating whether to enable TLS. Optional, default is <code>false</code>.</li> <li><code>username</code> - Username for connection to SMTP server (if required). Optional.</li> <li><code>password</code> - Password for connection to SMTP server (if required). Optional.</li> </ul> <p>If <code>email</code> section is missing then email notifications cannot be sent. If ones were configured in job configuration, then exception would be thrown at runtime.</p>"},{"location":"01-application-setup/01-ApplicationSettings/#mattermost-configuration","title":"Mattermost Configuration","text":"<p>In order to send notification to Mattermost it is necessary to configure connection to Mattermost API which should be defined in <code>mattermost</code> section of application configuration with following parameters:</p> <ul> <li><code>host</code> - Mattermost API host.</li> <li><code>token</code> - Mattermost API token (using Bot accounts for notifications is preferable).</li> </ul> <p>If <code>mattermost</code> section is missing then corresponding notifications cannot be sent. If ones were configured in job  configuration, then exception would be thrown at runtime.</p>"},{"location":"01-application-setup/01-ApplicationSettings/#default-spark-parameters","title":"Default Spark Parameters","text":"<p>It is also possible to provide list of default Spark configuration parameters used across multiple jobs. These parameters should be provided as <code>defaultSparkOptions</code> list where each parameter is a string in format: <code>spark.param.name=spark.param.value</code>.</p>"},{"location":"01-application-setup/01-ApplicationSettings/#example-of-application-configuration-file","title":"Example of Application Configuration File","text":"<p>Hocon configuration format supports variable substitution and Checkita Data Quality framework has a mechanism to  feed configuration files with extra variables at runtime. For more information, see Usage of Environment Variables and Extra Variables  chapter of the documentation.</p> <pre><code>appConfig: {\n\n  applicationName: \"Custom Data Quality Application Name\"\n\n  dateTimeOptions: {\n    timeZone: \"GMT+3\"\n    referenceDateFormat: \"yyyy-MM-dd\"\n    executionDateFormat: \"yyyy-MM-dd-HH-mm-ss\"\n  }\n\n  enablers: {\n    allowSqlQueries: false\n    allowNotifications: true\n    aggregatedKafkaOutput: true\n  }\n\n  defaultSparkOptions: [\n    \"spark.sql.orc.enabled=true\"\n    \"spark.sql.parquet.compression.codec=snappy\"\n    \"spark.sql.autoBroadcastJoinThreshold=-1\"\n  ]\n\n  storage: {\n    dbType: \"postgres\"\n    url: \"localhost:5432/public\"\n    username: \"postgres\"\n    password: \"postgres\"\n    schema: \"dqdb\"\n  }\n\n  email: {\n    host: \"smtp.some-company.domain\"\n    port: \"25\"\n    username: \"emailUser\"\n    password: \"emailPassword\"\n    address: \"some.service@some-company.domain\"\n    name: \"Data Quality Service\"\n    sslOnConnect: true\n  }\n\n  mattermost: {\n    host: \"https://some-team.mattermost.com\"\n    token: ${dqMattermostToken}\n  }\n}\n</code></pre>"},{"location":"01-application-setup/02-ApplicationSubmit/","title":"Submitting Data Quality Application","text":"<p>Since Checkita framework is based on Spark, it runs as an ordinary Spark application using <code>spark-submit</code> command. And as any Spark application, Checkita applications can be run both locally and on a cluster (in <code>client</code> or <code>cluster</code> mode).</p> <p>However, Checkita applications require some command line arguments to be passed on startup. These are:</p> <ul> <li><code>-a</code> - Required. Path to HOCON file with application settings: <code>application.conf</code>.   Note, that name of the file may vary, but usually aforementioned name is used.</li> <li><code>-j</code> - Required. List of paths to job configuration files. Paths must be separated by commas.   Hocon format supports  configuration merging, therefore, it is possible to define different parts of   job configuration in separate files and reuse some common configuration sections.</li> <li><code>-d</code> - Optional. Datetime for which the Data Quality job is being run. Date string must conform to format specified    in <code>referenceDateFormat</code> parameter of the application settings. If date is not provided on startup, then it will be    set to application start date.</li> <li><code>-l</code> - Optional. Flag indicating that application should be run in local mode.</li> <li><code>-s</code> - Optional. Flag indicating that application will be run using Shared Spark Context. In this case application   will get existing context instead of creating a new one. It is also quite important not to stop it upon job completion.</li> <li><code>-m</code> - Optional. Flag indicating that storage database migration must be performed prior results saving.</li> <li><code>-e</code> - Optional. Extra variables to be added to configuration files during prior parsing. These variables can be   used in configuration files, e.g. to pass secrets. Variables are provided in key-value format:   <code>\"k1=v1,k2=v2,k3=v3,...\"\"</code>.</li> <li><code>-v</code> - Optional. Application log verbosity. By default, log level is set to <code>INFO</code>.</li> </ul> <p>The following is an example of running an application in YARN in <code>cluster</code> mode. Framework storage database connection parameters are specified in <code>application.conf</code> and secrets may be passed either via environment variables or via extra variables argument. For more details see  Usage of Environment Variables and Extra Variables.</p> <pre><code>export DQ_APPLICATION=\"&lt;local or remote (HDFS, S3) path to uber-jar application&gt;\"\nexport DQ_APP_CONFIG=\"&lt;local or remote (HDFS, S3) path to application configuration file&gt;\"\nexport DQ_JOB_CONFIGS=\"&lt;local or remote (HDFS, S3) paths to job configuration files separated by commas&gt;\"\n\n# As configuration files are uploaded to driver and executors they will be located in working directories.\n# Therefore, in application arguments it is required to list just their file names:\nexport DQ_APP_CONFIG_FILE=$(basename $DQ_APP_CONFIG)\nexport DQ_JOB_CONFIG_FILES=\"&lt;job configuration files separated by commas (only file names)&gt;\"\nexport REFERENCE_DATE=\"2023-08-01\"\n\n# application entry point (executable class): ru.raiffeisen.checkita.apps.batch.DataQualityBatchApp\n# --name spark-submit argument has a higher priority over application name set in `application.conf`\n\nspark-submit\\\n   --class ru.raiffeisen.checkita.apps.batch.DataQualityBatchApp \\\n   --name \"Checkita Data Quality\" \\\n   --master yarn \\\n   --deploy-mode cluster \\\n   --num-executors 1 \\\n   --executor-memory 2g \\\n   --executor-cores 4 \\\n   --driver-memory 2g \\\n   --files \"$DQ_APP_CONFIG,$DQ_DQ_JOB_CONFIGS\" \\\n   --conf \"spark.executor.memoryOverhead=2g\" \\\n   --conf \"spark.driver.memoryOverhead=2g\" \\\n   --conf \"spark.driver.maxResultSize=4g\" \\\n   $DQ_APPLICATION \\\n   -a $DQ_APP_CONFIG_FILE \\\n   -j $DQ_JOB_CONFIG_FILES \\\n   -d $REFERENCE_DATE \\\n   -e \"storage_db_user=some_db_user,storage_db_password=some_db_password\"\n</code></pre>"},{"location":"01-application-setup/03-ResultsStorage/","title":"Data Quality Results Storage","text":"<p>In order to use all features of the framework, it is required to set up a results storage. Checkita can use various RDBMS as a results storage. Also, Hive can be used as a results storage and even a simple file storage is supported.</p> <p>The full list of various storage types is following:</p> <ul> <li><code>PostgreSQL</code> (v.9.3 and higher) - recommended database to be used as resutls storage.</li> <li><code>Oracle</code></li> <li><code>MySQL</code></li> <li><code>Microsoft SQL Server</code></li> <li><code>SQLite</code></li> <li><code>H2</code></li> <li><code>Hive</code></li> <li><code>File</code> (directory in local file system or remote one (HDFS, S3))</li> </ul> <p>Checkita framework support results storage schema evolution. Flyway is run under the hood to support schema migrations. Therefore, if one of the supported RDBMS is chosen for results storage then it is possible to set it up during the first run of the Data Quality job providing <code>-m</code> application argument on startup. For more details on how to run Data Quality applications refer to  Submitting Data Quality Application chapter.</p> <p>IMPORTANT: Flyway migrations usually run either in empty database/schema or in one that was initiated with  Flyway. In Checkita framework it is also possible to run migration in non-empty database/schema. In this case it is up to user to ensure that there are no conflicting table names in database/schema.</p> <p>If <code>File</code> type of storage is used then it is only required to provide a path to a directory/bucket, where results will be stored. Results are stored as parquet files with the same schema as for RDMS storage. No schema evolution mechanisms are provided for <code>File</code> type of storage. Therefore, if results schemas would evolve later, it will be up to user to update existing results to a new structure.</p> <p>IMPORTANT: There is no partitioning used for storing results as parquet files. Every job will read entire results history and overwrite it adding new ones. Therefore, using <code>File</code> type of storage is not recommended for production use. </p> <p>For <code>Hive</code> type of storage the schema evolution mechanisms are also not available. Therefore, it is up to user to create corresponding hive tables. DDL scripts from Hive Storage Setup Scripts chapter below can be used for that.</p> <p>IMPORTANT: Results hive table must be partitioned by <code>job_id</code>. Job ID is chosen as partition column to support faster results fetching during computation of trend checks (used for anomaly detection in data). <code>Hive</code> type of results storage works faster that <code>File</code> one, since only partition for current <code>job_id</code> is read and  overwritten. Nevertheless, this type of storage is also not recommended for use in production where large number of jobs will be run.</p>"},{"location":"01-application-setup/03-ResultsStorage/#results-types-and-schemas","title":"Results Types and Schemas","text":"<p>There are for types of result are written in storage:</p> <ul> <li>regular metrics results</li> <li>composed metrics results</li> <li>load checks results</li> <li>checks results</li> </ul> <p>Schemas for all results types are given below.</p> <p>Primary keys denotes how we keep track if unique records:  generally results for the same Data Quality job that is run for the same reference date are overwritten. History of various attempts of the same job for the same reference date is not stored. It is done in order trend checks work correctly. As these checks read historical results from Data Quality storage, it is required that there will be only one set of results per Data Quality job and given reference date.</p>"},{"location":"01-application-setup/03-ResultsStorage/#regular-metrics-results-schema","title":"Regular Metrics Results Schema","text":"<ul> <li>Primary key: <code>(job_id, metric_id, reference_date)</code></li> <li><code>source_id</code> &amp; <code>column_names</code> contain string representation of lists in format <code>'[val1,val2,val3]'</code>.</li> <li><code>params</code> is a JSON string.</li> </ul> Column Name Column Type Constraint job_id STRING NOT NULL metric_id STRING NOT NULL metric_name STRING NOT NULL description STRING source_id STRING NOT NULL column_names STRING params STRING result DOUBLE NOT NULL additional_result STRING reference_date TIMESTAMP NOT NULL execution_date TIMESTAMP NOT NULL"},{"location":"01-application-setup/03-ResultsStorage/#composed-metrics-results-schema","title":"Composed Metrics Results Schema","text":"<ul> <li>Primary key: <code>(job_id, metric_id, reference_date)</code></li> <li><code>source_id</code> contains string representation of lists in format <code>'[val1,val2,val3]'</code>.</li> </ul> Column Name Column Type Constraint job_id STRING NOT NULL metric_id STRING NOT NULL metric_name STRING NOT NULL description STRING source_id STRING NOT NULL formula STRING NOT NULL result DOUBLE NOT NULL additional_result STRING reference_date TIMESTAMP NOT NULL execution_date TIMESTAMP NOT NULL"},{"location":"01-application-setup/03-ResultsStorage/#load-checks-results-schema","title":"Load Checks Results Schema","text":"<ul> <li>Primary key: <code>(job_id, check_id, reference_date)</code></li> <li><code>source_id</code> contains string representation of lists in format <code>'[val1,val2,val3]'</code>.</li> </ul> Column Name Column Type Constraint job_id STRING NOT NULL check_id STRING NOT NULL check_name STRING NOT NULL source_id STRING NOT NULL expected STRING NOT NULL status STRING NOT NULL message STRING reference_date TIMESTAMP NOT NULL execution_date TIMESTAMP NOT NULL"},{"location":"01-application-setup/03-ResultsStorage/#checks-results-schema","title":"Checks Results Schema","text":"<ul> <li>Primary key: <code>(job_id, check_id, reference_date)</code></li> <li><code>source_id</code> contains string representation of lists in format <code>'[val1,val2,val3]'</code>.</li> </ul> Column Name Column Type Constraint job_id STRING NOT NULL check_id STRING NOT NULL check_name STRING NOT NULL description STRING source_id STRING NOT NULL base_metric STRING NOT NULL compared_metric STRING compared_threshold DOUBLE lower_bound DOUBLE upper_bound DOUBLE status STRING NOT NULL message STRING reference_date TIMESTAMP NOT NULL execution_date TIMESTAMP NOT NULL"},{"location":"01-application-setup/03-ResultsStorage/#hive-storage-setup-scripts","title":"Hive Storage Setup Scripts","text":"<p>Below is a HiveQL script that can be used to set up Hive results storage:</p> <pre><code>-- REPLACE &lt;schema_name&gt; and &lt;schema_dir&gt; with actual name and path:\nset hivevar:schema_name=&lt;schema_name&gt;;\nset hivevar:schema_dir=&lt;schema_path&gt;;\n\nCREATE SCHEMA IF NOT EXISTS ${schema_name};\n\nDROP TABLE IF EXISTS ${schema_name}.results_metric_regular;\nCREATE EXTERNAL TABLE ${schema_name}.results_metric_regular\n(\n    job_id            STRING COMMENT '',\n    metric_id         STRING COMMENT '',\n    metric_name       STRING COMMENT '',\n    description       STRING COMMENT '',\n    source_id         STRING COMMENT '',\n    column_names      STRING COMMENT '',\n    params            STRING COMMENT '',\n    result            DOUBLE COMMENT '',\n    additional_result STRING COMMENT '',\n    reference_date    TIMESTAMP COMMENT '',\n    execution_date    TIMESTAMP COMMENT ''\n)\nCOMMENT 'Data Quality Regular Metrics Results'\nPARTITIONED BY (job_id STRING)\nSTORED AS PARQUET\nLOCATION '${schema_dir}/results_metric_regular';\n\nDROP TABLE IF EXISTS ${schema_name}.results_metric_composed;\nCREATE EXTERNAL TABLE ${schema_name}.results_metric_composed\n(\n    job_id            STRING COMMENT '',\n    metric_id         STRING COMMENT '',\n    metric_name       STRING COMMENT '',\n    description       STRING COMMENT '',\n    source_id         STRING COMMENT '',\n    formula           STRING COMMENT '',\n    result            DOUBLE COMMENT '',\n    additional_result STRING COMMENT '',\n    reference_date    TIMESTAMP COMMENT '',\n    execution_date    TIMESTAMP COMMENT ''\n)\nCOMMENT 'Data Quality Composed Metrics Results'\nPARTITIONED BY (job_id STRING)\nSTORED AS PARQUET\nLOCATION '${schema_dir}/results_metric_composed';\n\nDROP TABLE IF EXISTS ${schema_name}.results_check_load;\nCREATE EXTERNAL TABLE ${schema_name}.results_check_load\n(\n    job_id         STRING COMMENT '',\n    check_id       STRING COMMENT '',\n    check_name     STRING COMMENT '',\n    source_id      STRING COMMENT '',\n    expected       STRING COMMENT '',\n    status         STRING COMMENT '',\n    message        STRING COMMENT '',\n    reference_date TIMESTAMP COMMENT '',\n    execution_date TIMESTAMP COMMENT ''\n)\nCOMMENT 'Data Quality Load Checks Results'\nPARTITIONED BY (job_id STRING)\nSTORED AS PARQUET\nLOCATION '${schema_dir}/results_check_load';\n\nDROP TABLE IF EXISTS ${schema_name}.results_check;\nCREATE EXTERNAL TABLE ${schema_name}.results_check\n(\n    job_id             STRING COMMENT '',\n    check_id           STRING COMMENT '',\n    check_name         STRING COMMENT '',\n    description        STRING COMMENT '',\n    source_id          STRING COMMENT '',\n    base_metric        STRING COMMENT '',\n    compared_metric    STRING COMMENT '',\n    compared_threshold DOUBLE COMMENT '',\n    lower_bound        DOUBLE COMMENT '',\n    upper_bound        DOUBLE COMMENT '',\n    status             STRING COMMENT '',\n    message            STRING COMMENT '',\n    reference_date     TIMESTAMP COMMENT '',\n    execution_date     TIMESTAMP COMMENT ''\n)\nCOMMENT 'Data Quality Checks Results'\nPARTITIONED BY (job_id STRING)\nSTORED AS PARQUET\nLOCATION '${schema_dir}/results_check';\n</code></pre>"},{"location":"02-general-concepts/","title":"General Concepts","text":"<p>In this section various aspects of working with Checkita Data Quality framework are explained.</p>"},{"location":"02-general-concepts/01-WorkingWithDateTime/","title":"Working with Date and Time","text":"<p>There are two type of datetime instances used in order to identify various Data Quality job runs. These are:</p> <ul> <li><code>referenceDate</code> - identifies date for which the job is run. This datetime usually indicates for which   period data is read and checked.</li> <li><code>executionDate</code> - stores actual application start datetime and used to indicate when exactly data quality job is run.</li> </ul> <p>Typical case is when we run some ETL pipeline after \"closure of business\", e.g. at midnight. Thus, the <code>referenceDate</code> will refer to a previous day, while <code>executionDate</code> will have value of actual start of data quality job. It is likely that we would like to represent these values differently. Thus, in application configuration we can configure different formats for <code>referenceDate</code> and <code>executionDate</code></p> <p>As <code>referenceDate</code> can point to a date in the past, then it is allowed to explicitly provide its values on application startup. If value of <code>referenceDate</code> is not provided, then it is set to datetime of actual start of data quality job. See Submitting Data Quality Application  chapter for more information on application startup arguments.</p> <p>Both of these datetime instances are widely used across framework. Thus, whenever string representation of them is required, it is obtained using datetime parameters set in the application configuration file.</p> <p>It also should be noted, that datetime rendering is performed with respect to timezone in which the application is running. Timezone is also set in application configuration file. The <code>UTC</code> time zone is used by default.</p> <p>The last but not least: we avoid using datetime string representation when storing results into storage database. Both <code>referenceDate</code> and <code>executionDate</code> are converted to timestamp at <code>UTC</code> timezone, instead. This ensures stable results querying from storage independent on datetime configuration parameters. See Data Quality Results Storage chapter for more information  on results storage.</p> <p>IMPORTANT: Actual string representation of <code>referenceDate</code> and <code>exectionDate</code> are always added to configuration files as extra variables. For more details on extra variables usage in configuration files, see  Usage of Environment Variables and Extra Variables chapter.</p>"},{"location":"02-general-concepts/02-EnvironmentAndExtraVariables/","title":"Usage of Environment Variables and Extra Variables","text":"<p>Hocon configuration format supports variable substitution. This mechanism allows more flexible management of both application configuration and job configuration.</p> <p>Thus, configurations files are feed with extra variables that are read from system and JVM environment and can also be explicitly defined at application startup.</p> <p>For more information on how to explicitly define extra variables on startup, see Submitting Data Quality Application chapter of the documentation.</p> <p>In order to use system or JVM environment variables their names must match following regex expression: <code>^(?i)(DQ)[a-z0-9_-]+$</code>, e.g. <code>DQ_STORAGE_PASSOWRD</code> or <code>dqMattermostToken</code>. All environment variables that match this regex expression will be retrieved and available for substitution in both application and job configuration files.</p> <p>Typical use case for variable substitution is to provide secrets for connection to external systems. It is not a good idea to store such information in configuration files and, therefore, there must be a mechanism to provide it at runtime.</p> <p>IMPORTANT: Variables are added to configuration files at runtime and are not stored in any form.</p>"},{"location":"02-general-concepts/03-StatusModel/","title":"Status Model used in Results","text":"<p>Unified status model is used for results that Checkita framework produces. Thus, all metrics and check results have common status indication that is following:</p> <ul> <li><code>Success</code> - Evaluation of metric or check completed without any errors and metric or check condition is met.</li> <li><code>Failure</code> - Evaluation of metric or check yielded results that do not meet configured condition, e.g.:<ul> <li>Regex match metric got a column value that do not match to required regex pattern;</li> <li>Check requiring that some metric result should be equal to zero got a non-zero metric result.</li> </ul> </li> <li><code>Error</code> - Caught runtime error during metric or check evaluation. Runtime error message is caught as well.</li> </ul> <p>Result status is always accompanied by message, that describes this status. What not common between metrics and checks is how statuses are communicated with user:</p> <ul> <li>When computing metrics, status is obtained for each data row during metric increment step. If status other than    <code>Success</code> then metric error is collected for this particular row of data. Then, metric error reports can be requested   as Error Collection Targets. For more information    on metric error collection, see Metric Error Collection chapter.</li> <li>As for checks, status is their primary result output. Therefore, it is written into data quality storage along with   a detailed message.</li> </ul>"},{"location":"02-general-concepts/04-ErrorCollection/","title":"Metric Error Collection","text":"<p>Metric calculation involves reading data row by row and incrementing metric value for each row. During increment step there could be something wrong: either due to problems with data or due to some unexpected runtime errors. In addition, some metrics have logical condition that needs to be met in order to increment the metric value. Failing to satisfy this condition is also considered as failure.</p> <p>Thus, in the situations, described above, there will be error collection mechanism triggered and following data  error or failure data collected:</p> <ul> <li>Metric information: metric id and list of columns;</li> <li>Source information over which metric is calculated: source id and list of key fields.</li> <li>Error information: status (either <code>Failure</code> or <code>Success</code>) and message.</li> <li>Excerpt from row data: only values from metric columns and key fields are collected.</li> </ul> <p>Since the processed source can be extremely large and, subsequently, can yield large amount of metric errors then out-of-memory errors are likely to happen. In order to prevent that, the number of errors collected per each metric is limited. Thus, maximum number of errors collected per metric cannot be more than <code>10000</code>. This number can be additionally limited in the application settings by setting <code>errorDumpSize</code> parameter to a lower number. See Enablers chapter for more details.</p> <p>Collected metric errors could be used to identify and debug problems in the data. In order to save or send metric error reports, Error Collection Targets can be configured in <code>targets</code> section of job configuration. Note that error collection reports will contain excerpts from data and, therefore, should be communicated with caution. For the same reason they are never saved in Data Quality storage.</p>"},{"location":"03-job-configuration/","title":"Job Configuration","text":"<p>Data Quality job in Checkita is a sequence of tasks that need to be performed in order to check quality of data. These tasks may include following:</p> <ul> <li>establishing connections to external systems;</li> <li>reading user-defined schemas (these schemas may later be used during source reading or as reference to    validate actual source schemas);</li> <li>reading sources from various file systems and external systems (using already established connections);</li> <li>creating virtual sources by applying SQL transformations to already existing sources;</li> <li>performing load checks to validate sources metadata;</li> <li>performing regular and composed metrics calculation;</li> <li>performing checks based on computed metric results;</li> <li>sending targets to various channels (specially developed mechanism to allow results communication other than    saving them to results storage)</li> </ul> <p>All the aforementioned tasks are configured in one or multiple Hocon configuration files. All job configurations are set within <code>jobConfig</code> section of the configuration files.</p> <p>There is only one parameter that is set at the top level and this is <code>jobId</code> - ID of the job to be run. This parameter is mandatory for any job configuration. Thus, <code>jobId</code> usually unites calculation of various metrics and  checks that are performed over the sources within single schema, data-mart or other logical formation of data sources.</p> <p>The rest of the parameters are defined in the subsections that are described in a separate  chapters of this documentation:</p> <ul> <li>Connections - describes configuration of connections to various external systems.</li> <li>Schemas - describes configuration of user-defined schemas.</li> <li>Sources - describes configuration of sources to read data from.</li> <li>Virtual Sources - describes configuration of virtual sources.</li> <li>Load Checks - describes configuration of load checks.</li> <li>Metrics - describes configuration of metrics.</li> <li>Checks - describes configuration of checks.</li> <li>Targets - describes configuration of targets.</li> </ul> <p>Example of fully filled job configuration can be found in Job Configuration Example  chapter of this documentation.</p>"},{"location":"03-job-configuration/01-Connections/","title":"Connections Configuration","text":"<p>Checkita framework allows creation of data sources based on data from external systems such as RDBMS or message queues like Kafka. In order to read data from external systems it is required to establish a connection in a first place.</p> <p>Thus, connections are described in <code>connections</code> section of job configuration. Currently, connection to following systems are supported:</p> <ul> <li>Connection to databases via JDBC:<ul> <li>PostgreSQL (also can be used for connection to GreenPlum);</li> <li>Oracle</li> <li>SQl</li> </ul> </li> <li>Connection to message queues:<ul> <li>Kafka</li> </ul> </li> </ul> <p>All connections must have and <code>id</code> to uniquely identify its configuration and also may have an optional list of additional  Spark parameters can be specified in field <code>parameters</code> to provide some extra configuration required by Spark to read data from a particular system.</p> <p>Example of <code>connections</code> section of job configuration is shown in  Connections Configuration Example below.</p>"},{"location":"03-job-configuration/01-Connections/#sqlite-connection-configuration","title":"SQLite Connection Configuration","text":"<p>Configuring connection to SQLite database is quite easy. It is required supplying only two parameters:</p> <ul> <li><code>id</code> - Required. Connection ID;</li> <li><code>url</code> - Required. Path to SQLite database file.</li> <li><code>parameters</code> - Optional. List of Spark parameters if required where each parameter is a string in format:   <code>spark.param.name=spark.param.value</code>.</li> </ul>"},{"location":"03-job-configuration/01-Connections/#postgresql-connection-configuration","title":"PostgreSQL Connection Configuration","text":"<p>Configuration to PostgreSQL can be set up using following parameters:</p> <ul> <li><code>id</code> - Required. Connection ID;</li> <li><code>url</code> - Required. Connection URL. Should contain host, port and name of database.   In addition, extra parameters can be supplied in connection URL if required.   Connection protocol must not be specified.</li> <li><code>username</code> - Optional. Username used to connect to PostgreSQL database if required.</li> <li><code>password</code> - Optional. Password used to connect to PostgreSQL database if required.</li> <li><code>parameters</code> - Optional. List of Spark parameters if required where each parameter is a string in format:   <code>spark.param.name=spark.param.value</code>.</li> </ul>"},{"location":"03-job-configuration/01-Connections/#oracle-connection-configuration","title":"Oracle Connection Configuration","text":"<p>Configuration to Oracle can be set up in the same way as to PostgreSQL, using following parameters:</p> <ul> <li><code>id</code> - Required. Connection ID;</li> <li><code>url</code> - Required. Connection URL. Should contain host, port and name of database.   In addition, extra parameters can be supplied in connection URL if required.   Connection protocol must not be specified.</li> <li><code>username</code> - Optional. Username used to connect to PostgreSQL database if required.</li> <li><code>password</code> - Optional. Password used to connect to PostgreSQL database if required.</li> <li><code>parameters</code> - Optional. List of Spark parameters if required where each parameter is a string in format:   <code>spark.param.name=spark.param.value</code>.</li> </ul>"},{"location":"03-job-configuration/01-Connections/#kafka-connection-configuration","title":"Kafka Connection Configuration","text":"<p>In order to connect to set up connection to Kafka brokers, it is required to supply following parameters:</p> <ul> <li><code>id</code> - Required. Connection ID;</li> <li><code>servers</code> - Required. List of broker servers to connect to.</li> <li><code>parameters</code> - Optional. List of Spark parameters if required where each parameter is a string in format:   <code>spark.param.name=spark.param.value</code>. Usually, Kafka authorisation settings are provided by means of spark parameters.</li> </ul> <p>If connection to Kafka cluster requires JAAS configuration file, then it should be provided via Java environment variables. Note, that these variables must be declared prior JVM starts, therefore, they must be set in <code>spark-submit</code>  command as follows:</p> <ul> <li>When running application in <code>cluster</code> mode:   <pre><code>--deploy-mode cluster \\\n--conf 'spark.driver.extraJavaOptions=\"-Djava.security.auth.login.config=./jaas.conf\"' \\\n--conf 'spark.executor.extraJavaOptions=\"-Djava.security.auth.login.config=./jaas.conf\"' \\\n--files /path/to/your/jaas.conf,&lt;other files required for DQ&gt;\n</code></pre></li> <li>When running application in <code>client</code> mode the driver JVM starts on client prior Spark configuration is read,   therefore, Java environment variables for driver must be set in advance using <code>--driver-java-options</code> argument:   <pre><code>--deploy-mode client \\\n--driver-java-options \"-Djava.security.auth.login.config=.jaas.conf\" \\\n--conf 'spark.executor.extraJavaOptions=\"-Djava.security.auth.login.config=./jaas.conf\"' \\\n--files file.keytab,jaas.conf,&lt;other files required for DQ&gt;\n</code></pre></li> </ul>"},{"location":"03-job-configuration/01-Connections/#connections-configuration-example","title":"Connections Configuration Example","text":"<p>As it is shown in the example below, connections of the same type are grouped within subsections named after the type of connection. These subsections should contain a list of connection configurations of the corresponding type.</p> <pre><code>jobConfig: {\n  connections: {\n    postgresql: [\n      {id: \"postgre_db1\", url: \"postgre1.db.com:5432/public\", user: \"dq-user\", password: \"dq-password\"}\n      {\n        id: \"postgre_db2\",\n        url: \"postgre2.db.com:5432/public\",\n        user: \"dq-user\",\n        password: \"dq-password\",\n        schema: \"dataquality\"\n      }\n    ]\n    oracle: [\n      {id: \"oracle_db1\", url: \"oracle.db.com:1521/public\", username: \"db-user\", password: \"dq-password\"}\n    ]\n    sqlite: [\n      {id: \"sqlite_db\", url: \"some/path/to/db.sqlite\"}\n    ],\n    kafka: [\n      {id: \"kafka_cluster_1\", servers: [\"server1:9092\", \"server2:9092\"]}\n      {\n        id: \"kafka_cluster_2\",\n        servers: [\"kafka-broker1:9092\", \"kafka-broker2:9092\", \"kafka-broker3:9092\"]\n        parameters: [\n          \"security.protocol=SASL_PLAINTEXT\",\n          \"sasl.mechanism=GSSAPI\",\n          \"sasl.kerberos.service.name=kafka-service\"\n        ]\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"03-job-configuration/02-Schemas/","title":"Schemas Configuration","text":"<p>Schemas are used in Data Quality jobs for two purposes:</p> <ul> <li>Provide explicit schema when reading sources without embedded schemas such as delimited or fixed width text files.</li> <li>Provide reference schema to validate actual source schema. Schemas are used in <code>schemaMatch</code> load checks.   See Schema Match Check.</li> </ul> <p>Schemas are set in <code>schemas</code> section of job configuration and can be defined in different formats as described below. Format in which schema is defined is set in <code>kind</code> field and defines what other fields are need to be provided.</p>"},{"location":"03-job-configuration/02-Schemas/#delimited-schema-configuration","title":"Delimited Schema Configuration","text":"<p>This kind of schema definition is primarily used to provide schemas for delimited text files such as CSV or TSV. Nevertheless, these schemas can be used for <code>schemaMatch</code> load checks as well. Using this type of configuration,  only flat schemas can be defined (nested columns are not allowed).</p> <p>Thus, delimited definition contains following parameters:</p> <ul> <li><code>kind: \"delimited\"</code> - Required. Sets delimited schema definition format.</li> <li><code>id</code> - Required. Schema ID;</li> <li><code>schema</code> - Required. List of schema columns where each column is an object with following fields:<ul> <li><code>name</code> - Required. Name of the column;</li> <li><code>type</code> - Required. Type of the column. See Supported Type Literals for allowed types.</li> </ul> </li> </ul>"},{"location":"03-job-configuration/02-Schemas/#fixed-full-schema-configuration","title":"Fixed-Full Schema Configuration","text":"<p>Fixed-full kind of schema definition is used to provide schemas for read fixed-width text files. The key difference from other schema definitions is that columns widths are also provided which is crucial information for parsing fixed-width files. This kind of schemas may also be used for reading delimited files and for reference in <code>schemaMatch</code> load checks. Using this type of configuration, only flat schemas can be defined (nested columns are not allowed).</p> <p>Fixed-fill schema definition contains following parameters:</p> <ul> <li><code>kind: \"fixedFull\"</code> - Required. Sets fixed-full schema definition format.</li> <li><code>id</code> - Required. Schema ID;</li> <li><code>schema</code> - Required. List of schema columns where each column is an object with following fields:<ul> <li><code>name</code> - Required. Name of the column;</li> <li><code>type</code> - Required. Type of the column. See Supported Type Literals for allowed types.</li> <li><code>width</code> - Required. Integer width of column (number of symbols).</li> </ul> </li> </ul>"},{"location":"03-job-configuration/02-Schemas/#fixed-short-schema-configuration","title":"Fixed-Short Schema Configuration","text":"<p>Fixed-short kind of schema definition provides a more compact syntax for defining schemas used for reading fixed-width files. The columns are defined by their name and width only. Subsequently, all columns will have StringType. This kind of schemas may also be used for reading delimited files and for reference in <code>schemaMatch</code> load checks. Using this type of configuration, only flat schemas can be defined (nested columns are not allowed).</p> <p>Fixed-short schema definition contains following parameters:</p> <ul> <li><code>kind: \"fixedShort\"</code> - Required. Sets fixed-short schema definition format.</li> <li><code>id</code> - Required. Schema ID;</li> <li><code>schema</code> - Required. List of schema columns where each column is a string in format <code>columnName:columnWidth</code>.   Type of columns is always a StringType.</li> </ul>"},{"location":"03-job-configuration/02-Schemas/#avro-schema-configuration","title":"Avro Schema Configuration","text":"<p>Avro kind of schema configuration is used to read schema from file with avro schema <code>.avsc</code>. Thus, schema read from  avro schema file can be used to read both, avro files and delimited text files as well as be used as reference  in <code>schemaMatch</code> load checks. In addition, avro schema format supports complex schemas with nested columns.</p> <p>In order to read schema from avro file it is required to supply following parameters:</p> <ul> <li><code>kind: \"avro\"</code> - Required. Sets avro schema definition format.</li> <li><code>id</code> - Required. Schema ID;</li> <li><code>schema</code> - Required. Path to avro schema file <code>.avsc</code> to read schema from.</li> </ul>"},{"location":"03-job-configuration/02-Schemas/#hive-schema-configuration","title":"Hive Schema Configuration","text":"<p>Hive catalogue can be used as a source of schemas. Hive kind of schema definition is intended to retrieve schemas from hive tables. These schemas can be used to read both, avro files and delimited text files as well as be used as reference in <code>schemaMatch</code> load checks.</p> <p>To retrieve schema from hive table it is required to set up following parameters:</p> <ul> <li><code>kind: \"hive\"</code> - Required. Sets hive schema definition format.</li> <li><code>id</code> - Required. Schema ID;</li> <li><code>schema</code> - Required. Hive schema to search for a table.</li> <li><code>table</code> - Required. Hive table to retrieve schema from.</li> <li><code>excludeColumns</code> - Optional. List of column names to exclude from schema. Sometimes it is required, e.g.   to exclude partition columns from schema.</li> </ul>"},{"location":"03-job-configuration/02-Schemas/#supported-type-literals","title":"Supported Type Literals","text":"<p>The following type literals are supported when defining schema columns in job configuration file:</p> <ul> <li><code>string</code></li> <li><code>boolean</code></li> <li><code>date</code></li> <li><code>timestamp</code></li> <li><code>integer (32-bit integer)</code></li> <li><code>long (64-bit integer)</code></li> <li><code>short (16-bit integer)</code></li> <li><code>byte (signed integer in a single byte)</code></li> <li><code>double</code></li> <li><code>float</code></li> <li><code>decimal(precision, scale)</code> (precision &lt;= 38; scale &lt;= precision)</li> </ul>"},{"location":"03-job-configuration/02-Schemas/#schemas-configuration-example","title":"Schemas Configuration Example","text":"<p>As it is shown in the example below, <code>schemas</code> section represent a list of schema definitions of various kinds.</p> <pre><code>jobConfig: {\n  schemas: [\n    {\n      id: \"schema1\"\n      kind: \"delimited\"\n      schema: [\n        {name: \"colA\", type: \"string\"},\n        {name: \"colB\", type: \"timestamp\"},\n        {name: \"colC\", type: \"decimal(10, 3)\"}\n      ]\n    }\n    {\n      id: \"schema2\"\n      kind: \"fixedFull\",\n      schema: [\n        {name: \"col1\", type: \"integer\", width: 5},\n        {name: \"col2\", type: \"double\", width: 6},\n        {name: \"col3\", type: \"boolean\", width: 4}\n      ]\n    }\n    {id: \"schema3\", kind: \"fixedShort\", schema: [\"colOne:5\", \"colTwo:7\", \"colThree:9\"]}\n    {id: \"hive_schema\", kind: \"hive\", schema: \"some_schema\", table: \"some_table\"}\n    {id: \"avro_schema\", kind: \"avro\", schema: \"path/to/avro_schema.avsc\"}\n  ]\n}\n</code></pre>"},{"location":"03-job-configuration/03-Sources/","title":"Sources Configuration","text":"<p>Reading sources is one of the major part of Data Quality job. During job execution, Checkita will read all sources into  a Spark DataFrames, that will be later processed to calculate metrics and perform quality checks. In addition,  dataframes' metadata is used to perform all types of load checks in order to ensure that source has the structure  as expected.</p> <p>Generally, sources can be read from file systems or object storage that Spark is connected to such as HDFS or S3. In additional, table-like source from Hive catalogue can be read. Apart from integrations natively supported by Spark, Checkita can read sources from external systems such as RDBMS or Kafka. For this purpose it is required to define connections to these systems in a first place. See Connections Configuration chapter for more  details on connections configurations.</p> <p>Thus, currently Checkita supports four general types of sources:</p> <ul> <li>File sources: read files from local or remote file systems (HDFS, S3, etc.);</li> <li>Hive sources: read hive table from Hive catalogue;</li> <li>Table sources: read tables from RDBMS via JDBC connection.</li> <li>Kafka sources: read topics from Kafka.</li> </ul> <p>All sources must be defined in <code>sources</code> section of job configuration.  More details on how to configure sources of each of these types are shown below. Example of <code>sources</code> section of  job configuration is shown in Sources Configuration Example below.</p>"},{"location":"03-job-configuration/03-Sources/#file-sources-configuration","title":"File Sources Configuration","text":"<p>Currently, there are five file types that Checkita can read as a source. These are:</p> <ul> <li>Fixed-width text files;</li> <li>Delimited text files (CSV, TSV);</li> <li>ORC files;</li> <li>Parquet files;</li> <li>Avro files.</li> </ul> <p>When configuring file source, it is mandatory to indicate its type. Subsequently, configuration parameters may  vary for files of different types.</p> <p>Common parameters for sources of any file type are:</p> <ul> <li><code>id</code> - Required. Source ID;</li> <li><code>kind</code> - Required. File type. Can be one of the following: <code>fixed</code>, <code>delimited</code>, <code>orc</code>, <code>parquet</code>, <code>avro</code>;</li> <li><code>path</code> - Required. File path. Can be a path to a directory or a S3-bucket. In this case all files from this   directory/bucket will be read (assuming they all have the same schema). Note, that when reading from file system which   is not spark default file system, it is required to add FS prefix to the path, e.g. <code>file://</code> to read from local FS,    or <code>s3a://</code> to read from S3.</li> <li><code>keyFields</code> - Optional. List of columns that form a Primary Key or are used to identify row within a dataset.   Key fields are primarily used in error collection reports. For more details on error collection, see    Metric Error Collection chapter.</li> </ul>"},{"location":"03-job-configuration/03-Sources/#fixed-width-file-sources","title":"Fixed Width File Sources","text":"<p>In order to read fixed-width file it is additionally required to provide ID of the schema used to parse file content. Schema itself should be defined in <code>schemas</code> section of job configuration as described in  Schemas Configuration chapter. </p> <ul> <li><code>schema</code> - Required. Schema ID used to parse fixed-width file.    The schema definition type should be either <code>fixedFull</code> or <code>fixedShort</code></li> </ul>"},{"location":"03-job-configuration/03-Sources/#delimited-file-sources","title":"Delimited File Sources","text":"<p>When reading delimited text file, its schema may be inferred from file header if it is presented in the file or  may be explicitly defined in <code>schemas</code> section of job configuration file  as described in Schemas Configuration chapter.</p> <p>Thus, additional parameters for configuring delimited file source are:</p> <ul> <li><code>schema</code> - Optional. Schema ID used to parse delimited file text file. It is possible to use schema of any   definition type as long as it has flat structure (nested columns are not supported for delimited text files).</li> <li><code>header</code> - Optional, default is <code>false</code>. Boolean parameter indicating whether schema should be inferred    from file header.</li> <li><code>delimiter</code> - Optional, default is <code>,</code>. Column delimiter.</li> <li><code>quote</code> - Optional, default is <code>\"</code>. Column enclosing character.</li> <li><code>escape</code> - Optional, default is <code>\\</code>. Escape character.</li> </ul> <p>IMPORTANT: If the <code>header</code> parameter is absent or set to<code>false</code>, then <code>schema</code> parameter must be set. And vice versa, if <code>header</code> parameter is set to <code>true</code>, then <code>schema</code> parameter must not be set. In other words, schema may be inferred from file header or be explicitly defined, but not both.</p>"},{"location":"03-job-configuration/03-Sources/#avro-file-sources","title":"Avro File Sources","text":"<p>Avro files can contain schema in its header. Therefore, there are two options to read avro files: either infer schema from file or provide it explicitly. In the second case, schema must be defined in <code>schemas</code> section of job  configuration file  as described in Schemas Configuration chapter. Therefore, there is only one  additional parameter for avro file source configuration:</p> <ul> <li><code>schema</code> - Optional. Schema ID used to read avro file. It is possible to use schema of any   definition type.</li> </ul>"},{"location":"03-job-configuration/03-Sources/#orc-file-sources","title":"ORC File Sources","text":"<p>As ORC format contains schema within itself, then there are no additional parameters required to read ORC files.</p>"},{"location":"03-job-configuration/03-Sources/#parquet-file-sources","title":"Parquet File Sources","text":"<p>As Parquet format contains schema within itself, then there are no additional parameters required to read Parquet files.</p>"},{"location":"03-job-configuration/03-Sources/#hive-sources-configuration","title":"Hive Sources Configuration","text":"<p>In order to read data from Hive table it is required to provide following:</p> <ul> <li><code>id</code> - Required. Source ID;</li> <li><code>schema</code> - Required. Hive schema.</li> <li><code>table</code> - Required. Hive table.</li> <li><code>partitions</code> - Optional. List of partitions to read where each element is an object with following fields.   If partitions are not set then entire table is read.<ul> <li><code>name</code> - Required. Partition column name</li> <li><code>values</code> - Required. List of partition column name values to read.</li> </ul> </li> <li><code>keyFields</code> - Optional. List of columns that form a Primary Key or are used to identify row within a dataset.   Key fields are primarily used in error collection reports. For more details on error collection, see   Metric Error Collection chapter.</li> </ul>"},{"location":"03-job-configuration/03-Sources/#table-sources-configuration","title":"Table Sources Configuration","text":"<p>Table source are read from supported RDBMS via JDBC connection. There are two options to read data from RDBMS:</p> <ul> <li>read entire table content;</li> <li>execute query on the RDBMS side and read only query result.</li> </ul> <p>In order to set up table source, it is required to supply following parameters:</p> <ul> <li><code>id</code> - Required. Source ID;</li> <li><code>connection</code> - Required. Connection ID to use for table source. Connection ID must refer to connection configuration   for one of the supported RDBMS. See Connections Configuration chapter for more information.</li> <li><code>table</code> - Optional. Table to read.</li> <li><code>query</code> - Optional. Query to execute. Query result is read as table source.</li> <li><code>keyFields</code> - Optional. List of columns that form a Primary Key or are used to identify row within a dataset.   Key fields are primarily used in error collection reports. For more details on error collection, see   Metric Error Collection chapter.</li> </ul> <p>IMPORTANT: Either <code>table</code> to read from must be specified or <code>query</code> to execute, but not both. In addition, using queries is only allowed when <code>allowSqlQueries</code> is set to true. Otherwise, any usage of arbitrary SQL queries will not be permitted. See Enablers chapter for more information.</p> <p>TIP: HOCON format supports multiline string values. In order to define such a value, it is required to enclose string in triple quotes, e.g.: <pre><code>multilineString: \"\"\"\n  SELECT * from schema.table\n  WHERE load_date = '2023-08-23';\n\"\"\"\n</code></pre></p>"},{"location":"03-job-configuration/03-Sources/#kafka-sources-configuration","title":"Kafka Sources Configuration","text":"<p>Despite, it is not common situation to read messages from Kafka topics in batch-mode, such feature is presented in  Checkita framework. In order to set up source that reads from Kafka topic/s, it is required to provide following parameters:</p> <ul> <li><code>id</code> - Required. Source ID;</li> <li><code>connection</code> - Required. Connection ID to use for kafka source. Connection ID must refer to Kafka connection    configuration. See Connections Configuration chapter for more information.</li> <li><code>topics</code> - Optional. List of topics to read. Topics can be specified in either of two formats:<ul> <li>List of topics without indication of partitions to read (read all topic partitions): <code>[\"topic1\", \"topic2\"]</code>;</li> <li>List of topics with indication of partitions to read: <code>[\"topic1@[0, 1]\", \"topic2@[2, 4]\"]</code></li> <li>All topics must be defined using the same format.</li> </ul> </li> <li><code>topicPattern</code> - Optional. Topic pattern name: read all topics that match pattern.</li> <li><code>format</code> - Topic format. Currently, only <code>xml</code> and <code>json</code> formats are supported.</li> <li><code>startingOffsets</code> - Optional, default is <code>earliest</code>. Json string setting starting offsets to read from topic.   By default, all topic is read.</li> <li><code>endingOffsets</code> - Optional, default is <code>latest</code>. Json string setting ending offset until which to read from topic.   By default, read topic till the end.</li> <li><code>options</code> - Optional. Additional Spark parameters related to reading messages from Kafka topics such as:   <code>failOnDataLoss, kafkaConsumer.pollTimeoutMs, fetchOffset.numRetries, fetchOffset.retryIntervalMs, maxOffsetsPerTrigger</code>.   Parameters are provided as a strings in format of <code>parameterName=parameterValue</code>.   For more information, see Spark Kafka Integration Guide.</li> <li><code>keyFields</code> - Optional. List of columns that form a Primary Key or are used to identify row within a dataset.   Key fields are primarily used in error collection reports. For more details on error collection, see   Metric Error Collection chapter.</li> </ul> <p>TIP: In order to define JSON strings, they must be enclosed in triple quotes: <code>\"\"\"{\"name1\": {\"name2\": \"value2\", \"name3\": \"value3\"\"}}\"\"\"</code>.</p>"},{"location":"03-job-configuration/03-Sources/#sources-configuration-example","title":"Sources Configuration Example","text":"<p>As it is shown in the example below, sources of the same type are grouped within subsections named after the type of the source. These subsections should contain a list of source configurations of the corresponding type.</p> <pre><code>  sources: {\n    file: [\n      {id: \"hdfs_fixed_file\", kind: \"fixed\", path: \"path/to/fixed/file.txt\", schema: \"schema2\"}\n      {\n        id: \"hdfs_delimited_source\",\n        kind: \"delimited\",\n        path: \"path/to/csv/file.csv\"\n        schema: \"schema1\"\n      }\n      {id: \"hdfs_avro_source\", kind: \"avro\", path: \"path/to/avro/file.avro\", schema: \"avro_schema\"}\n      {id: \"hdfs_orc_source\", kind: \"orc\", path: \"path/to/orc/file.orc\"}\n    ]\n    hive: [\n      {\n        id: \"hive_source_1\", schema: \"some_schema\", table: \"some_table\",\n        partitions: [{name: \"load_date\", values: [\"2023-06-30\", \"2023-07-01\"]}],\n        keyFields: [\"id\", \"name\"]\n      }\n    ]\n    table: [\n      {id: \"table_source_1\", connection: \"oracle_db1\", table: \"some_table\", keyFields: [\"id\", \"name\"]}\n      {id: \"table_source_2\", connection: \"sqlite_db\", table: \"other_table\"}\n    ]\n    kafka: [\n      {\n        id: \"kafka_source_1\",\n        connection: \"kafka_broker\",\n        topics: [\"topic1.pub\", \"topic2.pub\"]\n        format: \"json\"\n      }\n      {\n        id: \"kafka_source_2\",\n        brokerId: \"kafka_broker\",\n        topics: [\"topic3.pub@[1,3]\"]\n        startingOffsets: \"\"\"{\"topic3.pub\":{\"1\":1234,\"3\":2314}}\"\"\"\n        options: [\"kafkaConsumer.pollTimeoutMs=300000\"]\n        format: \"json\"\n      }\n    ]\n  }\n</code></pre>"},{"location":"03-job-configuration/04-VirtualSources/","title":"Virtual Sources Configuration","text":"<p>Checkita framework supports creation of virtual (temporary) sources base on regular once (defined in <code>sources</code> section of job configuration, as described in Sources Configuration chapter). Virtual sources are created by applying transformations to existing sources using Spark SQL API. Subsequently, metrics and checks can also be applied to virtual sources.</p> <p>It is also important to note, that virtual sources are created recursively, therefore, once virtual source is created it can be used to create another one in the same way as regular sources.</p> <p>The following types of virtual sources are supported:</p> <ul> <li><code>SQL</code>: enables creation of virtual source from existing once using arbitrary SQL query.</li> <li><code>Join</code>: creates virtual source by joining two (and only 2) existing sources.</li> <li><code>Filter</code>: creates virtual source from existing one by applying filter expression.</li> <li><code>Select</code>: creates virtual source from existing one by applying select expression.</li> <li><code>Aggregate</code>: creates virtual source by applying groupBy and aggregate operations to existing one.</li> </ul> <p>All types of virtual sources have common features:</p> <ul> <li>It is possible to cache virtual sources in memory or on disk. This could be handful when virtual sources is used as   parent for more than one virtual source. In such cases caching virtual source allows not to calculate it multiple times.</li> <li>Virtual source can be saved as a file in one of the supported format. This feature can be used for debugging purposes   or just to keep data transformations applied during quality checks.</li> </ul> <p>Thus, virtual sources are defined in <code>virtualSources</code> section of job configuration and have following common parameters:</p> <ul> <li><code>id</code> - Required. Virtual source ID;</li> <li><code>parentSources</code> - Required. List of parent sources to use for creation of virtual sources. There could be a   limitations imposed in number of parent sources, depending on virtual source type.</li> <li><code>persist</code> - Optional. One of the allowed Spark StorageLevels used to cache virtual sources. By default, virtual   sources are not cached. Supported Spark StorageLevels are:<ul> <li><code>NONE</code>, <code>DISK_ONLY</code>, <code>DISK_ONLY_2</code>, <code>MEMORY_ONLY</code>, <code>MEMORY_ONLY_2</code>, <code>MEMORY_ONLY_SER</code>,   <code>MEMORY_ONLY_SER_2</code>, <code>MEMORY_AND_DISK</code>, <code>MEMORY_AND_DISK_2</code>, <code>MEMORY_AND_DISK_SER</code>,   <code>MEMORY_AND_DISK_SER_2</code>, <code>OFF_HEAP</code>.</li> </ul> </li> <li><code>save</code> - Optional. File output configuration used to save virtual source. By default, virtual sources are not saved.   For more information on configuring file outputs, see File Output Configuration chapter.</li> <li><code>keyFields</code> - Optional. List of columns that form a Primary Key or are used to identify row within a dataset.   Key fields are primarily used in error collection reports. For more details on error collection, see   Metric Error Collection chapter.</li> </ul>"},{"location":"03-job-configuration/04-VirtualSources/#sql-virtual-source-configuration","title":"SQL Virtual Source Configuration","text":"<p><code>SQL</code> type of virtual sources is allowed only when <code>allowSqlQueries</code> is set to true. Otherwise, any usage of arbitrary SQL queries will not be permitted. See Enablers chapter for more information. At the same time, there is no limitation on number of parent sources used to create  SQL virtual source.</p> <p>In order to define SQL virtual source, it is required to provide an SQL query:</p> <ul> <li><code>kind: \"sql\"</code> - Required. Sets <code>SQL</code> virtual source type.</li> <li><code>query</code> - Required. SQL query to build virtual source. Existing sources are referred in SQL query by their IDs.</li> </ul>"},{"location":"03-job-configuration/04-VirtualSources/#join-virtual-source-configuration","title":"Join Virtual Source Configuration","text":"<p>In order to define <code>Join</code> type of virtual sources, it is required to provided two (and only two) parent sources  that are being joined as well as type of the join to use and list of column to join by. Note, that in order to  perform join, parent sources should have matching column names to join by. Join by condition is not currently supported:</p> <ul> <li><code>kind: \"join\"</code> - Required. Sets <code>Join</code> virtual source type.</li> <li><code>joinBy</code> - Required. List of columns to join by. Thus, parent sources must have the same columns names used for join.</li> <li><code>joinType</code> - Required. Type of Spark join to apply. Following join types are supported:<ul> <li><code>inner</code>, <code>outer</code>, <code>cross</code>, <code>full</code>, <code>right</code>, <code>left</code>, <code>semi</code>, <code>anti</code>,    <code>fullOuter</code>,  <code>rightOuter</code>, <code>leftOuter</code>, <code>leftSemi</code>, <code>leftAnti</code></li> </ul> </li> </ul>"},{"location":"03-job-configuration/04-VirtualSources/#filter-virtual-source-configuration","title":"Filter Virtual Source Configuration","text":"<p><code>Filter</code> virtual source is defined by applying sequence of filter expressions to parent source. Thus, only one parent source must be supplied to this type of virtual source configuration:</p> <ul> <li><code>kind: \"filter\"</code> - Required. Sets <code>Filter</code> virtual source type.</li> <li><code>expr</code> - Required. Sequence of filter SQL expressions applied to parent source.</li> </ul>"},{"location":"03-job-configuration/04-VirtualSources/#select-virtual-source-configuration","title":"Select Virtual Source Configuration","text":"<p><code>Select</code> virtual source is defined by applying sequence of select expression to parent source. Each select expression  should yield a new column. Thus, the number of columns in the virtual source correspond to number of provided select expressions. Subsequently, only one parent source must be supplied to this type of virtual source configuration:</p> <ul> <li><code>kind: \"select\"</code> - Required. Sets <code>Select</code> virtual source type.</li> <li><code>expr</code> - Required. Sequence of select SQL expressions applied to parent source.</li> </ul>"},{"location":"03-job-configuration/04-VirtualSources/#aggregate-virtual-source-configuration","title":"Aggregate Virtual Source Configuration","text":"<p><code>Aggregate</code> virtual source is defined by applying groupBy and aggregate operations to parent source. Thus, it is  required to provide a list of columns used to group rows as well as list of aggregate operations in form of SQL  expressions used to create columns with aggregated results. Thus, the number of columns in the virtual source  correspond to number of provided aggregate expressions. Subsequently, only one parent source must be supplied  to this type of virtual source configuration:</p> <ul> <li><code>kind: \"aggregate\"</code> - Required. Sets <code>Aggregate</code> virtual source type.</li> <li><code>groupBy</code> - Required. Sequence of columns used to group rows from parent source.</li> <li><code>expr</code> - Required. Sequence of SQL expressions used to get columns with aggregated results. </li> </ul>"},{"location":"03-job-configuration/04-VirtualSources/#virtual-sources-configuration-example","title":"Virtual Sources Configuration Example","text":"<p>As it is shown in the example below, <code>virtualSources</code> section represent a list of virtual source definitions of various kinds.</p> <pre><code>jobConfig: {\n  virtualSources: [\n    {\n      id: \"sqlVS\"\n      kind: \"sql\"\n      parentSources: [\"hive_source_1\"]\n      persist: \"disk_only\"\n      save: {\n        kind: \"orc\"\n        path: \"some/path/to/vs/location\"\n      }\n      query: \"select id, name, entity, description from hive_source_1 where load_date == '2023-06-30'\"\n    }\n    {\n      id: \"joinVS\"\n      kind: \"join\"\n      parentSources: [\"hdfs_avro_source\", \"hdfs_orc_source\"]\n      joinBy: [\"id\"]\n      joinType: \"leftouter\"\n      persist: \"memory_only\"\n      keyFields: [\"id\", \"order_id\"]\n    }\n    {\n      id: \"filterVS\"\n      kind: \"filter\"\n      parentSources: [\"kafka_source\"]\n      expr: [\"key is not null\"]\n      keyFields: [\"orderId\", \"dttm\"]\n    }\n    {\n      id: \"selectVS\"\n      kind: \"select\"\n      parentSources: [\"table_source_1\"]\n      expr: [\n        \"count(id) as id_cnt\",\n        \"sum(amount) as total_amount\"\n      ]\n    }\n    {\n      id: \"aggVS\"\n      kind: \"aggregate\"\n      parentSources: [\"hdfs_fixed_file\"]\n      groupBy: [\"col1\"]\n      expr: [\n        \"avg(col2) as avg_col2\",\n        \"sum(col3) as sum_col3\"\n      ],\n      keyFields: [\"col1\", \"avg_col2\", \"sum_col3\"]\n    }\n  ]\n}\n</code></pre>"},{"location":"03-job-configuration/05-LoadChecks/","title":"Load Checks Configuration","text":"<p>Load checks are the special type of checks that are distinguished from other checks as they are applied not to results of metrics computation but to sources metadata. Other key feature of load checks is that they are run prior actual data loading from the sources what is possible due Spark lazy evaluation mechanisms: sources are, essentially, Spark dataframes and load checks are used to verify their metadata.</p> <p>Load checks are defined in <code>loadChecks</code> section of job configuration and have following common parameters:</p> <ul> <li><code>id</code> - Required. Load check ID;</li> <li><code>source</code> - Required. Reference to a source ID which metadata is being checked;</li> </ul> <p>Currently, supported load checks are described below as well as configuration parameters specific to them.</p>"},{"location":"03-job-configuration/05-LoadChecks/#minimum-column-number-check","title":"Minimum Column Number Check","text":"<p>This check is used to verify if number of columns in the source is equal to or greater than specified number. Load checks of this type are configured in the <code>minColumnNum</code> subsection of the <code>loadChecks</code> section. In addition to common parameters, following parameters should be specified:</p> <ul> <li><code>option</code> - Required. Minimum number of columns that checked source must contain.</li> </ul>"},{"location":"03-job-configuration/05-LoadChecks/#exact-column-number-check","title":"Exact Column Number Check","text":"<p>This check is used to verify if number of columns in the source is exactly equal to specified number. Load checks of this type are configured in the <code>exactColumnNum</code> subsection of the <code>loadChecks</code> section. In addition to common parameters, following parameters should be specified:</p> <ul> <li><code>option</code> - Required. Required number of columns that checked source must contain.</li> </ul>"},{"location":"03-job-configuration/05-LoadChecks/#columns-existence-check","title":"Columns Existence Check","text":"<p>This check is used to verify if source contains columns with required names.  Load checks of this type are configured in the <code>columnsExist</code> subsection of the <code>loadChecks</code> section. In addition to common parameters, following parameters should be specified:</p> <ul> <li><code>columns</code> - Required. List of column names that must exists in checked source.</li> </ul>"},{"location":"03-job-configuration/05-LoadChecks/#schema-match-check","title":"Schema Match Check","text":"<p>This check is used to verify if source schema matches predefined reference schema. Reference schema must be defined  in <code>schemas</code> section of configuration files as described in Schemas Configuration chapter. Load checks of this type are configured in the <code>schemaMatch</code> subsection of the <code>loadChecks</code> section. In addition to common parameters, following parameters should be specified:</p> <ul> <li><code>schema</code> - Required. Reference Schema ID which should be used for comparison with source schema.</li> <li><code>ignoreOrder</code> - Optional, default is <code>false</code>. Boolean parameter indicating whether columns order should be ignored   during comparison of the schemas.</li> </ul>"},{"location":"03-job-configuration/05-LoadChecks/#load-checks-configuration-example","title":"Load Checks Configuration Example","text":"<p>As it is shown in the example below, load checks of the same type are grouped within subsections named after the type of the load check. These subsections should contain a list of load checks configurations of the corresponding type.</p> <pre><code>jobConfig: {\n  loadChecks: {\n    minColumnNum: [\n      {id: \"load_check_1\", source: \"kafka_source\", option: 2}\n    ]\n    exactColumnNum: [\n      {id: \"load_check_2\", source: \"hdfs_delimited_source\", option: 3}\n    ]\n    columnsExist: [\n      {id: \"loadCheck3\", source: \"sqlVS\", columns: [\"id\", \"name\", \"entity\", \"description\"]},\n      {id: \"load_check_4\", source: \"hdfs_delimited_source\", columns: [\"id\", \"name\", \"value\"]}\n    ]\n    schemaMatch: [\n      {id: \"load_check_5\", source: \"kafka_source\", schema: \"hive_schema\"}\n    ]\n  }\n}\n</code></pre>"},{"location":"03-job-configuration/06-Metrics/","title":"Metrics Configuration","text":"<p>Calculation of various metrics over the data is the main part of Data Quality job. Metrics allow evaluation of  various indicators that describe data from both technical and business points of view. Indicators in their turn can signal about problems in the data.</p> <p>All metrics are linked to a source over which they are calculated. Such metrics are called <code>regular</code>. Apart from regular metrics there is a special kind of metrics that can be calculated based on other metrics results thus allowing metric compositions. These metrics are called <code>composed</code> accordingly.</p> <p>Metrics are defined in <code>metrics</code> section of job configuration. Regular metrics are grouped by their type in <code>regular</code> subsection while composed metrics are listed in <code>composed</code> subsection.</p>"},{"location":"03-job-configuration/06-Metrics/#regular-metrics","title":"Regular Metrics","text":"<p>All regular metrics are defined using following common parameters: </p> <ul> <li><code>id</code> - Required. Metric ID;</li> <li><code>description</code> - Optional. Metric description.</li> <li><code>source</code> - Required. Reference to a source ID over which metric is caclulated;</li> <li><code>columns</code> - Required. List of columns over which metric is calculated. Regular metrics can be calculated for    multiple columns. This means that the result of the metrics will be calculated for row values in these columns.   There could be a limitation imposed on number of columns which metric can process.    The only exception is Row Count Metric which does not need columns to be specified.</li> <li><code>params</code> - Some of the metrics may require additional parameters to be set. They should be specified within this   object. The details on what parameters should be configured for metric are given below for each metric individually.   Some metric definitions that require additional parameters are also have their default values set. In this case,   <code>params</code> object can be omitted to use default options for all parameters.</li> </ul> <p>Additionally, some regular metrics have a logical condition that needs to be met when calculating metric increment per each individual row. If metric condition is not met, then <code>Failure</code> status is returned for this particular row of data. Scenario when metric can yield <code>Failure</code> status are explicitly described for each metric below. See Status Model used in Results chapter for more information on status model.</p>"},{"location":"03-job-configuration/06-Metrics/#row-count-metric","title":"Row Count Metric","text":"<p>Calculates number of rows in the source. This is the only metric for which columns list should not be specified as it is not required to compute number of rows. Metric definition does not require additional parameters: <code>params</code> should not be set.</p> <p>All row count metrics are defined in <code>rowCount</code> subsection.</p>"},{"location":"03-job-configuration/06-Metrics/#distinct-values-metric","title":"Distinct Values Metric","text":"<p>Counts number of unique values in provided columns. When applied to multiple columns, total number of unique values in these columns is returned. Metric definition does not require additional parameters: <code>params</code> should not be set.</p> <p>All distinct values metrics are defined in <code>distinctValues</code> subsection.</p> <p>IMPORTANT. Calculation of exact number of unique values required O(N) memory. Therefore, to prevent OOM errors when working with extremely large dataset and with high-cardinality columns it is recommended to use Approximate Distinct Values Metric which uses HLL probabilistic algorithm to estimate number of unique values.</p>"},{"location":"03-job-configuration/06-Metrics/#approximate-distinct-values-metric","title":"Approximate Distinct Values Metric","text":"<p>Calculates number of unique values approximately, using  HyperLogLog algorithm.</p> <p>This metric works with only one column.</p> <p>All approximate distinct values metrics are defined in <code>approximateDistinctValues</code> subsection.  Additional parameters can be supplied:</p> <ul> <li><code>accuracyError</code> - Optional, default is <code>0.01</code>. Accuracy error for estimating number of unique values.</li> </ul>"},{"location":"03-job-configuration/06-Metrics/#null-values-metrics","title":"Null Values Metrics","text":"<p>Counts number of null values in the specified columns. When applied to multiple columns, total number of null values in these columns is returned. Metric definition does not require additional parameters: <code>params</code> should not be set.</p> <p>All distinct values metrics are defined in <code>nullValues</code> subsection.</p> <p>Metric increment returns <code>Failure</code> status for rows where some values in the specified columns are null.</p>"},{"location":"03-job-configuration/06-Metrics/#empty-values-metric","title":"Empty Values Metric","text":"<p>Counts number of empty values in the specified columns (i.e. empty string values). When applied to multiple columns, total number of empty values in these columns is returned. Metric definition does not require additional parameters: <code>params</code> should not be set.</p> <p>All distinct values metrics are defined in <code>emptyValues</code> subsection.</p> <p>Metric increment returns <code>Failure</code> status for rows where some values in the specified columns are empty.</p>"},{"location":"03-job-configuration/06-Metrics/#completeness-metric","title":"Completeness Metric","text":"<p>Calculates the measure of completeness in the specified columns: <code>(values_count - null_count) / values_count</code>. When applied to multiple columns, total number of values and total number of nulls are used in the equation above.</p> <p>All completeness metrics are defined in <code>completeness</code> subsection. Additional parameters can be supplied:</p> <ul> <li><code>includeEmptyStrings</code> - Optional, default is <code>false</code>. Boolean parameter indicating whether empty string values   should be considered as nulls.</li> </ul>"},{"location":"03-job-configuration/06-Metrics/#sequence-completeness-metric","title":"Sequence Completeness Metric","text":"<p>Calculates measure of completeness of an incremental sequence of integers. In other words, it looks for the missing  elements in the sequence and returns the relation: <code>actual number of elements / required number of elements</code>.</p> <p>This metric works with only one column.</p> <p>The actual number of elements is just the number of unique values in the sequence. This metric defines it exactly, and therefore requires <code>O(N)</code> memory to store these values. Therefore, to prevent OOM errors for extremely large sequences, it is recommended to use the Approximate Sequence Completeness Metric, which uses HLL probabilistic algorithm to estimate number of unique values.</p> <p>The required number of elements is determined by the formula: <code>(max_value - min_value) / increment + 1</code>, Where: * <code>min_value</code> - the minimum value in the sequence; * <code>max_value</code> - the maximum value in the sequence; * <code>increment</code> - sequence step, default is 1.</p> <p>All sequence completeness metrics are defined in <code>sequenceCompleteness</code> subsection. Additional parameters can be supplied:</p> <ul> <li><code>incremet</code> - Optional, default is <code>1</code>. Sequence increment step.</li> </ul>"},{"location":"03-job-configuration/06-Metrics/#approximate-sequence-completeness-metric","title":"Approximate Sequence Completeness Metric","text":"<p>Calculates the measure of completeness of an incremental sequence of integers approximately using  the HyperLogLog algorithm. Works in the same  way is Sequence Completeness Metric with only difference, that actual number of elements in the sequence is determined approximately using HLL algorithm.</p> <p>This metric works with only one column.</p> <p>All approximate sequence completeness metrics are defined in <code>approximateSequenceCompleteness</code> subsection. Additional parameters can be supplied:</p> <ul> <li><code>incremet</code> - Optional, default is <code>1</code>. Sequence increment step.</li> <li><code>accuracyError</code> - Optional, default is <code>0.01</code>. Accuracy error for estimating number of unique values.</li> </ul>"},{"location":"03-job-configuration/06-Metrics/#minimum-string-metric","title":"Minimum String Metric","text":"<p>Calculates the minimum string length in the values of the specified columns.  Metric definition does not require additional parameters: <code>params</code> should not be set.</p> <p>All minimum string metrics are defined in <code>minString</code> subsection.</p> <p>Metric increment returns <code>Failure</code> status for rows where all values in the specified columns are not castable  to string and, therefore, minimum string length cannot be computed.</p>"},{"location":"03-job-configuration/06-Metrics/#maximum-string-metric","title":"Maximum String Metric","text":"<p>Calculates the maximum string length in the values of the specified columns. Metric definition does not require additional parameters: <code>params</code> should not be set.</p> <p>All maximum string metrics are defined in <code>maxString</code> subsection.</p> <p>Metric increment returns <code>Failure</code> status for rows where all values in the specified columns are not castable to string and, therefore, maximum string length cannot be computed.</p>"},{"location":"03-job-configuration/06-Metrics/#average-string-metric","title":"Average String Metric","text":"<p>Calculates the average string length in the values of the specified columns. Metric definition does not require additional parameters: <code>params</code> should not be set.</p> <p>All average string metrics are defined in <code>avgString</code> subsection.</p> <p>Metric increment returns <code>Failure</code> status for rows where all values in the specified columns are not castable to string and, therefore, average string length cannot be computed.</p>"},{"location":"03-job-configuration/06-Metrics/#string-length-metric","title":"String Length Metric","text":"<p>Calculate number of values that meet the defined string length criteria.</p> <p>All string length metrics are defined in <code>stringLength</code> subsection. Additional parameters should be supplied:</p> <ul> <li><code>length</code> - Required. Required string length threshold.</li> <li><code>compareRule</code> - Required. Comparison rule used to compare actual value string length with threshold one.<ul> <li>Following comparison rules are supported: <code>eq</code> (==), <code>lt</code> (&lt;), <code>lte</code> (&lt;=), <code>gt</code> (&gt;), <code>gte</code> (&gt;=).</li> </ul> </li> </ul> <p>Metric increment returns <code>Failure</code> status for rows where some values in the specified columns do not meet defined string length criteria.</p>"},{"location":"03-job-configuration/06-Metrics/#string-in-domain-metric","title":"String In Domain Metric","text":"<p>Counts number of values which fall into specified set of allowed values.</p> <p>All string in domain metrics are defined in <code>stringInDomain</code> subsection. Additional parameters should be supplied:</p> <ul> <li><code>domain</code> - Required. List of allowed values.</li> </ul> <p>Metric increment returns <code>Failure</code> status for rows where some values in the specified columns do not fall into set of  allowed values.</p>"},{"location":"03-job-configuration/06-Metrics/#string-out-domain-metric","title":"String Out Domain Metric","text":"<p>Counts number of values which do not fall into specified set of avoided values.</p> <p>All string out domain metrics are defined in <code>stringOutDomain</code> subsection. Additional parameters should be supplied:</p> <ul> <li><code>domain</code> - Required. List of avoided values.</li> </ul> <p>Metric increment returns <code>Failure</code> status for rows where some values in the specified columns do fall into set of avoided values.</p>"},{"location":"03-job-configuration/06-Metrics/#string-values-metric","title":"String Values Metric","text":"<p>Counts number of values that are equal to the value given in metric definition.</p> <p>All string values metrics are defined in <code>stringValues</code> subsection. Additional parameters should be supplied:</p> <ul> <li><code>compareValue</code> - Required. String value to compare with.</li> </ul> <p>Metric increment returns <code>Failure</code> status for rows where some values in the specified columns do not match defined compare value.</p>"},{"location":"03-job-configuration/06-Metrics/#regex-match","title":"Regex Match","text":"<p>Calculates number of values that match the defined regular expression.</p> <p>All regex match metrics are defined in <code>regexMatch</code> subsection. Additional parameters should be supplied:</p> <ul> <li><code>regex</code> - Required. Regular expression to match.</li> </ul> <p>Metric increment returns <code>Failure</code> status for rows where some values in the specified columns do not match defined regular expression.</p>"},{"location":"03-job-configuration/06-Metrics/#regex-mismatch","title":"Regex Mismatch","text":"<p>Calculates number of values that do not match the defined regular expression.</p> <p>All regex mismatch metrics are defined in <code>regexMismatch</code> subsection. Additional parameters should be supplied:</p> <ul> <li><code>regex</code> - Required. Regular expression that values should not match.</li> </ul> <p>Metric increment returns <code>Failure</code> status for rows where some values in the specified columns do match defined regular expression.</p>"},{"location":"03-job-configuration/06-Metrics/#formatted-date-metric","title":"Formatted Date Metric","text":"<p>Counts number of values which have the specified datetime format.</p> <p>All formatted date metrics are defined in <code>formattedDate</code> subsection. Additional parameters can be supplied:</p> <ul> <li><code>dateFormat</code> - Optional, default is <code>yyyy-MM-dd'T'HH:mm:ss.SSSZ</code>. Target datetime format.    The datetime format must be specified as    Java DateTimeFormatter pattern.</li> </ul> <p>NOTE If the specified columns are of type <code>Timestamp</code>, it is assumed that they fit any datetime format and,  therefore,  metric will return the total number of non-empty cells.  Accordingly, the datetime format does not need to be specified.</p> <p>Metric increment returns <code>Failure</code> status for rows where some values in the specified columns do not conform to defined datetime format.</p>"},{"location":"03-job-configuration/06-Metrics/#formatted-number-metric","title":"Formatted Number Metric","text":"<p>Counts number of values which are numeric and number format satisfy defined number format criteria.</p> <p>All formatted date metrics are defined in <code>formattedNumber</code> subsection. Additional parameters should be supplied:</p> <ul> <li><code>precision</code> - Required. The total number of digits in the value (excluding the decimal separator).</li> <li><code>scale</code> - Required. Number of decimal digits in the value.</li> <li><code>compareRule</code> - Optional, default is <code>inbound</code>. Number format comparison rule:<ul> <li><code>inbound</code> - the value must \"fit\" into the specified number format:    actual precision and scale of the value are less than or equal to given ones.</li> <li><code>outbound</code> - the value must be outside the specified format:    actual precision and scale of the value are strictly greater than given ones.</li> </ul> </li> </ul> <p>Metric increment returns <code>Failure</code> status for rows where some values in the specified columns do not satisfy defined number format criteria.</p>"},{"location":"03-job-configuration/06-Metrics/#minimum-number-metric","title":"Minimum Number Metric","text":"<p>Finds minimum number from the values in the specified columns. Metric definition does not require additional parameters: <code>params</code> should not be set.</p> <p>All minimum number metrics are defined in <code>minNumber</code> subsection.</p> <p>Metric increment returns <code>Failure</code> status for rows where all values in the specified columns are not castable to number and, therefore, minimum number cannot be computed.</p>"},{"location":"03-job-configuration/06-Metrics/#maximum-number-metric","title":"Maximum Number Metric","text":"<p>Finds maximum number from the values in the specified columns. Metric definition does not require additional parameters: <code>params</code> should not be set.</p> <p>All maximum number metrics are defined in <code>maxNumber</code> subsection.</p> <p>Metric increment returns <code>Failure</code> status for rows where all values in the specified columns are not castable to number and, therefore, maximum number cannot be computed.</p>"},{"location":"03-job-configuration/06-Metrics/#sum-number-metric","title":"Sum Number Metric","text":"<p>Finds sum of the values in the specified columns. Metric definition does not require additional parameters: <code>params</code> should not be set.</p> <p>All sum number metrics are defined in <code>sumNumber</code> subsection.</p> <p>Metric increment returns <code>Failure</code> status for rows where some values in the specified columns are not castable to number.</p>"},{"location":"03-job-configuration/06-Metrics/#average-number-metric","title":"Average Number Metric","text":"<p>Finds average of the values in the specified column. Metric definition does not require additional parameters: <code>params</code> should not be set.</p> <p>This metric works with only one column.</p> <p>All average number metrics are defined in <code>avgNumber</code> subsection.</p> <p>Metric increment returns <code>Failure</code> status for rows where value in the specified column is not castable to number.</p>"},{"location":"03-job-configuration/06-Metrics/#standard-deviation-number-metric","title":"Standard Deviation Number Metric","text":"<p>Finds standard deviation for the values in the specified column. Metric definition does not require additional parameters: <code>params</code> should not be set.</p> <p>This metric works with only one column.</p> <p>All average number metrics are defined in <code>stdNumber</code> subsection.</p> <p>Metric increment returns <code>Failure</code> status for rows where value in the specified column is not castable to number.</p>"},{"location":"03-job-configuration/06-Metrics/#casted-number-metric","title":"Casted Number Metric","text":"<p>Counts number of values which string value can be converted to a number (double). Metric definition does not require additional parameters: <code>params</code> should not be set.</p> <p>All sum number metrics are defined in <code>castedNumber</code> subsection.</p> <p>Metric increment returns <code>Failure</code> status for rows where some values in the specified columns are not castable to number.</p>"},{"location":"03-job-configuration/06-Metrics/#number-in-domain-metric","title":"Number In Domain Metric","text":"<p>Counts number of values which being cast to number (double) fall into specified set of allowed numbers.</p> <p>All number in domain metrics are defined in <code>numberInDomain</code> subsection. Additional parameters should be supplied:</p> <ul> <li><code>domain</code> - Required. List of allowed numbers.</li> </ul> <p>Metric increment returns <code>Failure</code> status for rows where some values in the specified columns do not fall into set of allowed numbers.</p>"},{"location":"03-job-configuration/06-Metrics/#number-out-domain-metric","title":"Number Out Domain Metric","text":"<p>Counts number of values which being cast to number (double) do not fall into specified set of avoided numbers.</p> <p>All number out domain metrics are defined in <code>numberOutDomain</code> subsection. Additional parameters should be supplied:</p> <ul> <li><code>domain</code> - Required. List of avoided numbers.</li> </ul> <p>Metric increment returns <code>Failure</code> status for rows where some values in the specified columns do fall into set of avoided numbers.</p>"},{"location":"03-job-configuration/06-Metrics/#number-less-than-metric","title":"Number Less Than Metric","text":"<p>Counts number of values which being cast to number (double) are less than (or equal to) the specified value.</p> <p>All number less than metrics are defined in <code>numberLessThan</code> subsection. Additional parameters should be supplied:</p> <ul> <li><code>compareValue</code> - Required. Number to compare with.</li> <li><code>includeBound</code> - Optional, default is <code>false</code>. Specifies whether to include <code>compareValue</code> in the range for comparison.</li> </ul> <p>Metric increment returns <code>Failure</code> status for rows where some values in the specified columns do not satisfy the  comparison criteria.</p>"},{"location":"03-job-configuration/06-Metrics/#number-greater-than-metric","title":"Number Greater Than Metric","text":"<p>Counts number of values which being cast to number (double) are greater than (or equal to) the specified value.</p> <p>All number greater than metrics are defined in <code>numberGreaterThan</code> subsection. Additional parameters should be supplied:</p> <ul> <li><code>compareValue</code> - Required. Number to compare with.</li> <li><code>includeBound</code> - Optional, default is <code>false</code>. Specifies whether to include <code>compareValue</code> in the range for comparison.</li> </ul> <p>Metric increment returns <code>Failure</code> status for rows where some values in the specified columns do not satisfy the comparison criteria.</p>"},{"location":"03-job-configuration/06-Metrics/#number-between-metric","title":"Number Between Metric","text":"<p>Counts number of values which being cast to number (double) are within the given interval.</p> <p>All number between metrics are defined in <code>numberBetween</code> subsection. Additional parameters should be supplied:</p> <ul> <li><code>lowerCompareValue</code> - Required. The lower bound of the interval.</li> <li><code>upperCompareValue</code> - Required. The upper bound of the interval.</li> <li><code>includeBound</code> - Optional, default is <code>false</code>. Specifies whether to include interval bounds in the range for comparison.</li> </ul> <p>Metric increment returns <code>Failure</code> status for rows where some values in the specified columns do not satisfy the comparison criteria.</p>"},{"location":"03-job-configuration/06-Metrics/#number-not-between-metric","title":"Number Not Between Metric","text":"<p>Counts number of values which being cast to number (double) are outside the given interval.</p> <p>All number between metrics are defined in <code>numberNotBetween</code> subsection. Additional parameters should be supplied:</p> <ul> <li><code>lowerCompareValue</code> - Required. The lower bound of the interval.</li> <li><code>upperCompareValue</code> - Required. The upper bound of the interval.</li> <li><code>includeBound</code> - Optional, default is <code>false</code>. Specifies whether to include interval bounds in the range for comparison.</li> </ul> <p>Metric increment returns <code>Failure</code> status for rows where some values in the specified columns do not satisfy the comparison criteria.</p>"},{"location":"03-job-configuration/06-Metrics/#number-values-metric","title":"Number Values Metric","text":"<p>Counts number of values which being cast to number (double) are equal to the number given in metric definition.</p> <p>All number values metrics are defined in <code>numberValues</code> subsection. Additional parameters should be supplied:</p> <ul> <li><code>compareValue</code> - Required. Number value to compare with.</li> </ul> <p>Metric increment returns <code>Failure</code> status for rows where some values in the specified columns do not match defined compare value.</p>"},{"location":"03-job-configuration/06-Metrics/#median-value-metric","title":"Median Value Metric","text":"<p>Calculates median value of the values in the specified column. Metric calculator uses  TDigest library for computation of median value.</p> <p>This metric works with only one column.</p> <p>All median value metrics are defined in <code>medianValue</code> subsection. Additional parameters can be supplied:</p> <ul> <li><code>accuracyError</code> - Optional, default is <code>0.01</code>. Accuracy error for calculation of median value.</li> </ul> <p>Metric increment returns <code>Failure</code> status for rows where value in the specified column is not castable to number.</p>"},{"location":"03-job-configuration/06-Metrics/#first-quantile-metric","title":"First Quantile Metric","text":"<p>Calculates first quantile for the values in the specified column. Metric calculator uses TDigest library for computation of first quantile.</p> <p>This metric works with only one column.</p> <p>All median value metrics are defined in <code>firstQuantile</code> subsection. Additional parameters can be supplied:</p> <ul> <li><code>accuracyError</code> - Optional, default is <code>0.01</code>. Accuracy error for calculation of first quantile value.</li> </ul> <p>Metric increment returns <code>Failure</code> status for rows where value in the specified column is not castable to number.</p>"},{"location":"03-job-configuration/06-Metrics/#third-quantile-metric","title":"Third Quantile Metric","text":"<p>Calculates third quantile for the values in the specified column. Metric calculator uses TDigest library for computation of third quantile.</p> <p>This metric works with only one column.</p> <p>All third value metrics are defined in <code>thirdQuantile</code> subsection. Additional parameters can be supplied:</p> <ul> <li><code>accuracyError</code> - Optional, default is <code>0.01</code>. Accuracy error for calculation of third value.</li> </ul> <p>Metric increment returns <code>Failure</code> status for rows where value in the specified column is not castable to number.</p>"},{"location":"03-job-configuration/06-Metrics/#get-quantile-metric","title":"Get Quantile Metric","text":"<p>Calculates an arbitrary quantile for the values in the specified column. Metric calculator uses TDigest library for computation of quantile.</p> <p>This metric works with only one column.</p> <p>All get quantile metrics are defined in <code>getQuantile</code> subsection. Additional parameters should be supplied:</p> <ul> <li><code>accuracyError</code> - Optional, default is <code>0.01</code>. Accuracy error for calculation of quantile value.</li> <li><code>target</code> - Required. A number in the interval <code>[0, 1]</code> corresponding to the quantile that need to be caclulated.</li> </ul> <p>Metric increment returns <code>Failure</code> status for rows where value in the specified column is not castable to number.</p>"},{"location":"03-job-configuration/06-Metrics/#get-percentile-metric","title":"Get Percentile Metric","text":"<p>This metric is inverse of Get Quantile Metric. It calculates a percentile value (quantile in %) which corresponds to the specified number from the set of values in the column. Metric calculator uses TDigest library for computation of percentile value.</p> <p>This metric works with only one column.</p> <p>All get percentile metrics are defined in <code>getPercentile</code> subsection. Additional parameters should be supplied:</p> <ul> <li><code>accuracyError</code> - Optional, default is <code>0.01</code>. Accuracy error for calculation of percentile.</li> <li><code>target</code> - Required. The number from the set of values in the column, for which percentile   is determined.</li> </ul> <p>Metric increment returns <code>Failure</code> status for rows where value in the specified column is not castable to number.</p>"},{"location":"03-job-configuration/06-Metrics/#column-equality-metric","title":"Column Equality Metric","text":"<p>Calculates the number of rows where values in the specified columns are equal to each other. Metric definition does not require additional parameters: <code>params</code> should not be set.</p> <p>This metric works with at least two columns.</p> <p>All column equality metrics are defined in <code>columnEq</code> subsection.</p> <p>Metric increment returns <code>Failure</code> status for rows where some values in the specified column are not castable to string or when they are not equal.</p>"},{"location":"03-job-configuration/06-Metrics/#day-distance-metric","title":"Day Distance Metric","text":"<p>Calculates the number of rows where difference between date in two columns expressed in terms of days is less (strictly less) than the specified threshold value.</p> <p>This metric works with exactly two columns.</p> <p>All day distance metrics are defined in <code>dayDistance</code> subsection. Additional parameters should be supplied:</p> <ul> <li><code>threshold</code> - Required. Maximum allowed difference between two dates in days   (not included in the range for comparison).</li> <li><code>dateFormat</code> - Optional, default is <code>yyyy-MM-dd'T'HH:mm:ss.SSSZ</code>. Target datetime format.   The datetime format must be specified as   Java DateTimeFormatter pattern.</li> </ul> <p>NOTE If the specified columns are of type <code>Timestamp</code>, it is assumed that they fit any datetime format and, therefore,  metric will return the total number of non-empty cells. Accordingly, the datetime format does not need to be specified.</p> <p>Metric increment returns <code>Failure</code> status for rows where some values in the specified columns do not conform to  the specified datetime format or when date difference in days is greater than or equal to specified threshold.</p>"},{"location":"03-job-configuration/06-Metrics/#levenshtein-distance-metric","title":"Levenshtein Distance Metric","text":"<p>Calculates number of rows where Levenshtein distance between string values in the provided columns is less than (strictly less) specified threshold.</p> <p>This metric works with exactly two columns.</p> <p>All levenshtein distance metrics are defined in <code>levenshteinDistance</code> subsection. Additional parameters should be supplied:</p> <ul> <li><code>threshold</code> - Required. Maximum allowed Levenshtein distance.</li> <li><code>normalize</code> - Optional, default is <code>false</code>. Boolean parameter indicating whether the Levenshtein distance   should be normalized with respect to the maximum of the two string lengths.</li> </ul> <p>IMPORTANT. If Levenshtein distance is normalized then threshold value must be in range <code>[0, 1]</code>.</p> <p>Metric increment returns <code>Failure</code> status for rows where some values in the specified columns are not castable to string or when Levenshtein distance is greater than or equal to specified threshold.</p>"},{"location":"03-job-configuration/06-Metrics/#comoment-metric","title":"CoMoment Metric","text":"<p>Calculates the covariance moment of the values in two columns (co-moment). Metric definition does not require additional parameters: <code>params</code> should not be set.</p> <p>This metric works with exactly two columns.</p> <p>IMPORTANT. For the metric to be calculated, values in the specified columns must not be empty or null and  also can be cast to number (double). If at least one corrupt value is found, then metric calculator returns NaN value.</p> <p>Metric increment returns <code>Failure</code> status for rows where some values in the specified columns cannot be cast to number.</p>"},{"location":"03-job-configuration/06-Metrics/#covariance-metric","title":"Covariance Metric","text":"<p>Calculates the covariance of the values in two columns. Metric definition does not require additional parameters: <code>params</code> should not be set.</p> <p>This metric works with exactly two columns.</p> <p>IMPORTANT. For the metric to be calculated, values in the specified columns must not be empty or null and also can be cast to number (double). If at least one corrupt value is found, then metric calculator returns NaN value.</p> <p>Metric increment returns <code>Failure</code> status for rows where some values in the specified columns cannot be cast to number.</p>"},{"location":"03-job-configuration/06-Metrics/#covariance-bessel-metric","title":"Covariance Bessel Metric","text":"<p>Calculates the covariance of the values in two columns with the Bessel correction. Metric definition does not require additional parameters: <code>params</code> should not be set.</p> <p>This metric works with exactly two columns.</p> <p>IMPORTANT. For the metric to be calculated, values in the specified columns must not be empty or null and also can be cast to number (double). If at least one corrupt value is found, then metric calculator returns NaN value.</p> <p>Metric increment returns <code>Failure</code> status for rows where some values in the specified columns cannot be cast to number.</p>"},{"location":"03-job-configuration/06-Metrics/#top-n-metric","title":"Top N Metric","text":"<p>This is a specific metric that calculates approximate N most frequently occurring values in a column. The metric calculator uses Twitter Algebird library, which implements abstract algebra methods for Scala.</p> <p>This metric works with only one column.</p> <p>All top N metrics are defined in <code>topN</code> subsection. Additional parameters can be supplied:</p> <ul> <li><code>targetNumber</code> - Optional, default is <code>10</code>. Number N of values to search.</li> <li><code>maxCapacity</code> - Optional, default is <code>100</code>. Maximum container size for storing top values.</li> </ul>"},{"location":"03-job-configuration/06-Metrics/#composed-metrics","title":"Composed Metrics","text":"<p>Composed metrics are defined using a formula (specified in the <code>formula</code> field) for their calculation.  As composed metric are intended for using other metric results to compute a derivative result then, these metrics can be referenced in the formula by their IDs. </p> <p>Formula must be written using Mustache Template notation, e.g.: <code>{{ metric_1 }} + {{ metic_2 }}</code>.</p> <p>Basic (+-*/) and exponentiation (^) math operations are supported, as well as grouping using parentheses.</p> <p>This, composed metrics are defined in the <code>composed</code> subsection using following parameters:</p> <ul> <li><code>id</code> - Required. Composed metric ID;</li> <li><code>description</code> - Optional. Composed metric description.</li> <li><code>formula</code> - Required. Formula to calculate composed metric</li> </ul>"},{"location":"03-job-configuration/06-Metrics/#metrics-configuration-example","title":"Metrics Configuration Example","text":"<p>As it is shown in the example below, regular metrics of the same type are grouped within subsections named after the  type of the metric. These subsections should contain a list of metrics configurations of the corresponding type. Composed metrics are listed in the separate subsection.</p> <pre><code>jobConfig: {\n  metrics: {\n    regular: {\n      rowCount: [\n        {id: \"hive_table_row_cnt\", description: \"Row count in hive_source_1\", source: \"hive_source_1\"},\n        {id: \"csv_file_row_cnt\", description: \"Row count in hdfs_delimited_source\", source: \"hdfs_delimited_source\"}\n      ]\n      distinctValues: [\n        {\n          id: \"fixed_file_dist_name\", description: \"Distinct values in hdfs_fixed_file\",\n          source: \"hdfs_fixed_file\", columns: [\"colA\"]\n        }\n      ]\n      nullValues: [\n        {id: \"hive_table_nulls\", description: \"Null values in columns id and name\", source: \"hive_source_1\", columns: [\"id\", \"name\"]}\n      ]\n      completeness: [\n        {id: \"orc_data_compl\", description: \"Completness of column id\", source: \"hdfs_orc_source\", columns: [\"id\"]}\n        {\n          id: \"hive_table_nulls\", \n          description: \"Completness of columns id and name\", \n          source: \"hive_source_1\", \n          columns: [\"id\", \"name\"]\n        }\n      ]\n      avgNumber: [\n        {id: \"avro_file1_avg_bal\", description: \"Avg number of column balance\", source: \"hdfs_avro_source\", columns: [\"balance\"]}\n      ]\n      regexMatch: [\n        {\n          id: \"table_source1_inn_regex\", description: \"Regex match for inn column\", source: \"table_source_1\",\n          columns: [\"inn\"], params: {regex: \"\"\"^\\d{10}$\"\"\"}\n        }\n      ]\n      stringInDomain: [\n        {\n          id: \"orc_data_segment_domain\", source: \"hdfs_orc_source\",\n          columns: [\"segment\"], params: {domain: [\"FI\", \"MID\", \"SME\", \"INTL\", \"CIB\"]}\n        }\n      ]\n      topN: [\n        {\n          id: \"filterVS_top3_currency\", description: \"Top 3 currency in filterVS\", source: \"filterVS\",\n          columns: [\"id\"], params: {targetNumber: 3, maxCapacity: 10}\n        }\n      ],\n      levenshteinDistance: [\n        {\n          id: \"lvnstDist\", source: \"table_source_2\", columns: [\"col1\", \"col2\"],\n          params: {normalize: true, threshold: 0.3}\n        }\n      ]\n    }\n    composed: [\n      {\n        id: \"pct_of_null\", description: \"Percent of null values in hive_table1\",\n        formula: \"100 * {{ hive_table_nulls }} ^ 2 / ( {{ hive_table_row_cnt }} + 1)\"\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"03-job-configuration/07-Checks/","title":"Checks Configurations","text":"<p>Performing checks ove the metric results is an important step in Checkita framework. As metric results are calculated then checks can be configured to identify if there are any problems with quality of data.</p> <p>In Checkita there are two main group of checks:</p> <ul> <li><code>Spanshot</code> checks - allows comparison of metric results with static thresholds or with other metric results in the    same Data Quality job.</li> <li><code>Trend</code> checks - allows evaluation of how metric result is changing over a certain period of time. Checks of this type   are used to detect anomalies in data. In order trend check work it is required to set up Data Quality storage since   check calculator need to fetch historical results for the metric of interest.</li> </ul> <p>After evaluation, check will have a status as described in  Status Model used in Results chapter. </p>"},{"location":"03-job-configuration/07-Checks/#snapshot-checks","title":"Snapshot Checks","text":"<p>Snapshot checks represent a simple comparison of metric results with a static threshold or with other metric result.</p> <p>The following snapshot checks are supported:</p> <ul> <li><code>equalTo</code> - checks if metric results is equal to a given threshold value or to other metric result.</li> <li><code>lessThan</code> - checks if metric result is less than a given threshold value or other metric result.</li> <li><code>greaterThan</code> - checks if metric result is greater than a given threshold value or other metric result.</li> <li><code>differByLT</code> - checks if relative difference between two metric results is less than a given threshold.   This check succeeds when following expression is true: <code>| metric - compareMetric | / compareMetric &lt; threshold</code>.</li> </ul> <p>Snapshot checks are configured using common set of parameters, which are:</p> <ul> <li><code>id</code> - Required. Check ID</li> <li><code>description</code> - Optional. Description of the check.</li> <li><code>metric</code> - Required. Metric ID which results is checked.</li> <li><code>compareMetric</code> - Optional. Metric ID which result is used as a threshold.</li> <li><code>threshold</code> - Optional. Explicit threshold value.</li> </ul> <p>IMPORTANT. When configuring check it should be specified either an explicit threshold value in <code>threshold</code> field or other metric ID in <code>compareMetric</code> field which result will be used as a threshold value. The only exception to this rule is <code>differByLY</code> check for which it is required to specify both, threshold value and metric ID to compare with.</p>"},{"location":"03-job-configuration/07-Checks/#trend-checks","title":"Trend Checks","text":"<p>Trend checks are used to detect anomalies in data. This type of checks allows to verify that the value of the metric  corresponds to its average value within a given deviation for a certain period of time. Maximum allowed deviation is  configured by providing a threshold value.</p> <p>Following trend checks are supported:</p> <ul> <li><code>averageBoundFull</code> - sets the same upper and lower deviation from metric average result. Check succeeds when following   expression is true: <code>(1 - threshold) * avgResult &lt;= currentResult &lt;= (1 + threshold) * avgResult</code>.</li> <li><code>averageBoundUpper</code> - verifies only upper deviation from the metric average result. Check succeeds when following   expression is true: <code>currentResult &lt;= (1 + threshold) * avgResult</code>.</li> <li><code>averageBoundLower</code> - verifies only lower deviation from the metric average result. Check succeeds when following   expression is ture: <code>(1 - threshold) * avgResult &lt;= currentResult</code>.</li> <li><code>averageBoundRange</code> - sets different thresholds for upper and lower deviations from metric average results.   Check succeeds when following expression is true:   <code>(1 - thresholdLower) * avgResult &lt;= currentResult &lt;= (1 + thresholdUpper) * avgResult</code>.</li> </ul> <p>Trend checks are configured using following set of parameters:</p> <ul> <li><code>id</code> - Required. Check ID</li> <li><code>description</code> - Optional. Description of the check.</li> <li><code>metric</code> - Required. Metric ID which result is checked.</li> <li><code>rule</code> - Required. The rule for calculating historical average value of the metric. There are two rules supported:<ul> <li><code>record</code> - calculates the average value of metric for the configured number of historical records.</li> <li><code>datetime</code> - calculates the average value of metric for the configured datetime window.</li> </ul> </li> <li><code>windowSize</code> - Required. Size of the window for average metric value calculation:<ul> <li>If <code>rule</code> is set to <code>record</code> then window size is the number of records to retrieve.</li> <li>If <code>rule</code> is set to <code>datetime</code> then window size is a duration string which should conform to Scala Duration.</li> </ul> </li> <li><code>windowOffset</code> - Optional, default is <code>0</code> or <code>0s</code>. Set window offset back from current reference date   (see Working with Date and Time chapter for more details on    reference date). By default, offset is absent and window start from current reference date.<ul> <li>If <code>rule</code> is set to <code>record</code> then window offset is the number of records to skip from reference date.</li> <li>If <code>rule</code> is set to <code>datetime</code> then window offset is a duration string which should conform to Scala Duration.</li> </ul> </li> <li><code>threshold</code> - Required. Sets maximum allowed deviation from historical average metric result. Not used with   <code>averageBoundRange</code> check.</li> <li><code>thresholdLower</code> - Required. Sets maximum allowed lower deviation from historical average metric result. *Used only   for <code>averageBoundRange</code> check.</li> <li><code>thresholdUpper</code> - Required. Sets maximum allowed upper deviation from historical average metric result. *Used only   for <code>averageBoundRange</code> check.</li> </ul> <p>NOTE. Scala Duration string has a format of <code>&lt;length&gt;&lt;unit&gt;</code> where following units are allowed: <code>d</code>, <code>day</code>, <code>h</code>, <code>hr</code>, <code>hour</code>, <code>m</code>, <code>min</code>, <code>minute</code>, <code>s</code>, <code>sec</code>, <code>second</code>, <code>ms</code>, <code>milli</code>, <code>millisecond</code>, <code>\u00b5s</code>, <code>micro</code>, <code>microsecond</code>, <code>ns</code>, <code>nano</code>, <code>nanosecond</code>.</p>"},{"location":"03-job-configuration/07-Checks/#top-n-rank-check","title":"Top N Rank Check","text":"<p>This is a special check designed specifically for Top N Metric and working only with it. Top N rank check calculates the Jacquard distance between the current and previous sets of top N metric and checks if it does not exceed the threshold value.</p> <p>IMPORTANT: Calculation of this check is currently supported only between the current and previous topN metric sets.</p> <p>Top N rank check is configured using following parameters:</p> <ul> <li><code>id</code> - Required. Check ID</li> <li><code>description</code> - Optional. Description of the check.</li> <li><code>metric</code> - Required. Metric ID which result is checked.</li> <li><code>targetNumber</code> - Required. Number of records from the set of top N metric results that is considered.    This number should be less than or equal to number of collected top values in top N metric.</li> <li><code>threshold</code> - Required. Maximum allowed Jacquard distance between current and previous sets of records from    top N metric result. Should be a number in interval <code>[0, 1]</code>.</li> </ul>"},{"location":"03-job-configuration/07-Checks/#checks-configuration-example","title":"Checks Configuration Example","text":"<p>As it is shown in the example below, checks are grouped into two subsections: <code>trend</code> and <code>snapshot</code>. Then, checks of the same type are grouped within subsections named after the type of the checks.  These subsections should contain a list of metrics configurations of the corresponding type.</p> <pre><code>jobConfig: {\n  checks: {\n    trend: {\n      averageBoundFull: [\n        {\n          id: \"avg_bal_check\",\n          description: \"Check that average balance stays within +/-25% of the week average\"\n          metric: \"avro_file1_avg_bal\",\n          rule: \"datetime\"\n          windowSize: \"8d\"\n          threshold: 0.25\n        }\n      ]\n      averageBoundUpper: [\n        {id: \"avg_pct_null\", metric: \"pct_of_null\", rule: \"datetime\", windowSize: \"15d\", threshold: 0.5}\n      ]\n      averageBoundLower: [\n        {id: \"avg_distinct\", metric: \"fixed_file_dist_name\", rule: \"record\", windowSize: 31, threshold: 0.3}\n      ]\n      averageBoundRange: [\n        {\n          id: \"avg_inn_match\",\n          metric: \"table_source1_inn_regex\",\n          rule: \"datetime\",\n          windowSize: \"8d\",\n          thresholdLower: 0.2\n          thresholdUpper: 0.4\n        }\n      ]\n      topNRank: [\n        {id: \"top2_curr_match\", metric: \"filterVS_top3_currency\", targetNumber: 2, threshold: 0.1}\n      ]\n    }\n    snapshot: {\n      differByLT: [\n        {\n          id: \"row_cnt_diff\",\n          description: \"Number of rows in two tables should not differ on more than 5%.\",\n          metric: \"hive_table_row_cnt\"\n          compareMetric: \"csv_file_row_cnt\"\n          threshold: 0.05\n        }\n      ]\n      equalTo: [\n        {id: \"zero_nulls\", description: \"Hive Table1 mustn't contain nulls\", metric: \"hive_table_nulls\", threshold: 0}\n      ]\n      greaterThan: [\n        {id: \"completeness_check\", metric: \"orc_data_compl\", threshold: 0.99}\n      ]\n      lessThan: [\n        {id: \"null_threshold\", metric: \"pct_of_null\", threshold: 0.01}\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"03-job-configuration/08-Targets/","title":"Targets Configuration","text":"<p>Targets are designed to provide alternative channels for sending results. First of all, targets can be used to send notifications to users about problems in their data or just send summary of Data Quality job. In addition, targets provide different ways for saving results, e.g. write them to file in HDFS or send to Kafka topic.</p> <p>All targets are configured in <code>targets</code> section of the job configuration. There are four general types of targets that can be configured depending on what information is being sent or saved:</p> <ul> <li>Result Targets - used to save results as file or send them to Kafka in addition to storing into   Data Quality storage.</li> <li>Error Collection Targets - used to save collected metric errors as file or to send them   to Kafka topic. Note that metric errors are not saved in the Data Quality storage as they contain some excerpts from    data being checked. For more information, see Metric Error Collection   chapter.</li> <li>Summary Targets - used to send notifications with summary report for Data Quality job.   Summary report may also be sent to Kafka topic.</li> <li>Check Alert Targets - used to watch over some checks and send notification to users in case if   some watched checks have failed.</li> </ul>"},{"location":"03-job-configuration/08-Targets/#result-targets","title":"Result Targets","text":"<p>Results targets are configured in the <code>results</code> subsection and can be one of the following type depending on where they are sent or saved:</p> <ul> <li><code>file</code> - Save results as file in local or remote (HDFS, S3, etc.) file system.</li> <li><code>hive</code> - Save results in HDFS as Hive table. Note that Hive table with required schema must be created prior   results saving.</li> <li><code>kafka</code> - Send results to Kafka topic in JSON format.</li> </ul> <p>For result target of any type it is required to configure list of result to be saved or sent:</p> <ul> <li><code>resultTypes</code> - Required. List of result types to save or sent. May include following:   <code>regularMetrics</code>, <code>composedMetrics</code>, <code>loadChecks</code>, <code>checks</code>. Note that all results types are reduced     to Unified Targets Schema and saved together.</li> </ul>"},{"location":"03-job-configuration/08-Targets/#save-results-to-file","title":"Save Results to File","text":"<p>In order to save results to file, it is required to configure result target of <code>file</code> type. In addition to list of  saved results, it is required to configure file output.</p> <ul> <li><code>save</code> - Required. File output configuration used to save results.   For more information on configuring file outputs, see File Output Configuration chapter.</li> </ul> <p>File with results will have Unified Targets Schema.</p>"},{"location":"03-job-configuration/08-Targets/#save-results-to-hive","title":"Save Results to Hive","text":"<p>In order to save results to Hive table, it is required to configure result target of <code>hive</code> type. Hive table to which  results will be saved must be created in advance with Unified Targets Schema.</p> <p>Thus, in addition to list of saved results, it is required to indicate Hive schema and table:</p> <ul> <li><code>schema</code> - Required. Hive schema.</li> <li><code>table</code> - Required. Hive table.</li> </ul> <p>Note that results will be appended to Hive table.</p>"},{"location":"03-job-configuration/08-Targets/#send-results-to-kafka","title":"Send Results to Kafka","text":"<p>In order to send results to Kafka topic, it is required to configure result target of <code>kafka</code> type.  Connection to Kafka cluster must be configured in <code>connections</code> section of job configuration as described in  Kafka Connection Configuration. </p> <p>Thus, in addition to list of saved results, it is required provide following parameters:</p> <ul> <li><code>connection</code> - Required. Kafka connection ID.</li> <li><code>topic</code> - Required. Kafka topic to send results to.</li> <li><code>options</code> - Optional. Additional list of Kafka parameters for sending messages to topic.    Parameters are provided as a strings in format of <code>parameterName=parameterValue</code>.</li> </ul> <p>Results will be saved as JSON messages. In addition, <code>aggregatedKafkaOutput</code> parameter configured in application  settings controls how results will be sent (see Enablers chapter):</p> <ul> <li>One message per each result.</li> <li>One large message with list of all results.</li> </ul>"},{"location":"03-job-configuration/08-Targets/#error-collection-targets","title":"Error Collection Targets","text":"<p>Error collection targets are configured in <code>errorCollection</code> subsection and can be one of the following type depending  on where metric errors are sent or saved:</p> <ul> <li><code>file</code> - Save metric errors as file in local or remote (HDFS, S3, etc.) file system.</li> <li><code>hive</code> - Save metric errors in HDFS as Hive table. Note that Hive table with required schema must be created prior   metric errors saving.</li> <li><code>kafka</code> - Send metric errors to Kafka topic in JSON format.</li> </ul> <p>Note that metric errors are transformed to Unified Targets Schema when send or saved.</p> <p>For error collection target of any type the following parameters can be supplied:</p> <ul> <li><code>metrics</code> - Optional. List of metric for which errors will be saved. If omitted, then errors are saved for all   metrics defined in Data Quality job.</li> <li><code>dumpSize</code> - Optional, default is <code>100</code>. Allows additionally limit number of errors saved per metric in order to    make reports more compact. Could not be larger, than application-level limitation as described in    Enablers chapter.</li> </ul>"},{"location":"03-job-configuration/08-Targets/#save-metric-errors-to-file","title":"Save Metric Errors to File","text":"<p>In order to save metric errors to file, it is required to configure error collection target of <code>file</code> type.  In addition to common error collection target parameters, it is required to configure file output:</p> <ul> <li><code>save</code> - Required. File output configuration used to save results.   For more information on configuring file outputs, see File Output Configuration chapter.</li> </ul> <p>File with metric errors will have Unified Targets Schema.</p>"},{"location":"03-job-configuration/08-Targets/#save-metric-errors-to-hive","title":"Save Metric Errors to Hive","text":"<p>In order to save metric errors to Hive table, it is required to configure result error collection target of <code>hive</code> type. Hive table to which metric errors will be saved must be created in advance with Unified Targets Schema.</p> <p>Thus, in addition to common error collection target parameters, it is required to indicate Hive schema and table:</p> <ul> <li><code>schema</code> - Required. Hive schema.</li> <li><code>table</code> - Required. Hive table.</li> </ul> <p>Note that metric errors will be appended to Hive table.</p>"},{"location":"03-job-configuration/08-Targets/#send-metric-errors-to-kafka","title":"Send Metric Errors to Kafka","text":"<p>In order to send metric errors to Kafka topic, it is required to configure error collection target of <code>kafka</code> type. Connection to Kafka cluster must be configured in <code>connections</code> section of job configuration as described in Kafka Connection Configuration.</p> <p>Thus, in addition to common error collection target parameters, it is required provide following ones:</p> <ul> <li><code>connection</code> - Required. Kafka connection ID.</li> <li><code>topic</code> - Required. Kafka topic to send results to.</li> <li><code>options</code> - Optional. Additional list of Kafka parameters for sending messages to topic.   Parameters are provided as a strings in format of <code>parameterName=parameterValue</code>.</li> </ul> <p>Metric errors will be saved as JSON messages. In addition, <code>aggregatedKafkaOutput</code> parameter configured in application settings controls how metric errors will be sent  (see Enablers chapter):</p> <ul> <li>One message per each result.</li> <li>One large message with list of all results. </li> </ul> <p>IMPORTANT. Be careful, when using this option for saving metric errors as there could be a significant number of them. In order to fit into Kafka message size limits it is recommended to limit number of errors sent per each metric by setting <code>dumpSize</code> parameter to a reasonably low number.</p>"},{"location":"03-job-configuration/08-Targets/#summary-targets","title":"Summary Targets","text":"<p>Checkita framework collects summary upon completion of each Data Quality job. Summary targets are designed accordingly, to enable sending summary reports to users. Thus, summary targets are configured in <code>summary</code> subsection and can be  one of the following type depending on where summary reports are sent or saved:</p> <ul> <li><code>email</code> - Send summary report to user(s) via email.</li> <li><code>mattermost</code> - Send summary report to mattermost either to channel or to user's direct messages.</li> <li><code>kafka</code> - Send summary report to Kafka topic in JSON format. When sending summary report to Kafka, it is transformed   to Unified Targets Schema.</li> </ul> <p>For summary target of <code>email</code> or <code>mattermost</code> type the following parameters can be supplied:</p> <ul> <li><code>attachMetricErrors</code> - Optional, default is <code>false</code>. Boolean parameter indicating whether report with collected   metric errors should be attached to email or message with summary report.</li> <li><code>attachFailedChecks</code> - Optional, default is <code>false</code>. Boolean parameter indicating whether report with failed checks   should be attached to email or message with summary report.</li> <li><code>metrics</code> - Optional. If <code>attachMetricErrors</code> is set to <code>true</code>, then this parameter can be used to specify list of    metric for which errors will be saved. If omitted, then errors are saved for all metrics defined in Data Quality job.</li> <li><code>dumpSize</code> - Optional, default is <code>100</code>. If <code>attachMetricErrors</code> is set to <code>true</code>, then this parameter allows    additionally limit number of errors saved per metric in order to make report more compact.    Could not be larger, than application-level limitation as described in   Enablers chapter.</li> </ul>"},{"location":"03-job-configuration/08-Targets/#send-summary-to-email","title":"Send Summary to Email","text":"<p>In order to send summary report via email, it is required to configure summary target of <code>email</code> type. In addition to common summary target parameters, it is required to configure following ones:</p> <ul> <li><code>recipients</code> - Required. List of recipients' emails to which summary report will be sent.</li> <li><code>template</code> - Optional. HTML template to build email body.</li> <li><code>templateFile</code> - Optional. Location of the file with HTML template to build email body.</li> </ul> <p>HTML template is optional. If HTML template is not provided then the default summary report body is  compiled. Moreover, it should be noted, that <code>template</code> parameter has higher priority than <code>templateFile</code> one.  Therefore, if both of them are set then explicitly defined then HTML template from <code>template</code> parameter is used.</p> <p>In addition, HTML templates support parameter substitution using  Mustache Template notation, e.g.: <code>This {{ parameterName }} has a value of {{ parameterValue }}</code>. List of available parameters that can be used for substitution in HTML templates is given in  Job Summary Parameters Available for Templates chapter below.</p>"},{"location":"03-job-configuration/08-Targets/#send-summary-to-mattermost","title":"Send Summary to Mattermost","text":"<p>In order to send summary report to mattermost, it is required to configure summary target of <code>mattermost</code> type. In addition to common summary target parameters, it is required to configure following ones:</p> <ul> <li><code>recipients</code> - Required. List of recipients' to which summary report will be sent. Message can be sent either to   a channel or to a user's direct messages:<ul> <li>When sending message to a channel, it is required to specify channel name prefixed with <code>#</code> sign: <code>#someChannel</code>.</li> <li>When sending message to a user's direct messages, it is required to specify username with <code>@</code> prefix: <code>@someUser</code>.</li> </ul> </li> <li><code>template</code> - Optional. Markdown template to build message body.</li> <li><code>templateFile</code> - Optional. Location of the file with Markdown template to build message body.</li> </ul> <p>Markdown template is optional. If Markdown template is not provided then the default summary report body is compiled. Moreover, it should be noted, that <code>template</code> parameter has higher priority than <code>templateFile</code> one. Therefore, if both of them are set then explicitly defined then Markdown template from <code>template</code> parameter is used.</p> <p>In addition, Markdown templates support parameter substitution using Mustache Template notation, e.g.: <code>This {{ parameterName }} has a value of {{ parameterValue }}</code>. List of available parameters that can be used for substitution in Markdown templates is given in Job Summary Parameters Available for Templates chapter below.</p>"},{"location":"03-job-configuration/08-Targets/#send-summary-to-kafka","title":"Send Summary to Kafka","text":"<p>In order to send summary report to Kafka topic, it is required to configure summary target of <code>kafka</code> type. Connection to Kafka cluster must be configured in <code>connections</code> section of job configuration as described in Kafka Connection Configuration.</p> <p>Kafka messages do not support any from of attachments, therefore, only summary report itself can be sent to Kafka topic. Summary report is sent in form of JSON string that will contain all the parameters defined in Job Summary Parameters Available for Templates chapter below. JSON string format will conform to Unified Targets Schema.</p> <p>Thus, in order to configure <code>kafka</code> summary target it is required to specify following parameters:</p> <ul> <li><code>connection</code> - Required. Kafka connection ID.</li> <li><code>topic</code> - Required. Kafka topic to send results to.</li> <li><code>options</code> - Optional. Additional list of Kafka parameters for sending messages to topic.   Parameters are provided as a strings in format of <code>parameterName=parameterValue</code>.</li> </ul>"},{"location":"03-job-configuration/08-Targets/#check-alert-targets","title":"Check Alert Targets","text":"<p>Check alert targets are developed specifically to enable notification sending in case if some of watched checks  have failed. These targets are configured in <code>checkAlert</code> subsection and can be one of the following type depending  on where alerts are sent:</p> <ul> <li><code>email</code> - Send check alert to user(s) via email.</li> <li><code>mattermost</code> - Send check alert to mattermost either to channel or to user's direct messages.</li> </ul> <p>For check alert target of any type the following parameters can be supplied:</p> <ul> <li><code>id</code> - Required. ID of check alert. There could be different check alert configurations for different sets of checks.   Therefore, check alerts should have an ID, in order to distinguish them.</li> <li><code>checks</code> - Optional. List of watched checks. If any of watched checks fails then alert notification is sent.   If omitted, then all checks defined in the Data Quality job are being watched.</li> </ul>"},{"location":"03-job-configuration/08-Targets/#send-check-alerts-to-email","title":"Send Check Alerts to Email","text":"<p>In order to send check alert via email, it is required to configure check alert target of <code>email</code> type. In addition to common check alert target parameters, it is required to configure following ones:</p> <ul> <li><code>recipients</code> - Required. List of recipients' emails to which check alert will be sent.</li> <li><code>template</code> - Optional. HTML template to build email body.</li> <li><code>templateFile</code> - Optional. Location of the file with HTML template to build email body.</li> </ul> <p>HTML template is optional. If HTML template is not provided then the default check alert body is compiled. Moreover, it should be noted, that <code>template</code> parameter has higher priority than <code>templateFile</code> one. Therefore, if both of them are set then explicitly defined then HTML template from <code>template</code> parameter is used.</p> <p>In addition, HTML templates support parameter substitution using Mustache Template notation, e.g.: <code>This {{ parameterName }} has a value of {{ parameterValue }}</code>. List of available parameters that can be used for substitution in HTML templates is given in Job Summary Parameters Available for Templates chapter below.</p>"},{"location":"03-job-configuration/08-Targets/#send-check-alerts-to-mattermost","title":"Send Check Alerts to Mattermost","text":"<p>In order to check alert to mattermost, it is required to configure check alert target of <code>mattermost</code> type. In addition to common check alert target parameters, it is required to configure following ones:</p> <ul> <li><code>recipients</code> - Required. List of recipients' to which check alert will be sent. Message can be sent either to   a channel or to a user's direct messages:</li> <li>When sending message to a channel, it is required to specify channel name prefixed with <code>#</code> sign: <code>#someChannel</code>.</li> <li>When sending message to a user's direct messages, it is required to specify username with <code>@</code> prefix: <code>@someUser</code>.</li> <li><code>template</code> - Optional. Markdown template to build message body.</li> <li><code>templateFile</code> - Optional. Location of the file with Markdown template to build message body.</li> </ul> <p>Markdown template is optional. If Markdown template is not provided then the default check alert body is compiled. Moreover, it should be noted, that <code>template</code> parameter has higher priority than <code>templateFile</code> one. Therefore, if both of them are set then explicitly defined then Markdown template from <code>template</code> parameter is used.</p> <p>In addition, Markdown templates support parameter substitution using Mustache Template notation, e.g.: <code>This {{ parameterName }} has a value of {{ parameterValue }}</code>. List of available parameters that can be used for substitution in Markdown templates is given in Job Summary Parameters Available for Templates chapter below.</p>"},{"location":"03-job-configuration/08-Targets/#unified-targets-schema","title":"Unified Targets Schema","text":"<p>All targets that are saved to five or sent to Kafka are reduced to unified schema. Such approach have some advantages:</p> <ul> <li>Results of various types can be sent all together as single file or a large Kafka message.</li> <li>Saved targets from different Data Quality jobs can be merged into a larger file. This allows to avoid \"small files\"   problem when saving targets in HDFS or S3.</li> <li>As all targets sent to Kafka topic conform to unified schema then it is easier to parse message with different type   of results.</li> </ul> <p>Thus, unified schema is following:</p> Column Name Column Type Comment jobId STRING ID of Data Quality Job referenceDate STRING Reference datetime for which job is run executionDate STRING Datetime of actual job start entityType STRING Type of result data STRING JSON string. Content varies depending in entityType <p>From the schema above it is seen that all data that is specific to a results of each type is stored as JSON string. When sending results to Kafka, the schema would be the same but <code>data</code> will become a nested JSON object.</p>"},{"location":"03-job-configuration/08-Targets/#job-summary-parameters-available-for-templates","title":"Job Summary Parameters Available for Templates","text":"<p>It is already noted that HTML or Markdown templates used to build body of notifications support parameter substitution  using Mustache Template notation. List of available parameters that can  be used for substitution is shown below.</p> <p>For example, Markdown template with check alert notification could look like:</p> <pre><code># Checkita Data Quality Notification - Failed Check Alert\n\nYou requested notifications on failed checks in Data Quality Job: `{{ jobId}}`.\n\nInform you that some watched checks have failed for job started for:\n\n* Reference date: `{{ referenceDate }}`\n* Execution date: `{{ executionDate }}`\n\nAttached files contain information about failed checks. Please, review them.\n</code></pre> <ul> <li><code>jobId</code> - ID of the current Data Quality job.</li> <li><code>jobStatus</code> - Job status: <code>Success</code> if all checks are passed, <code>Failure</code> otherwise.</li> <li><code>referenceDate</code> - Reference datetime for which job is run.</li> <li><code>executionDate</code> - Datetime of actual job start.</li> <li><code>numSources</code> - Total number of sources in the job.</li> <li><code>numMetrics</code> - Total number of metric in the job.</li> <li><code>numChecks</code> - Total number of checks in the job.</li> <li><code>numLoadChecks</code> - Total number of load checks in the job.</li> <li><code>numMetricsWithErrors</code> - Number of metrics that yielded errors during their computation.</li> <li><code>numFailedChecks</code> - Number of failed checks.</li> <li><code>numFailedLoadChecks</code> - Number of failed load checks.</li> <li><code>listMetricsWithErrors</code> - List of all metrics that yielded errors during their computation.</li> <li><code>listFailedChecks</code> - List of failed checks.</li> <li><code>listFailedLoadChecks</code> - List of failed load checks.</li> </ul>"},{"location":"03-job-configuration/08-Targets/#targets-configuration-example","title":"Targets Configuration Example","text":"<p>As it is shown in the example below, targets are grouped into subsections named after their type. These subsections may contain various target configuration depending on the channel where targets are saved or sent. Due to multiple check alert configurations are allowed then they are grouped as list of check alerts sent to a specific channel (email or mattermost).</p> <pre><code>jobConfig: {\n  targets: {\n    results: {\n      file: {\n        resultTypes: [\"checks\", \"loadChecks\"]\n        save: {\n          kind: \"delimited\"\n          path: \"/tmp/dataquality/results\"\n          header: true\n        }\n      }\n      hive: {\n        resultTypes: [\"regularMetrics\", \"composedMetrics\", \"loadChecks\", \"checks\"],\n        schema: \"WORKSPACE_CIBAA\",\n        table: \"DQ_TARGETS\"\n      }\n      kafka: {\n        resultTypes: [\"regularMetrics\", \"composedMetrics\", \"loadChecks\", \"checks\"],\n        connection: \"kafka_broker\"\n        topic: \"some.topic\"\n      }\n    }\n    errorCollection: {\n      file: {\n        metrics: [\"pct_of_null\", \"hive_table_row_cnt\", \"hive_table_nulls\"]\n        dumpSize: 50\n        save: {\n          kind: \"orc\"\n          path: \"tmp/DQ/ERRORS\"\n        }\n      }\n      kafka: {\n        metrics: [\"hive_table_nulls\", \"fixed_file_dist_name\", \"table_source1_inn_regex\"]\n        dumpSize: 25\n        connection: \"kafka_broker\"\n        topic: \"some.topic\"\n        options: [\"addParam=true\"]\n      }\n    }\n    summary: {\n      email: {\n        attachMetricErrors: true\n        metrics: [\"hive_table_nulls\", \"fixed_file_dist_name\", \"table_source1_inn_regex\"]\n        dumpSize: 10\n        recipients: [\"some.person@some.domain\"]\n      }\n      mattermost: {\n        attachMetricErrors: true\n        metrics: [\"hive_table_nulls\", \"fixed_file_dist_name\", \"table_source1_inn_regex\"]\n        dumpSize: 10\n        recipients: [\"@someUser\", \"#someChannel\"]\n      }\n      kafka: {\n        connection: \"kafka_broker\"\n        topic: \"dev.dq_results.topic\"\n      }\n    }\n    checkAlerts: {\n      email: [\n        {\n          id: \"alert1\"\n          checks: [\"avg_bal_check\", \"zero_nulls\"]\n          recipients: [\"some.peron@some.domain\"]\n        }\n        {\n          id: \"alert2\"\n          checks: [\"top2_curr_match\", \"completeness_check\"]\n          recipients: [\"another.peron@some.domain\"]\n        }\n      ]\n      mattermost: [\n        {\n          id: \"alert3\"\n          checks: [\"avg_bal_check\", \"zero_nulls\"]\n          recipients: [\"@someUser\"]\n        }\n        {\n          id: \"alert4\"\n          checks: [\"top2_curr_match\", \"completeness_check\"]\n          recipients: [\"#someChannel\"]\n        }\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"03-job-configuration/09-FileOutputs/","title":"File Output Configuration","text":"<p>Checkita framework has mechanism designed to save some it results to a file either in a local or remote (HDFS, S3, etc.) file system. Thus, it is possible to save virtual sources that are build during Data Quality job execution. Saved virtual sources can later be used for various purposes such as for investigating data quality problems. Apart from that, Checkita supports saving various Data Quality job results as files. In order to do that, it is required to configure targets of the desired type. See Targets Configuration for more information.</p> <p>Thus, Checkita framework support saving file outputs of the following formats:</p> <ul> <li>Delimited text (CSV or TSV).</li> <li>ORC format.</li> <li>Parquet format.</li> <li>Avro format.</li> </ul> <p>This, in order to configure file output it is required to supply following parameters:</p> <ul> <li><code>kind</code> - Required. File format. Should be one of the following: <code>delimited</code>, <code>orc</code>, <code>parquet</code>, <code>avro</code>.</li> <li><code>path</code> - Required. File path to save. Spark DataFrame writer is used under hood to save outputs. Therefore, path,   that is provided should point to a directory. If directory non-empty then content is overwritten.</li> </ul> <p>Additional parameters can be defined for delimited text file output. These are:</p> <ul> <li><code>delimiter</code> - Optional, default is <code>,</code>. Column delimiter.</li> <li><code>quote</code> - Optional, default is <code>\"</code>. Column enclosing character.</li> <li><code>escape</code> - Optional, default is <code>\\</code>. Escape character.</li> <li><code>header</code> - Optional, default is <code>false</code>. Boolean parameter indicating whether file should be written with    columns header or without it.</li> </ul>"},{"location":"03-job-configuration/09-FileOutputs/#file-output-configuration-example","title":"File Output Configuration Example","text":"<ul> <li> <p>parquet file   <pre><code>  {\n    kind: \"parquet\"\n    path: \"/tmp/parquet_file_ooutput\"\n  }\n</code></pre></p> </li> <li> <p>delimited file   <pre><code>{\n  kind: \"delimited\"\n  path: \"/tmp/dataquality/results\"\n  header: true\n}\n</code></pre></p> </li> </ul>"},{"location":"03-job-configuration/10-JobConfigExample/","title":"Job Configuration Example","text":"<p>Below example represents abstract but fully filled Data Quality job configuration with most of the features of  Checkita framework configured.</p> <pre><code>jobConfig: {\n  jobId: \"job_id_for_this_configuration\"\n\n  connections: {\n    oracle: [\n      {id: \"oracle_db1\", url: \"oracle.db.com:1521/public\", username: \"db-user\", password: \"dq-password\"}\n    ]\n    sqlite: [\n      {id: \"sqlite_db\", url: \"some/path/to/db.sqlite\"}\n    ],\n    kafka: [\n      {id: \"kafka_broker\", servers: [\"server1:9092\", \"server2:9092\"]}\n    ]\n  }\n\n  schemas: [\n    {\n      id: \"schema1\"\n      kind: \"delimited\"\n      schema: [\n        {name: \"colA\", type: \"string\"},\n        {name: \"colB\", type: \"timestamp\"},\n        {name: \"colC\", type: \"decimal(10, 3)\"}\n      ]\n    },\n    {\n      id: \"schema2\"\n      kind: \"fixedFull\",\n      schema: [\n        {name: \"col1\", type: \"integer\", width: 5},\n        {name: \"col2\", type: \"double\", width: 6},\n        {name: \"col3\", type: \"boolean\", width: 4}\n      ]\n    },\n    {id: \"schema3\", kind: \"fixedShort\", schema: [\"colOne:5\", \"colTwo:7\", \"colThree:9\"]}\n    {id: \"hive_schema\", kind: \"hive\", schema: \"some_schema\", table: \"some_table\"}\n    {id: \"avro_schema\", kind: \"avro\", schema: \"some/path/to/avro_schema.avsc\"}\n\n  ]\n\n  sources: {\n    table: [\n      {id: \"table_source_1\", connection: \"oracle_db1\", table: \"some_table\", keyFields: [\"id\", \"name\"]}\n      {id: \"table_source_2\", connection: \"sqlite_db\", table: \"other_table\"}\n    ]\n    hive: [\n      {\n        id: \"hive_source_1\", schema: \"some_schema\", table: \"some_table\",\n        partitions: [{name: \"dlk_cob_date\", values: [\"2023-06-30\", \"2023-07-01\"]}],\n        keyFields: [\"id\", \"name\"]\n      }\n    ]\n    file: [\n      {id: \"hdfs_avro_source\", kind: \"avro\", path: \"path/to/avro/file.avro\", schema: \"avro_schema\"},\n      {id: \"hdfs_orc_source\", kind: \"orc\", path: \"path/to/orc/file.orc\"},\n      {\n        id: \"hdfs_delimited_source\",\n        kind: \"delimited\",\n        path: \"path/to/csv/file.csv\"\n        schema: \"schema1\"\n      },\n      {id: \"hdfs_fixed_file\", kind: \"fixed\", path: \"path/to/fixed/file.txt\", schema: \"schema2\"},\n    ],\n    kafka: [\n      {\n        id: \"kafka_source\",\n        connection: \"kafka_broker\",\n        topics: [\"topic1.pub\", \"topic2.pub\"]\n        format: \"json\"\n      }\n    ]\n  }\n\n  virtualSources: [\n    {\n      id: \"sqlVS\"\n      kind: \"sql\"\n      parentSources: [\"hive_source_1\"]\n      persist: \"disk_only\"\n      save: {\n        kind: \"orc\"\n        path: ${basePath}\"/sqlVs\"\n      }\n      query: \"select id, name, entity, description from hive_source_1 where dlk_cob_date == '2023-06-30'\"\n    }\n    {\n      id: \"joinVS\"\n      kind: \"join\"\n      parentSources: [\"hdfs_avro_source\", \"hdfs_orc_source\"]\n      joinBy: [\"id\"]\n      joinType: \"leftouter\"\n      persist: \"memory_only\"\n      keyFields: [\"id\", \"order_id\"]\n    }\n    {\n      id: \"filterVS\"\n      kind: \"filter\"\n      parentSources: [\"kafka_source\"]\n      expr: [\"key is not null\"]\n      keyFields: [\"batchId\", \"dttm\"]\n    }\n    {\n      id: \"selectVS\"\n      kind: \"select\"\n      parentSources: [\"table_source_1\"]\n      expr: [\n        \"count(id) as id_cnt\",\n        \"count(name) as name_cnt\"\n      ]\n    }\n    {\n      id: \"aggVS\"\n      kind: \"aggregate\"\n      parentSources: [\"hdfs_fixed_file\"]\n      groupBy: [\"col1\"]\n      expr: [\n        \"avg(col2) as avg_col2\",\n        \"sum(col3) as sum_col3\"\n      ],\n      keyFields: [\"col1\", \"avg_col2\", \"sum_col3\"]\n    }\n  ]\n\n  loadChecks: {\n    exactColumnNum: [\n      {id: \"loadCheck1\", source: \"hdfs_delimited_source\", option: 3}\n    ]\n    minColumnNum: [\n      {id: \"loadCheck2\", source: \"kafka_source\", option: 2}\n    ]\n    columnsExist: [\n      {id: \"loadCheck3\", source: \"sqlVS\", columns: [\"id\", \"name\", \"entity\", \"description\"]},\n      {id: \"load_check_4\", source: \"hdfs_delimited_source\", columns: [\"id\", \"name\", \"value\"]}\n    ]\n    schemaMatch: [\n      {id: \"load_check_5\", source: \"kafka_source\", schema: \"hive_schema\"}\n    ]\n  }\n\n  metrics: {\n    regular: {\n      rowCount: [\n        {id: \"hive_table_row_cnt\", description: \"Row count in hive_source_1\", source: \"hive_source_1\"},\n        {id: \"csv_file_row_cnt\", description: \"Row count in hdfs_delimited_source\", source: \"hdfs_delimited_source\"}\n      ]\n      distinctValues: [\n        {\n          id: \"fixed_file_dist_name\", description: \"Distinct values in hdfs_fixed_file\",\n          source: \"hdfs_fixed_file\", columns: [\"colA\"]\n        }\n      ]\n      nullValues: [\n        {id: \"hive_table_nulls\", description: \"Null values in columns id and name\", source: \"hive_source_1\", columns: [\"id\", \"name\"]}\n      ]\n      completeness: [\n        {id: \"orc_data_compl\", description: \"Completness of column id\", source: \"hdfs_orc_source\", columns: [\"id\"]}\n      ]\n      avgNumber: [\n        {id: \"avro_file1_avg_bal\", description: \"Avg number of column balance\", source: \"hdfs_avro_source\", columns: [\"balance\"]}\n      ]\n      regexMatch: [\n        {\n          id: \"table_source1_inn_regex\", description: \"Regex match for inn column\", source: \"table_source_1\",\n          columns: [\"inn\"], params: {regex: \"\"\"^\\d{10}$\"\"\"}\n        }\n      ]\n      stringInDomain: [\n        {\n          id: \"orc_data_segment_domain\", source: \"hdfs_orc_source\",\n          columns: [\"segment\"], params: {domain: [\"FI\", \"MID\", \"SME\", \"INTL\", \"CIB\"]}\n        }\n      ]\n      topN: [\n        {\n          id: \"filterVS_top3_currency\", description: \"Top 3 currency in filterVS\", source: \"filterVS\",\n          columns: [\"id\"], params: {targetNumber: 3, maxCapacity: 10}\n        }\n      ],\n      levenshteinDistance: [\n        {\n          id: \"lvnstDist\", source: \"table_source_2\", columns: [\"col1\", \"col2\"],\n          params: {normalize: true, threshold: 0.3}\n        }\n      ]\n    }\n    composed: [\n      {\n        id: \"pct_of_null\", description: \"Percent of null values in hive_table1\",\n        formula: \"100 * {{ hive_table_nulls }} ^ 2 / ( {{ hive_table_row_cnt }} + 1)\"\n      }\n    ]\n  }\n\n  checks: {\n    trend: {\n      averageBoundFull: [\n        {\n          id: \"avg_bal_check\",\n          description: \"Check that average balance stays within +/-25% of the week average\"\n          metric: \"avro_file1_avg_bal\",\n          rule: \"datetime\"\n          windowSize: \"8d\"\n          threshold: 0.25\n        }\n      ]\n      averageBoundUpper: [\n        {id: \"avg_pct_null\", metric: \"pct_of_null\", rule: \"datetime\", windowSize: \"15d\", threshold: 0.5}\n      ]\n      averageBoundLower: [\n        {id: \"avg_distinct\", metric: \"fixed_file_dist_name\", rule: \"record\", windowSize: 31, threshold: 0.3}\n      ]\n      averageBoundRange: [\n        {\n          id: \"avg_inn_match\",\n          metric: \"table_source1_inn_regex\",\n          rule: \"datetime\",\n          windowSize: \"8d\",\n          thresholdLower: 0.2\n          thresholdUpper: 0.4\n        }\n      ]\n      topNRank: [\n        {id: \"top2_curr_match\", metric: \"filterVS_top3_currency\", targetNumber: 2, threshold: 0.1}\n      ]\n    }\n    snapshot: {\n      differByLT: [\n        {\n          id: \"row_cnt_diff\",\n          description: \"Number of rows in two tables should not differ on more than 5%.\",\n          metric: \"hive_table_row_cnt\"\n          compareMetric: \"csv_file_row_cnt\"\n          threshold: 0.05\n        }\n      ]\n      equalTo: [\n        {id: \"zero_nulls\", description: \"Hive Table1 mustn't contain nulls\", metric: \"hive_table_nulls\", threshold: 0}\n      ]\n      greaterThan: [\n        {id: \"completeness_check\", metric: \"orc_data_compl\", threshold: 0.99}\n      ]\n      lessThan: [\n        {id: \"null_threshold\", metric: \"pct_of_null\", threshold: 0.01}\n      ]\n    }\n  }\n\n  targets: {\n    results: {\n      file: {\n        resultTypes: [\"checks\", \"loadChecks\"]\n        save: {\n          kind: \"delimited\"\n          path: ${basePath}\"/results/\"${referenceDate}\n          header: true\n        }\n      }\n      hive: {\n        resultTypes: [\"regularMetrics\", \"composedMetrics\", \"loadChecks\", \"checks\"],\n        schema: \"DQ_SCHEMA\",\n        table: \"DQ_TARGETS\"\n      }\n      kafka: {\n        resultTypes: [\"regularMetrics\", \"composedMetrics\", \"loadChecks\", \"checks\"],\n        connection: \"kafka_broker\"\n        topic: \"some.topic\"\n      }\n    }\n    errorCollection: {\n      file: {\n        metrics: [\"pct_of_null\", \"hive_table_row_cnt\", \"hive_table_nulls\"]\n        dumpSize: 50\n        save: {\n          kind: \"orc\"\n          path: ${basePath}\"/errors/\"${referenceDate}\n        }\n      }\n      kafka: {\n        metrics: [\"hive_table_nulls\", \"fixed_file_dist_name\", \"table_source1_inn_regex\"]\n        dumpSize: 25\n        connection: \"kafka_broker\"\n        topic: \"some.topic\"\n        options: [\"addParam=true\"]\n      }\n    }\n    summary: {\n      email: {\n        attachMetricErrors: true\n        metrics: [\"hive_table_nulls\", \"fixed_file_dist_name\", \"table_source1_inn_regex\"]\n        dumpSize: 10\n        recipients: [\"some.person@some.domain\"]\n      }\n      mattermost: {\n        attachMetricErrors: true\n        metrics: [\"hive_table_nulls\", \"fixed_file_dist_name\", \"table_source1_inn_regex\"]\n        dumpSize: 10\n        recipients: [\"@someUser\", \"#someChannel\"]\n      }\n      kafka: {\n        connection: \"kafka_broker\"\n        topic: \"dev.dq_results.topic\"\n      }\n    }\n    checkAlerts: {\n      email: [\n        {\n          id: \"alert1\"\n          checks: [\"avg_bal_check\", \"zero_nulls\"]\n          recipients: [\"some.peron@some.domain\"]\n        }\n        {\n          id: \"alert2\"\n          checks: [\"top2_curr_match\", \"completeness_check\"]\n          recipients: [\"another.peron@some.domain\"]\n        }\n      ]\n      mattermost: [\n        {\n          id: \"alert3\"\n          checks: [\"avg_bal_check\", \"zero_nulls\"]\n          recipients: [\"@someUser\"]\n        }\n        {\n          id: \"alert4\"\n          checks: [\"top2_curr_match\", \"completeness_check\"]\n          recipients: [\"#someChannel\"]\n        }\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"ru/","title":"Home","text":"<p>\u0410\u043a\u0442\u0443\u0430\u043b\u044c\u043d\u0430\u044f \u0432\u0435\u0440\u0441\u0438\u044f: 1.0.0</p> <p>\u0414\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f \u043d\u0430 \u0440\u0443\u0441\u0441\u043a\u043e\u043c \u044f\u0437\u044b\u043a\u0435 \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u0441\u044f \u0432 \u0441\u0442\u0430\u0434\u0438\u0438 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0438. \u041f\u043e\u0436\u0430\u043b\u0443\u0439\u0441\u0442\u0430, \u043f\u043e\u043b\u044c\u0437\u0443\u0439\u0442\u0435\u0441\u044c \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0435\u0439 \u043d\u0430 \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u043c.</p> <p>\u0414\u043b\u044f \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0435\u043d\u0438\u044f \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u0431\u043e\u043b\u044c\u0448\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445, \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0442\u044c \u0440\u0430\u0441\u0447\u0435\u0442\u044b \u0431\u043e\u043b\u044c\u0448\u043e\u0433\u043e \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u043c\u0435\u0442\u0440\u0438\u043a \u0438 \u043f\u0440\u043e\u0432\u0435\u0440\u043e\u043a \u043d\u0430\u0434 \u043e\u0433\u0440\u043e\u043c\u043d\u044b\u043c\u0438 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430\u043c\u0438, \u0447\u0442\u043e \u0432 \u0441\u0432\u043e\u044e \u043e\u0447\u0435\u0440\u0435\u0434\u044c \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0441\u043b\u043e\u0436\u043d\u043e\u0439 \u0437\u0430\u0434\u0430\u0447\u0435\u0439.</p> <p>Checkita - \u044d\u0442\u043e Data Quality \u0444\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0440\u0435\u0448\u0430\u0435\u0442 \u044d\u0442\u0443 \u0437\u0430\u0434\u0430\u0447\u0443, \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u044f \u0444\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0438 \u0443\u043f\u0440\u043e\u0441\u0442\u0438\u0442\u044c \u043f\u0440\u043e\u0446\u0435\u0441\u0441 \u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u044f \u0438 \u0447\u0442\u0435\u043d\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445 \u0438\u0437 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u043e\u0432, \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u044f \u043c\u0435\u0442\u0440\u0438\u043a \u0438 \u043f\u0440\u043e\u0432\u0435\u0440\u043e\u043a \u043d\u0430\u0434 \u0434\u0430\u043d\u043d\u044b\u043c \u0432 \u044d\u0442\u0438\u0445 \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u0430\u0445, \u0430 \u0442\u0430\u043a\u0436\u0435 \u043e\u0442\u043f\u0440\u0430\u0432\u043a\u0443 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u0438 \u0443\u0432\u0435\u0434\u043e\u043c\u043b\u0435\u043d\u0438\u0439 \u043f\u043e \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u043c \u043a\u0430\u043d\u0430\u043b\u0430\u043c.</p> <p>\u0418\u0442\u0430\u043a, Checkita \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0442\u044c \u0440\u0430\u0441\u0447\u0435\u0442 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u043c\u0435\u0442\u0440\u0438\u043a \u0438 \u043f\u0440\u043e\u0432\u0435\u0440\u043e\u043a \u043d\u0430\u0434 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 (\u043a\u0430\u043a \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c\u0438, \u0442\u0430\u043a \u0438 \u043d\u0435\u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c\u0438). \u0424\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a \u0441\u043f\u043e\u0441\u043e\u0431\u0435\u043d \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0442\u044c \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u044b\u0435 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u044f \u043d\u0430\u0434 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0437\u0430 \"\u043e\u0434\u0438\u043d \u043f\u0440\u043e\u0445\u043e\u0434\", \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f Spark \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0432\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u044f\u0434\u0440\u0430. \u041a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0435 Hocon \u0444\u0430\u0439\u043b\u044b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0442\u0441\u044f \u043a\u0430\u043a \u0434\u043b\u044f \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u044f \u043d\u0430\u0441\u0442\u0440\u043e\u0435\u043a \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f \u0438, \u0442\u0430\u043a \u0438 \u0434\u043b\u044f \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u044f \u043f\u0430\u0439\u043f\u043b\u0430\u0439\u043d\u0430 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u044f \u043c\u0435\u0442\u0440\u0438\u043a \u0438 \u043f\u0440\u043e\u0432\u0435\u0440\u043e\u043a. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u0440\u0430\u0441\u0447\u0435\u0442\u043e\u0432 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u044e\u0442\u0441\u044f \u0432 \u0432\u044b\u0434\u0435\u043b\u0435\u043d\u043d\u0443\u044e \u0431\u0430\u0437\u0443 \u0444\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a\u0430, \u0430 \u0442\u0430\u043a\u0436\u0435 \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u043e\u0442\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u044b \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\u043c \u043f\u043e \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u043c \u043a\u0430\u043d\u0430\u043b\u0430\u043c: \u0444\u0430\u0439\u043b (\u043b\u043e\u043a\u0430\u043b\u044c\u043d\u0430\u044f FS, HDFS, S3), Email, Mattermost, Kafka.</p> <p>\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435 Spark \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0432\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u044f\u0434\u0440\u0430 \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0442\u044c \u0440\u0430\u0441\u0447\u0435\u0442\u044b \u043c\u0435\u0442\u0440\u0438\u043a \u0438 \u043f\u0440\u043e\u0432\u0435\u0440\u043e\u043a \u043d\u0430 \u0443\u0440\u043e\u0432\u043d\u0435 \"\u0441\u044b\u0440\u044b\u0445\" \u0434\u0430\u043d\u043d\u044b\u0445, \u043d\u0435 \u0442\u0440\u0435\u0431\u0443\u044f \u043a\u0430\u043a\u0438\u0445-\u043b\u0438\u0431\u043e SQL \u0430\u0431\u0441\u0442\u0440\u0430\u043a\u0446\u0438\u0439 \u043d\u0430\u0434 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 (\u0442\u0430\u043a\u0438\u0445, \u043a\u0430\u043a Hive \u0438\u043b\u0438 Impala), \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0432 \u0441\u0432\u043e\u044e \u043e\u0447\u0435\u0440\u0435\u0434\u044c \u043c\u043e\u0433\u0443\u0442 \u0441\u043a\u0440\u044b\u0432\u0430\u0442\u044c \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043e\u0448\u0438\u0431\u043a\u0438 \u0432 \u0434\u0430\u043d\u043d\u044b\u0445 (\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u043f\u043b\u043e\u0445\u043e\u0435 \u0444\u043e\u0440\u043c\u0430\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0438\u043b\u0438 \u043d\u0435\u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0438\u044f \u0441\u0445\u0435\u043c\u044b).</p> <p>Checkita \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0442\u044c \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u0435:</p> <ul> <li>\u0427\u0438\u0442\u0430\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435 \u0438\u0437 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u043e\u0432 (HDFS, S3, Hive, Jdbc, Kafka) \u0438 \u0432 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u0444\u043e\u0440\u043c\u0430\u0442\u0430\u0445 (text, orc, parquet, avro).</li> <li>\u041f\u0440\u0438\u043c\u0435\u043d\u044f\u0442\u044c SQL \u0437\u0430\u043f\u0440\u043e\u0441\u044b \u043a \u0434\u0430\u043d\u043d\u044b\u043c, \u0442\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c \u0444\u043e\u0440\u043c\u0438\u0440\u0443\u044f \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u043d\u044b\u0435 \"\u0432\u0438\u0440\u0442\u0443\u0430\u043b\u044c\u043d\u044b\u0435 \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u0438\" \u0434\u0430\u043d\u043d\u044b\u0445.   \u0414\u0430\u043d\u043d\u044b\u0439 \u0444\u0443\u043d\u043a\u0446\u0438\u043e\u043d\u0430\u043b \u043e\u0441\u0443\u0449\u0435\u0441\u0442\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043f\u043e\u0441\u0440\u0435\u0434\u0441\u0442\u0432\u043e\u043c Spark DataFrame API.</li> <li>\u0412\u044b\u043f\u043e\u043b\u043d\u044f\u0442\u044c \u0440\u0430\u0441\u0447\u0435\u0442 \u0448\u0438\u0440\u043e\u043a\u043e\u0433\u043e \u0441\u043f\u0435\u043a\u0442\u0440\u0430 \u043c\u0435\u0442\u0440\u0438\u043a \u043d\u0430\u0434 \u0434\u0430\u043d\u043d\u044b\u043c\u0438, \u0430 \u0442\u0430\u043a\u0436\u0435 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0442\u044c \u043a\u043e\u043c\u043f\u043e\u0437\u0438\u0446\u0438\u044e \u043c\u0435\u0442\u0440\u0438\u043a.</li> <li>\u041f\u0440\u043e\u0432\u043e\u0434\u0438\u0442\u044c \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u043d\u0430\u0434 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0440\u0430\u0441\u0441\u0447\u0438\u0442\u0430\u043d\u043d\u044b\u0445 \u043c\u0435\u0442\u0440\u0438\u043a.</li> <li>\u0412\u044b\u043f\u043e\u043b\u043d\u044f\u0442\u044c \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u043e\u0441\u043d\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0438\u0445 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430\u0445 \u0440\u0430\u0441\u0447\u0435\u0442\u0430 (\u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u0435 \u0430\u043d\u043e\u043c\u0430\u043b\u0438\u0439 \u0432 \u0434\u0430\u043d\u043d\u044b\u0445).</li> <li>\u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0442\u044c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u0440\u0430\u0441\u0447\u0435\u0442\u0430 \u0432 \u0431\u0430\u0437\u0443 \u0434\u0430\u043d\u043d\u044b\u0445 \u0444\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a\u0430, \u0430 \u043e\u0442\u043f\u0440\u0430\u0432\u043b\u044f\u0442\u044c \u0438\u0445 \u043f\u043e \u0434\u0440\u0443\u0433\u0438\u043c \u043a\u0430\u043d\u0430\u043b\u0430\u043c   (HDFS, S3, Hive, Kafka, Email, Mattermost).</li> </ul> <p>Checkita \u0440\u0430\u0437\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u0435\u0442\u0441\u044f \u0441 \u0444\u043e\u043a\u0443\u0441\u043e\u043c \u043d\u0430 \u0438\u043d\u0442\u0435\u0433\u0440\u0430\u0446\u0438\u044e \u0432 ETL \u043f\u0430\u0439\u043f\u043b\u0430\u0439\u043d\u044b \u0438 \u0441\u0438\u0441\u0442\u0435\u043c\u044b \u043a\u0430\u0442\u0430\u043b\u043e\u0433\u043e\u0432 \u0434\u0430\u043d\u043d\u044b\u0445:</p> <ul> <li>\u0424\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442 Spark \u0438 \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u0437\u0430\u043f\u0443\u0449\u0435\u043d \u043a\u0430\u043a \u043e\u0431\u044b\u0447\u043d\u043e\u0435 Spark-\u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435. Spark, \u0432 \u0441\u0432\u043e\u044e \u043e\u0447\u0435\u0440\u0435\u0434\u044c,   \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0448\u0438\u0440\u043e\u043a\u043e \u0440\u0430\u0441\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0435\u043d\u043d\u044b\u043c \u0440\u0435\u0448\u0435\u043d\u0438\u0435\u043c \u0434\u043b\u044f \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0439 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0431\u043e\u043b\u044c\u0448\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445.</li> <li>No-code \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u044f \u043f\u0430\u0439\u043f\u043b\u0430\u0439\u043d\u043e\u0432 \u043f\u043e\u0441\u0440\u0435\u0434\u0441\u0442\u0432\u043e\u043c Hocon \u0444\u0430\u0439\u043b\u043e\u0432, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u043b\u0435\u0433\u043a\u043e \u0441\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u044b \u0438 \u0432\u0435\u0440\u0441\u0438\u043e\u043d\u0438\u0440\u043e\u0432\u0430\u043d\u044b \u0432 VSC.</li> <li>\u0412\u044b\u0434\u0435\u043b\u0435\u043d\u043d\u0430\u044f \u0431\u0430\u0437\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u0445\u0440\u0430\u043d\u0435\u043d\u0438\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0430 \u0434\u043b\u044f \u043f\u043e\u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u0433\u043e \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f   \u044d\u0442\u0438\u0445 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u043f\u043e\u0441\u0440\u0435\u0434\u0441\u0442\u0432\u043e\u043c \u0434\u0435\u0448-\u0431\u043e\u0440\u0434\u043e\u0432 \u0438\u043b\u0438 \u043f\u0440\u043e\u0441\u0442\u044b\u0445 UI.</li> <li>\u0412\u0441\u0442\u0440\u043e\u0435\u043d\u043d\u0430\u044f \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0430 \u043d\u043e\u0442\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0439 (Email, Mattermost) \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0438\u043d\u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439   \u043e \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u0430\u0445 \u0441 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e\u043c \u0434\u0430\u043d\u043d\u044b\u0445.</li> <li>\u0410\u043b\u044c\u0442\u0435\u0440\u043d\u0430\u0442\u0438\u0432\u043d\u044b\u0435 \u043a\u0430\u043d\u0430\u043b\u044b \u043e\u0442\u043f\u0440\u0430\u0432\u043a\u0438 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432, \u0442\u0430\u043a\u0438\u0435 \u043a\u0430\u043a Kafka, \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u044b \u0434\u043b\u044f \u0438\u043d\u0442\u0435\u0433\u0440\u0430\u0446\u0438\u0438 \u0441 \u0434\u0440\u0443\u0433\u0438\u043c\u0438 \u0441\u0435\u0440\u0432\u0438\u0441\u0430\u043c\u0438.</li> </ul>"},{"location":"ru/#_1","title":"\u041e\u0441\u043d\u043e\u0432\u043d\u044b\u0435 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438","text":"<p>\u0424\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a \u043d\u0430\u043f\u0438\u0441\u0430\u043d \u043d\u0430 Scala 2.12 \u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442 Spark 2.4+ \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0432\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u044f\u0434\u0440\u0430. \u0412 \u043f\u0440\u043e\u0435\u043a\u0442\u0435 \u043d\u0430\u0441\u0442\u0440\u043e\u0435\u043d\u0430 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0438\u0437\u0443\u0435\u043c\u0430\u044f \u0441\u0431\u043e\u0440\u043a\u0430, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0441\u043e\u0431\u0438\u0440\u0430\u0442\u044c \u0444\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a \u043f\u043e\u0434 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u0443\u044e \u0432\u0435\u0440\u0441\u0438\u044e Spark, \u043f\u0443\u0431\u043b\u0438\u043a\u043e\u0432\u0430\u0442\u044c \u043f\u0440\u043e\u0435\u043a\u0442 \u0432 \u0437\u0430\u0434\u0430\u043d\u043d\u044b\u0439 \u0440\u0435\u043f\u043e\u0437\u0438\u0442\u043e\u0440\u0438\u0439, \u0430 \u0442\u0430\u043a\u0436\u0435 \u0441\u043e\u0431\u0438\u0440\u0430\u0442\u044c Uber-jar, \u043a\u0430\u043a \u0441 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u044f\u043c\u0438 Spark, \u0442\u0430\u043a \u0438 \u0431\u0435\u0437 \u043d\u0438\u0445.</p>"},{"location":"ru/01-application-setup/","title":"\u041e\u0431\u0449\u0430\u044f \u0418\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f","text":"<p>Checkita \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u043a\u0430\u043a Spark-\u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435. \u0421\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u043e, \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u0437\u0430\u043f\u0443\u0449\u0435\u043d\u043e \u0442\u0430\u043a\u0438\u043c \u0436\u0435 \u043e\u0431\u0440\u0430\u0437\u043e\u043c, \u043a\u0430\u043a \u0438 \u043b\u044e\u0431\u043e\u0435 \u0434\u0440\u0443\u0433\u043e\u0435 Spark-\u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435:</p> <ul> <li>\u043b\u043e\u043a\u0430\u043b\u044c\u043d\u043e, \u043d\u0430 \u043a\u043b\u0438\u0435\u043d\u0442\u0441\u043a\u043e\u0439 \u043c\u0430\u0448\u0438\u043d\u0435;</li> <li>\u0432 \u0432\u044b\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u043c Spark-\u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0435;</li> <li>\u0447\u0435\u0440\u0435\u0437 \u043c\u0435\u043d\u0435\u0434\u0436\u0435\u0440 \u0440\u0435\u0441\u0443\u0440\u0441\u043e\u0432 (Yarn, Mesos);</li> <li>\u0432 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0435 Kubernetes.</li> </ul> <p>\u0422\u0430\u043a\u0436\u0435 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u0438\u0432\u0430\u044e\u0442\u0441\u044f \u043e\u0431\u0430 \u0440\u0435\u0436\u0438\u043c\u0430 \u0437\u0430\u043f\u0443\u0441\u043a\u0430 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f: <code>client</code> and <code>cluster</code>.</p> <p>\u0424\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a \u0440\u0430\u0437\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u0435\u0442\u0441\u044f \u0432 \u043f\u0435\u0440\u0432\u0443\u044e \u043e\u0447\u0435\u0440\u0435\u0434\u044c \u0434\u043b\u044f \u043f\u0430\u043a\u0435\u0442\u043d\u043e\u0439 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0439 \u043c\u043e\u043c\u0435\u043d\u0442 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u0438\u0432\u0430\u0435\u0442 \u0442\u043e\u043b\u044c\u043a\u043e \u0442\u0430\u043a\u043e\u0439 \u0440\u0435\u0436\u0438\u043c \u0440\u0430\u0431\u043e\u0442\u044b. \u0422\u0438\u043f\u043e\u0432\u0430\u044f \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0430 \u0434\u043b\u044f \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u0444\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a\u043e\u043c \u043f\u043e\u043a\u0430\u0437\u0430\u043d\u0430 \u043d\u0430 \u0441\u0445\u0435\u043c\u0435 \u043d\u0438\u0436\u0435:</p> <ul> <li>\u0421\u043e\u0431\u0438\u0440\u0430\u0435\u0442\u0441\u044f Uber-jar \u0444\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a\u0430 (\u043e\u0431\u044b\u0447\u043d\u043e \u0431\u0435\u0437 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0435\u0439 \u0441\u0430\u043c\u043e\u0433\u043e Spark, \u0442.\u043a. \u043e\u043d\u0438 \u0443\u0436\u0435 \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u044b \u043d\u0430 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0435).</li> <li>\u041f\u043e\u0434\u0433\u043e\u0442\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u0442\u0441\u044f Hocon \u0444\u0430\u0439\u043b \u0441 \u043e\u0431\u0449\u0438\u043c\u0438 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430\u043c\u0438 \u0444\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a\u0430.</li> <li>\u0424\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0439 \u0444\u0430\u0439\u043b \u0441 \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u0435\u043c Data Quality \u043f\u0430\u0439\u043f\u043b\u0430\u0439\u043d\u0430 \u0432 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0438\u0438 \u0441 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0435\u0439.</li> <li>\u0417\u0430\u043f\u0443\u0441\u043a\u0430\u0435\u0442\u0441\u044f Spark Application.</li> <li>Spark Application \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u0442 \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u0438 \u043e\u043f\u0438\u0441\u0430\u043d\u043d\u044b\u0435 \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u043c \u0444\u0430\u0439\u043b\u0435 (HDFS, S3, Hive, \u0432\u043d\u0435\u0448\u043d\u0438\u0435 \u0411\u0414),   \u0440\u0430\u0441\u0441\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u0442 \u043c\u0435\u0442\u0440\u0438\u043a\u0438, \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u0438 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b:</li> <li>\u041e\u0441\u043d\u043e\u0432\u043d\u044b\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u044e\u0442\u0441\u044f \u0432 \u0431\u0430\u0437\u0443 \u0434\u0430\u043d\u043d\u044b\u0445 \u0444\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a\u0430.</li> <li>\u0414\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u043e, \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u0438 \u0443\u0432\u0435\u0434\u043e\u043c\u043b\u0435\u043d\u0438\u044f \u043e\u0442\u043f\u0440\u0430\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u043f\u043e \u043a\u0430\u043d\u0430\u043b\u0430\u043c \u043d\u0430\u0441\u0442\u0440\u043e\u0435\u043d\u043d\u044b\u043c \u0432 \u043f\u0430\u0439\u043f\u043b\u0430\u0439\u043d\u0435.</li> <li>\u041d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u0440\u0430\u0441\u0447\u0435\u0442\u0430 Data Quality \u0444\u043e\u0440\u043c\u0438\u0440\u0443\u044e\u0442\u0441\u044f \u0434\u0435\u0448-\u0431\u043e\u0440\u0434\u044b \u0434\u043b\u044f \u043c\u043e\u043d\u0438\u0442\u043e\u0440\u0438\u043d\u0433\u0430 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u0434\u0430\u043d\u043d\u044b\u0445   (\u043d\u0435 \u0432\u0445\u043e\u0434\u0438\u0442 \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u043e\u043d\u0430\u043b \u0434\u0430\u043d\u043d\u043e\u0433\u043e \u0444\u0440\u0435\u043c\u0432\u043e\u0440\u043a\u0430).</li> </ul> <p>\u0422\u0430\u043a\u0436\u0435, Data Quality Framework \u043c\u043e\u0436\u0435\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0438 \u0434\u043b\u044f \u043f\u043e\u0442\u043e\u043a\u043e\u0432\u043e\u0439 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0434\u0430\u043d\u043d\u044b\u0445, \u043e\u0434\u043d\u0430\u043a\u043e \u0434\u0430\u043d\u043d\u044b\u0439 \u0444\u0443\u043d\u043a\u0446\u0438\u043e\u043d\u0430\u043b \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u0441\u044f \u043d\u0430 \u0441\u0442\u0430\u0434\u0438\u0438 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0438.</p> <p></p>"},{"location":"ru/01-application-setup/01-ApplicationSettings/","title":"\u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438 \u041f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f","text":"<p>\u041e\u0431\u0449\u0438\u0435 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f Checkita Data Quality \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0438\u0440\u0443\u044e\u0442\u0441\u044f \u0432  Hocon \u0444\u0430\u0439\u043b\u0435 <code>application.conf</code>, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043f\u0435\u0440\u0435\u0434\u0430\u0435\u0442\u0441\u044f \u0432 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0432\u043e \u0432\u0440\u0435\u043c\u044f \u0435\u0433\u043e \u0441\u0442\u0430\u0440\u0442\u0430. \u0412\u0441\u0435 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438 \u0437\u0430\u0434\u0430\u044e\u0442\u0441\u044f \u0432\u043d\u0443\u0442\u0440\u0438 \u0441\u0435\u043a\u0446\u0438\u0438 <code>appConfig</code>.</p> <p>\u0415\u0434\u0438\u043d\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0437\u0430\u0434\u0430\u0435\u0442\u0441\u044f \u043d\u0430 \u0432\u0435\u0440\u0445\u043d\u0435\u043c \u0443\u0440\u043e\u0432\u043d\u0435 - \u044d\u0442\u043e <code>applicationName</code>: \u0438\u043c\u044f Spark \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f. \u042d\u0442\u043e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 \u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0439, \u0438 \u0435\u0441\u043b\u0438 \u043e\u043d \u043d\u0435 \u0437\u0430\u0434\u0430\u043d, \u0442\u043e \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0431\u0443\u0434\u0435\u0442 \u0437\u0430\u043f\u0443\u0449\u0435\u043d\u043e \u0441 \u0438\u043c\u0435\u043d\u0435\u043c <code>Checkita Data Quality</code>  \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e.</p> <p>\u041e\u0441\u0442\u0430\u043b\u044c\u043d\u044b\u0435 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f \u0437\u0430\u0434\u0430\u044e\u0442\u0441\u044f \u0432 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u043f\u043e\u0434-\u0441\u0435\u043a\u0446\u0438\u044f\u0445, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043e\u043f\u0438\u0441\u0430\u043d\u044b \u043d\u0438\u0436\u0435:</p>"},{"location":"ru/01-application-setup/01-ApplicationSettings/#_2","title":"\u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438 \u0434\u0430\u0442\u044b \u0438 \u0432\u0440\u0435\u043c\u0435\u043d\u0438","text":"<p>\u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438 \u0434\u0430\u0442\u044b \u0438 \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u043e\u043f\u0438\u0441\u044b\u0432\u0430\u044e\u0442\u0441\u044f \u0432 \u0441\u0435\u043a\u0446\u0438\u0438 <code>dateTimeOptions</code>. \u0414\u043b\u044f \u0431\u043e\u043b\u0435\u0435 \u043f\u043e\u0434\u0440\u043e\u0431\u043d\u043e\u0439 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u043e \u0440\u0430\u0431\u043e\u0442\u0435 \u0441 \u0434\u0430\u0442\u0430\u043c\u0438 \u0432  \u0444\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a\u0435 Checkita Data Quality, \u0441\u043c. \u0433\u043b\u0430\u0432\u0443 \u0420\u0430\u0431\u043e\u0442\u0430 \u0441 \u0414\u0430\u0442\u0430\u043c\u0438.</p> <p>\u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0434\u043b\u044f \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u0434\u0430\u0442\u0430\u043c\u0438 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0435:</p> <ul> <li><code>timeZone</code> - \u0412\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f \u0437\u043e\u043d\u0430, \u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u0443\u043a\u0430\u0437\u0430\u043d\u043e \u0441\u0442\u0440\u043e\u043a\u043e\u0432\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0434\u0430\u0442\u044b.   \u041e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e, \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e <code>\"UTC\"</code>.</li> <li><code>referenceDateFormat</code> - \u0444\u043e\u0440\u043c\u0430\u0442 \u0434\u0430\u0442\u044b/\u0432\u0440\u0435\u043c\u0435\u043d\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0434\u043b\u044f \u0441\u0442\u0440\u043e\u043a\u043e\u0432\u043e\u0433\u043e \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0440\u0435\u0444\u0435\u0440\u0435\u043d\u0441\u043d\u043e\u0439 \u0434\u0430\u0442\u044b.   \u041e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e, \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e <code>\"yyyy-MM-dd'T'HH:mm:ss.SSS\"</code>.</li> <li><code>executionDateFormat</code> - \u0444\u043e\u0440\u043c\u0430\u0442 \u0434\u0430\u0442\u044b/\u0432\u0440\u0435\u043c\u0435\u043d\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0434\u043b\u044f \u0441\u0442\u0440\u043e\u043a\u043e\u0432\u043e\u0433\u043e \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0434\u0430\u0442\u044b \u0441\u0442\u0430\u0440\u0442\u0430 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f.   \u041e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e, \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e <code>\"yyyy-MM-dd'T'HH:mm:ss.SSS\"</code></li> </ul> <p>\u0415\u0441\u043b\u0438 \u0441\u0435\u043a\u0446\u0438\u044f <code>dateTimeOptions</code> \u043d\u0435 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0430, \u0442\u043e \u0431\u0443\u0434\u0443\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e \u0434\u043b\u044f \u0432\u0441\u0435\u0445  \u0432\u044b\u0448\u0435\u0443\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0445 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432.</p>"},{"location":"ru/01-application-setup/01-ApplicationSettings/#_3","title":"\u0410\u043a\u0442\u0438\u0432\u0430\u0442\u043e\u0440\u044b","text":"<p>\u0421\u0435\u043a\u0446\u0438\u044f <code>enablers</code> \u0432 \u0444\u0430\u0439\u043b\u0435 \u0441 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430\u043c\u0438 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u0442 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0435 \u0431\u0438\u043d\u0430\u0440\u043d\u044b\u0435 \u0430\u043a\u0442\u0438\u0432\u0430\u0442\u043e\u0440\u044b \u0438\u043b\u0438 \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u0438\u0440\u0443\u044e\u0442 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0435 \u0430\u0441\u043f\u0435\u043a\u0442\u044b \u0438\u0441\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f data quality \u043f\u0430\u0439\u043f\u043b\u0430\u0439\u043d\u0430:</p> <ul> <li><code>allowSqlQueries</code> - \u0411\u0438\u043d\u0430\u0440\u043d\u044b\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u0438\u0440\u0443\u0435\u0442 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u043b\u044c\u043d\u044b\u0445 SQL \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u043f\u0440\u0438   \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0438 \u043f\u0430\u0439\u043f\u043b\u0430\u0439\u043d\u0430. \u041e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e, \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e <code>false</code></li> <li><code>allowNotifications</code> - \u0411\u0438\u043d\u0430\u0440\u043d\u044b\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u0438\u0440\u0443\u0435\u0442 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u043e\u0442\u043f\u0440\u0430\u0432\u043a\u0438 \u0443\u0432\u0435\u0434\u043e\u043c\u043b\u0435\u043d\u0438\u0439 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\u043c.   \u041e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e, \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e <code>false</code></li> <li><code>aggregatedKafkaOutput</code> - \u0411\u0438\u043d\u0430\u0440\u043d\u044b\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u043e\u0442\u043f\u0440\u0430\u0432\u043b\u044f\u0442\u044c \u0430\u0433\u0440\u0435\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u044f \u0434\u043b\u044f Kafka \u0442\u0430\u0440\u0433\u0435\u0442\u043e\u0432    (\u043f\u043e \u043e\u0434\u043d\u043e\u043c\u0443 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u044e \u043d\u0430 \u043a\u0430\u0436\u0434\u044b\u0439 \u0442\u0438\u043f \u0442\u0430\u0440\u0433\u0435\u0442\u0430). \u041f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e \u043e\u0442\u043f\u0440\u0430\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u043e\u0435 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u0435 \u0432 Kafka \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u0441\u0443\u0449\u043d\u043e\u0441\u0442\u0438.   \u041e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e, \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e <code>false</code></li> <li><code>enableCaseSensitivity</code> - \u0411\u0438\u043d\u0430\u0440\u043d\u044b\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0443\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u0442 \u0447\u0443\u0432\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u043a \u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0443 \u0432 \u0438\u043c\u0435\u043d\u0430\u0445 \u043a\u043e\u043b\u043e\u043d\u043e\u043a.   \u0422\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u0438\u0440\u0443\u0435\u0442\u0441\u044f, \u043a\u0430\u043a \u0438\u043c\u0435\u043d\u0430 \u043a\u043e\u043b\u043e\u043d\u043e\u043a \u0431\u0443\u0434\u0443\u0442 \u0441\u0440\u0430\u0432\u043d\u0438\u0432\u0430\u0442\u044c\u0441\u044f \u043c\u0435\u0436\u0434\u0443 \u0441\u043e\u0431\u043e\u0439 \u0438 \u043a\u0430\u043a \u0431\u0443\u0434\u0435\u0442 \u043f\u0440\u043e\u0438\u0441\u0445\u043e\u0434\u0438\u0442\u044c \u0438\u0445 \u043f\u043e\u0438\u0441\u043a \u0432 \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u0435.   \u041e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e, \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e <code>false</code></li> <li><code>errorDumpSize</code> - \u041c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043e\u0448\u0438\u0431\u043e\u043a, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u0441\u043e\u0431\u0440\u0430\u043d\u044b \u0434\u043b\u044f \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u043e\u0439 \u043c\u0435\u0442\u0440\u0438\u043a\u0438. \u0424\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a \u0438\u043c\u0435\u0435\u0442   \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u0441\u043e\u0431\u0438\u0440\u0430\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435 \u0438\u0437 \u0441\u0442\u0440\u043e\u043a\u0438 \u0432 \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u0435, \u0434\u043b\u044f \u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0435 \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u0437\u0430\u0432\u0435\u0440\u0448\u0438\u043b\u043e\u0441\u044c \u0441 \u043e\u0448\u0438\u0431\u043a\u043e\u0439. \u041e\u0434\u043d\u0430\u043a\u043e,   \u0434\u043b\u044f \u0442\u043e\u0433\u043e, \u0447\u0442\u043e\u0431\u044b \u043f\u0440\u0435\u0434\u043e\u0442\u0432\u0440\u0430\u0442\u0438\u0442\u044c OOM, \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043e\u0448\u0438\u0431\u043e\u043a \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0438\u0442\u044c \u0432 \u0440\u0430\u0437\u0443\u043c\u043d\u044b\u0445 \u043f\u0440\u0435\u0434\u0435\u043b\u0430\u0445. \u0422\u0430\u043a,   \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e \u0434\u043e\u043f\u0443\u0441\u0442\u0438\u043c\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043e\u0448\u0438\u0431\u043e\u043a \u043d\u0430 \u043c\u0435\u0442\u0440\u0438\u043a\u0443 \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u043e \u043d\u0430 \u0443\u0440\u043e\u0432\u043d\u0435 <code>10000</code>. \u041d\u043e \u0435\u0433\u043e \u0442\u0430\u043a\u0436\u0435 \u043c\u043e\u0436\u043d\u043e \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u043e   \u0441\u043d\u0438\u0437\u0438\u0442\u044c \u0437\u0430\u0434\u0430\u0432 \u044d\u0442\u043e\u0442 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440. \u041e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e, \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e <code>10000</code></li> <li><code>outputRepartition</code> - \u0423\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u0442 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0430\u0440\u0442\u0438\u0446\u0438\u0439 \u043f\u0440\u0438 \u0437\u0430\u043f\u0438\u0441\u0438 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u0432 \u0444\u0430\u0439\u043b\u043e\u0432.   \u041f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u0442\u0441\u044f \u043e\u0434\u0438\u043d \u0444\u0430\u0439\u043b. \u041e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e, \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e <code>1</code></li> </ul> <p>\u0415\u0441\u043b\u0438 \u0441\u0435\u043a\u0446\u0438\u044f <code>enablers</code> \u043d\u0435 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0430, \u0442\u043e \u0431\u0443\u0434\u0443\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u0432\u044b\u0448\u0435\u0443\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0445 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432.</p>"},{"location":"ru/01-application-setup/01-ApplicationSettings/#_4","title":"\u041a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u044f \u0425\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0430","text":"<p>\u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0434\u043b\u044f \u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u044f \u043a \u0431\u0430\u0437\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u0441 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430\u043c\u0438 \u0440\u0430\u0431\u043e\u0442\u044b Data Quality \u043f\u0430\u0439\u043f\u043b\u0430\u0439\u043d\u043e\u0432 \u0437\u0430\u0434\u0430\u044e\u0442\u0441\u044f \u0432 \u0441\u0435\u043a\u0446\u0438\u0438 <code>storage</code> \u0444\u0430\u0439\u043b\u0430 \u0441 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430\u043c\u0438 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f.</p> <p>\u0414\u043b\u044f \u0431\u043e\u043b\u0435\u0435 \u043f\u043e\u0434\u0440\u043e\u0431\u043d\u043e\u0439 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u043e \u0445\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u0441\u043c. \u0433\u043b\u0430\u0432\u0443 \u0425\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0435 \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432.</p> <p>\u0422\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c, \u0434\u043b\u044f \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438 \u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u044f \u043a \u0431\u0430\u0437\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u0441 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430\u043c\u0438, \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0437\u0430\u0434\u0430\u0442\u044c \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b:</p> <ul> <li><code>dbType</code> - \u0422\u0438\u043f \u0431\u0430\u0437\u044b \u0434\u0430\u043d\u043d\u044b\u0445, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0434\u043b\u044f \u0445\u0440\u0430\u043d\u0435\u043d\u0438\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432. \u041e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u043e.</li> <li><code>url</code> - URL \u0434\u043b\u044f \u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u044f \u043a \u0431\u0430\u0437\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 (\u0431\u0435\u0437 \u0443\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043f\u0440\u043e\u0442\u043e\u043a\u043e\u043b\u043e\u0432). \u041e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u043e.</li> <li><code>username</code> - \u0418\u043c\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u0434\u043b\u044f \u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u044f \u043a \u0431\u0430\u0437\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 (\u0435\u0441\u043b\u0438 \u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f). \u041e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e.</li> <li><code>password</code> - \u041f\u0430\u0440\u043e\u043b\u044c \u0434\u043b\u044f \u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u044f \u043a \u0431\u0430\u0437\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 (\u0435\u0441\u043b\u0438 \u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f). \u041e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e.</li> <li><code>schema</code> - \u0421\u0445\u0435\u043c\u0430 \u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u043d\u0430\u0445\u043e\u0434\u044f\u0442\u0441\u044f \u0442\u0430\u0431\u043b\u0438\u0446\u044b \u0441 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430\u043c\u0438 Data Quality (\u0435\u0441\u043b\u0438 \u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f). \u041e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e.</li> </ul> <p>\u0412\u0410\u0416\u041d\u041e \u0415\u0441\u043b\u0438 \u0441\u0435\u043a\u0446\u0438\u044f <code>storage</code> \u043d\u0435 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0430, \u0442\u043e \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0431\u0443\u0434\u0435\u0442 \u0437\u0430\u043f\u0443\u0449\u0435\u043d\u043e \u0431\u0435\u0437 \u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u044f \u043a \u0431\u0430\u0437\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u0441 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430\u043c\u0438:</p> <ul> <li>\u0440\u0435\u0437\u0443\u043b\u0442\u0430\u0442\u044b \u043d\u0435 \u0431\u0443\u0434\u0443\u0442 \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u044b (\u0431\u0443\u0434\u0435\u0442 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u0430 \u0442\u043e\u043b\u044c\u043a\u043e \u043e\u0442\u043f\u0440\u0430\u0432\u043a\u0430 \u0442\u0430\u0440\u0433\u0435\u0442\u043e\u0432);</li> <li>\u0442\u0440\u0435\u043d\u0434\u043e\u0432\u044b\u0435 \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 (\u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0442\u0441\u044f \u0434\u043b\u044f \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0430\u043d\u043e\u043c\u0430\u043b\u0438\u0439 \u0432 \u0434\u0430\u043d\u043d\u044b\u0445) \u043d\u0435 \u0431\u0443\u0434\u0443\u0442 \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u044b, \u0442.\u043a. \u0438\u043c \u0434\u043b\u044f \u0440\u0430\u0431\u043e\u0442\u044b   \u0442\u0440\u0435\u0431\u0443\u044e\u0442\u0441\u044f \u0438\u0441\u0442\u043e\u0440\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0435.</li> </ul>"},{"location":"ru/01-application-setup/01-ApplicationSettings/#email","title":"\u041a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u044f Email","text":"<p>\u0414\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u043e\u0442\u043f\u0440\u0430\u0432\u043b\u044f\u0442\u044c \u0443\u0432\u0435\u0434\u043e\u043c\u043b\u0435\u043d\u0438\u044f \u043d\u0430 Email, \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043d\u0430\u0441\u0442\u0440\u043e\u0438\u0442\u044c \u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435 \u043a SMTP \u0441\u0435\u0440\u0432\u0435\u0440\u0443, \u043a\u043e\u0442\u043e\u0440\u043e\u0435 \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u0432 \u0441\u0435\u043a\u0446\u0438\u0438 <code>email</code> \u0444\u0430\u0439\u043b\u0430 \u0441 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430\u043c\u0438 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f \u0441\u043e \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c\u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438:</p> <ul> <li><code>host</code> - \u0410\u0434\u0440\u0435\u0441 SMTP \u0441\u0435\u0440\u0432\u0435\u0440\u0430. \u041e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u043e.</li> <li><code>port</code> - \u041f\u043e\u0440\u0442 \u0434\u043b\u044f \u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u044f \u043a SMTP \u0441\u0435\u0440\u0432\u0435\u0440\u0443. \u041e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u043e.</li> <li><code>address</code> - \u0410\u0434\u0440\u0435\u0441 \u043e\u0442\u043f\u0440\u0430\u0432\u0438\u0442\u0435\u043b\u044f. \u041e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u043e.</li> <li><code>name</code> - \u0418\u043c\u044f \u043e\u0442\u043f\u0440\u0430\u0432\u0438\u0442\u0435\u043b\u044f. \u041e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u043e.</li> <li><code>sslOnConnect</code> - \u0411\u0438\u043d\u0430\u0440\u043d\u044b\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440, \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0449\u0438\u0439 \u043d\u0430 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u044c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f SSL \u0434\u043b\u044f \u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u044f.   \u041e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e, \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e <code>false</code>.</li> <li><code>tlsEnabled</code> - \u0411\u0438\u043d\u0430\u0440\u043d\u044b\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440, \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0449\u0438\u0439 \u043d\u0430 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u044c \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0438 TLS \u043f\u0440\u0438 \u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0438.   \u041e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e, \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e <code>false</code>.</li> <li><code>username</code> - \u0418\u043c\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u0434\u043b\u044f \u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u044f \u043a SMTP \u0441\u0435\u0440\u0432\u0435\u0440\u0443 (\u0435\u0441\u043b\u0438 \u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f). \u041e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e.</li> <li><code>password</code> - \u041f\u0430\u0440\u043e\u043b\u044c \u0434\u043b\u044f \u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u044f \u043a SMTP \u0441\u0435\u0440\u0432\u0435\u0440\u0443 (\u0435\u0441\u043b\u0438 \u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f). \u041e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e.</li> </ul> <p>\u0415\u0441\u043b\u0438 \u0441\u0435\u043a\u0446\u0438\u044f <code>email</code> \u043d\u0435 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0430, \u0442\u043e \u0443\u0432\u0435\u0434\u043e\u043c\u043b\u0435\u043d\u0438\u044f \u043d\u0430 Email \u043d\u0435 \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u043e\u0442\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u044b. \u041f\u0440\u0438 \u044d\u0442\u043e\u043c, \u0435\u0441\u043b\u0438 \u0442\u0430\u043a\u0438\u0435 \u0443\u0432\u0435\u0434\u043e\u043c\u043b\u0435\u043d\u0438\u044f \u0431\u044b\u043b\u0438 \u0441\u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0438\u0440\u043e\u0432\u0430\u043d\u044b \u0432 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430\u0445 \u043f\u0430\u0439\u043f\u043b\u0430\u0439\u043d\u0430, \u0442\u043e \u0432\u043e \u0432\u0440\u0435\u043c\u044f \u0438\u0441\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f \u0431\u0443\u0434\u0435\u0442 \u0431\u0440\u043e\u0448\u0435\u043d\u0430 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0432\u0443\u044e\u0449\u0430\u044f \u043e\u0448\u0438\u0431\u043a\u0430.</p>"},{"location":"ru/01-application-setup/01-ApplicationSettings/#mattermost","title":"\u041a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u044f Mattermost","text":"<p>\u0414\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u043e\u0442\u043f\u0440\u0430\u0432\u043b\u044f\u0442\u044c \u0443\u0432\u0435\u0434\u043e\u043c\u043b\u0435\u043d\u0438\u044f \u0432 Mattermost, \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043d\u0430\u0441\u0442\u0440\u043e\u0438\u0442\u044c \u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435 \u043a Mattermost API, \u043a\u043e\u0442\u043e\u0440\u043e\u0435 \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u0432 \u0441\u0435\u043a\u0446\u0438\u0438 <code>mattermost</code> \u0444\u0430\u0439\u043b\u0430 \u0441 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430\u043c\u0438 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f \u0441\u043e \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c\u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438:</p> <ul> <li><code>host</code> - \u0430\u0434\u0440\u0435\u0441 \u043f\u043e \u043a\u043e\u0442\u043e\u0440\u043e\u043c\u0443 \u0434\u043e\u0441\u0442\u0443\u043f\u0435\u043d Mattermost API.</li> <li><code>token</code> - \u0422\u043e\u043a\u0435\u043d \u0434\u043b\u044f \u0434\u043e\u0441\u0442\u0443\u043f\u0430 \u043a Mattermost API (\u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u0431\u043e\u0442\u043e\u0432 \u043f\u0440\u0435\u0434\u043f\u043e\u0447\u0442\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u0434\u043b\u044f \u043e\u0442\u043f\u0440\u0430\u0432\u043a\u0438 \u0443\u0432\u0435\u0434\u043e\u043c\u043b\u0435\u043d\u0438\u0439).</li> </ul> <p>\u0415\u0441\u043b\u0438 \u0441\u0435\u043a\u0446\u0438\u044f <code>mattermost</code> \u043d\u0435 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0430, \u0442\u043e \u0443\u0432\u0435\u0434\u043e\u043c\u043b\u0435\u043d\u0438\u044f \u0432 Mattermost \u043d\u0435 \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u043e\u0442\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u044b. \u041f\u0440\u0438 \u044d\u0442\u043e\u043c, \u0435\u0441\u043b\u0438 \u0442\u0430\u043a\u0438\u0435 \u0443\u0432\u0435\u0434\u043e\u043c\u043b\u0435\u043d\u0438\u044f \u0431\u044b\u043b\u0438 \u0441\u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0438\u0440\u043e\u0432\u0430\u043d\u044b \u0432 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430\u0445 \u043f\u0430\u0439\u043f\u043b\u0430\u0439\u043d\u0430, \u0442\u043e \u0432\u043e \u0432\u0440\u0435\u043c\u044f \u0438\u0441\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f \u0431\u0443\u0434\u0435\u0442 \u0431\u0440\u043e\u0448\u0435\u043d\u0430 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0432\u0443\u044e\u0449\u0430\u044f \u043e\u0448\u0438\u0431\u043a\u0430.</p>"},{"location":"ru/01-application-setup/01-ApplicationSettings/#spark","title":"\u041e\u0431\u0449\u0438\u0435 Spark \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b","text":"<p>\u0412 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430\u0445 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f \u0442\u0430\u043a\u0436\u0435 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e \u0443\u043a\u0430\u0437\u0430\u0442\u044c \u043d\u0430\u0431\u043e\u0440 Spark-\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u043e\u0431\u0449\u0438\u043c\u0438 \u0434\u043b\u044f \u0431\u043e\u043b\u044c\u0448\u0438\u043d\u0441\u0442\u0432\u0430 \u0437\u0430\u043f\u0443\u0441\u043a\u0430\u0435\u043c\u044b\u0445 Data Quality \u043f\u0430\u0439\u043f\u043b\u0430\u0439\u043d\u043e\u0432. \u0422\u0430\u043a\u0438\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0434\u043e\u043b\u0436\u043d\u044b \u0431\u044b\u0442\u044c \u0443\u043a\u0430\u0437\u0430\u043d\u044b \u0432 \u0441\u043f\u0438\u0441\u043a\u0435 <code>defaultSparkOptions</code> \u043a\u0430\u043a \u0441\u0442\u0440\u043e\u043a\u0438 \u0432 \u0444\u043e\u0440\u043c\u0430\u0442\u0435 <code>spark.param.name=spark.param.value</code>.</p>"},{"location":"ru/01-application-setup/01-ApplicationSettings/#_5","title":"\u041f\u0440\u0438\u043c\u0435\u0440 \u0424\u0430\u0439\u043b\u0430 \u0441 \u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430\u043c\u0438 \u041f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f","text":"<p>Hocon \u0444\u043e\u0440\u043c\u0430\u0442 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u0438\u0432\u0430\u0435\u0442 \u043f\u043e\u0434\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0443 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445, \u0430 \u0444\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a Checkita Data Quality, \u0432 \u0441\u0432\u043e\u044e \u043e\u0447\u0435\u0440\u0435\u0434\u044c, \u0438\u043c\u0435\u0435\u0442 \u043c\u0435\u0445\u0430\u043d\u0438\u0437\u043c \u0434\u043b\u044f \u0442\u043e\u0433\u043e, \u0447\u0442\u043e\u0431\u044b \u0434\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0435 \u0444\u0430\u0439\u043b\u044b \u043f\u043e \u0432\u0440\u0435\u043c\u044f \u0438\u0441\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f. \u0414\u043b\u044f \u0431\u043e\u043b\u0435\u0435 \u043f\u043e\u0434\u0440\u043e\u0431\u043d\u043e\u0439 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438, \u0441\u043c. \u0433\u043b\u0430\u0432\u0443 \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u041f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u041e\u043a\u0440\u0443\u0436\u0435\u043d\u0438\u044f \u0438 \u0414\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u041f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445.</p> <pre><code>appConfig: {\n\n  applicationName: \"Custom Data Quality Application Name\"\n\n  dateTimeOptions: {\n    timeZone: \"GMT+3\"\n    referenceDateFormat: \"yyyy-MM-dd\"\n    executionDateFormat: \"yyyy-MM-dd-HH-mm-ss\"\n  }\n\n  enablers: {\n    allowSqlQueries: false\n    allowNotifications: true\n    aggregatedKafkaOutput: true\n  }\n\n  defaultSparkOptions: [\n    \"spark.sql.orc.enabled=true\"\n    \"spark.sql.parquet.compression.codec=snappy\"\n    \"spark.sql.autoBroadcastJoinThreshold=-1\"\n  ]\n\n  storage: {\n    dbType: \"postgres\"\n    url: \"localhost:5432/public\"\n    username: \"postgres\"\n    password: \"postgres\"\n    schema: \"dqdb\"\n  }\n\n  email: {\n    host: \"smtp.some-company.domain\"\n    port: \"25\"\n    username: \"emailUser\"\n    password: \"emailPassword\"\n    address: \"some.service@some-company.domain\"\n    name: \"Data Quality Service\"\n    sslOnConnect: true\n  }\n\n  mattermost: {\n    host: \"https://some-team.mattermost.com\"\n    token: ${dqMattermostToken}\n  }\n}\n</code></pre>"},{"location":"ru/01-application-setup/02-ApplicationSubmit/","title":"\u0417\u0430\u043f\u0443\u0441\u043a \u041f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0439 Data Quality","text":"<p>\u041f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 Checkita \u0444\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 Spark, \u0442\u043e \u043e\u043d \u0437\u0430\u043f\u0443\u0441\u043a\u0430\u0435\u0442\u0441\u044f \u043a\u0430\u043a \u043e\u0431\u044b\u0447\u043d\u043e\u0435 Spark-\u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u043a\u043e\u043c\u0430\u043d\u0434\u0443 <code>spark-submit</code>. \u041a\u0430\u043a \u0438 \u043b\u044e\u0431\u043e\u0435 \u0434\u0440\u0443\u0433\u043e\u0435 Spark-\u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435, \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435 Checkita \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u0437\u0430\u043f\u0443\u0449\u0435\u043d\u043e \u043a\u0430\u043a \u043b\u043e\u043a\u0430\u043b\u044c\u043d\u043e, \u0442\u0430\u043a \u0438 \u0432 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0435 (\u0432 \u0440\u0435\u0436\u0438\u043c\u0435 <code>client</code> \u0438\u043b\u0438 <code>cluster</code>).</p> <p>\u041e\u0434\u043d\u0430\u043a\u043e, \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f Checkita \u0442\u0440\u0435\u0431\u0443\u044e\u0442 \u043f\u0435\u0440\u0435\u0434\u0430\u0447\u0438 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u044b\u0439 \u0430\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u043f\u0440\u0438 \u0441\u0442\u0430\u0440\u0442\u0435, \u0430 \u0438\u043c\u0435\u043d\u043d\u043e:</p> <ul> <li><code>-a</code> - \u041e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u043e. \u041f\u0443\u0442\u044c \u0434\u043e HOCON \u0444\u0430\u0439\u043b\u0430 \u0441 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430\u043c\u0438 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f: <code>applicaiton.conf</code>. \u0421\u0442\u043e\u0438\u0442 \u043e\u0442\u043c\u0435\u0442\u0438\u0442\u044c, \u0447\u0442\u043e \u0438\u043c\u044f \u0444\u0430\u0439\u043b\u0430   \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u0434\u0440\u0443\u0433\u0438\u043c, \u043e\u0434\u043d\u0430\u043a\u043e \u043e\u0431\u044b\u0447\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0444\u0430\u0439\u043b \u0441 \u0442\u0430\u043a\u0438\u043c \u0438\u043c\u0435\u043d\u0435\u043c.</li> <li><code>-j</code> - \u041e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u043e. \u0421\u043f\u0438\u0441\u043e\u043a \u043f\u0443\u0442\u0435\u0439 \u0434\u043e \u0444\u0430\u0439\u043b\u043e\u0432 \u0441 \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u044f\u043c\u0438 Data Quality \u043f\u0430\u0439\u043f\u043b\u0430\u0439\u043d\u0430. \u041f\u0443\u0442\u0438 \u0434\u043e\u043b\u0436\u043d\u044b \u0431\u044b\u0442\u044c \u0440\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u044b    \u0437\u0430\u043f\u044f\u0442\u044b\u043c\u0438. HOCON \u0444\u043e\u0440\u043c\u0430\u0442 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u0438\u0432\u0430\u0435\u0442 \u0441\u043b\u0438\u044f\u043d\u0438\u0435 \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u044f, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043c\u043e\u0436\u043d\u043e \u043e\u043f\u0438\u0441\u044b\u0432\u0430\u0442\u044c \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0435 \u0447\u0430\u0441\u0442\u0438 \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u0438   \u043f\u0430\u0439\u043f\u043b\u0430\u0439\u043d\u0430 \u0432 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0445 \u0444\u0430\u0439\u043b\u0430\u0445 \u0438 \u043f\u0435\u0440\u0435\u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0438\u0445.</li> <li><code>-d</code> - \u041e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e. \u0414\u0430\u0442\u0430 \u0437\u0430 \u043a\u043e\u0442\u043e\u0440\u0443\u044e \u0437\u0430\u043f\u0443\u0441\u043a\u0430\u0435\u0442\u0441\u044f Data Quality \u043f\u0430\u0439\u043f\u043b\u0430\u0439\u043d. \u0424\u043e\u0440\u043c\u0430\u0442, \u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u0443\u043a\u0430\u0437\u0430\u043d\u0430 \u0434\u0430\u0442\u0430 \u0434\u043e\u043b\u0436\u0435\u043d    \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c \u0442\u043e\u043c\u0443, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0443\u043a\u0430\u0437\u0430\u043d \u0432 \u043f\u043e\u043b\u0435 <code>referenceDateFormat</code> \u0432 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430\u0445 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f. \u0415\u0441\u043b\u0438 \u0434\u0430\u0442\u0430 \u043d\u0435 \u0443\u043a\u0430\u0437\u0430\u043d\u0430,   \u0442\u043e \u0435\u0439 \u0431\u0443\u0434\u0435\u0442 \u043f\u0440\u0438\u0441\u0432\u043e\u0435\u043d\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0444\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0434\u0430\u0442\u044b \u0441\u0442\u0430\u0440\u0442\u0430 \u043f\u0430\u0439\u043f\u043b\u0430\u0439\u043d\u0430.</li> <li><code>-l</code> - \u041e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e. \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442 \u043d\u0430 \u0442\u043e, \u0447\u0442\u043e \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0434\u043e\u043b\u0436\u043d\u043e \u0431\u044b\u0442\u044c \u0437\u0430\u043f\u0443\u0449\u0435\u043d\u043e \u0432 \u043b\u043e\u043a\u0430\u043b\u044c\u043d\u043e\u043c \u0440\u0435\u0436\u0438\u043c\u0435.</li> <li><code>-s</code> - \u041e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e. \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442 \u043d\u0430 \u0442\u043e, \u0447\u0442\u043e \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0431\u0443\u0434\u0435\u0442 \u0437\u0430\u043f\u0443\u0449\u0435\u043d\u043e \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c Shared   Spark Context. \u0412 \u044d\u0442\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0435 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0431\u0443\u0434\u0435\u0442 \u043f\u043e\u043b\u0443\u0447\u0430\u0442\u044c \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0439 Spark Context, \u0432\u043c\u0435\u0441\u0442\u043e \u0442\u043e\u0433\u043e, \u0447\u0442\u043e\u0431\u044b \u0441\u043e\u0437\u0434\u0430\u0432\u0430\u0442\u044c \u043d\u043e\u0432\u044b\u0439.   \u0422\u0430\u043a\u0436\u0435, \u0432\u0430\u0436\u043d\u043e, \u0447\u0442\u043e\u0431\u044b \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0432 \u044d\u0442\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0435 \u043d\u0435 \u043e\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u043b\u043e Spark Context \u043f\u043e \u0437\u0430\u0432\u0435\u0440\u0448\u0435\u043d\u0438\u0438.</li> <li><code>-m</code> - \u041e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e. \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442 \u043d\u0430 \u0442\u043e, \u0447\u0442\u043e \u043c\u0438\u0433\u0440\u0430\u0446\u0438\u044f \u0431\u0430\u0437\u044b \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043e\u043b\u0436\u043d\u0430 \u0431\u044b\u0442\u044c \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0430 \u043f\u0435\u0440\u0435\u0434   \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u0435\u043c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 (\u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0443\u0431\u0435\u0434\u0438\u0442\u044c\u0441\u044f \u0432 \u0442\u043e\u043c, \u0447\u0442\u043e \u0445\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u0432 \u0430\u043a\u0442\u0443\u0430\u043b\u044c\u043d\u043e\u043c \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0438 \u0438\u043b\u0438 \u0432\u044b\u043f\u043e\u043b\u043d\u0438\u0442\u044c \u0441\u043a\u0440\u0438\u043f\u0442\u044b   \u0434\u043b\u044f \u043f\u0440\u0438\u0432\u0435\u0434\u0435\u043d\u0438\u044f \u0435\u0433\u043e \u043a \u0430\u043a\u0442\u0443\u0430\u043b\u044c\u043d\u043e\u043c\u0443 \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u044e).</li> <li><code>-e</code> - \u041e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e. \u0424\u043b\u0430\u0433, \u0441 \u043a\u043e\u0442\u043e\u0440\u044b\u043c \u043c\u043e\u0436\u043d\u043e \u043f\u0435\u0440\u0435\u0434\u0430\u0442\u044c \u043d\u0430\u0431\u043e\u0440 \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u043f\u0440\u0438 \u0441\u0442\u0430\u0440\u0442\u0435 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f.    \u0414\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u0431\u0443\u0434\u0443\u0442 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u044b \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0435 \u0444\u0430\u0439\u043b\u044b \u0438 \u0431\u0443\u0434\u0443\u0442 \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u044b \u0434\u043b\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f. \u041f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435    \u043f\u0435\u0440\u0435\u0434\u0430\u044e\u0442\u0441\u044f \u0432 \u0444\u043e\u0440\u043c\u0430\u0442\u0435 \u043a\u043b\u044e\u0447-\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435: <code>\"k1=v1,k2=v2,k3=v3,...\"\"</code>.</li> <li><code>-v</code> - \u041e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e. \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442, \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u043c\u043e\u0436\u043d\u043e \u043d\u0430\u0437\u043d\u0430\u0447\u0438\u0442\u044c \u0443\u0440\u043e\u0432\u0435\u043d\u044c \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0432 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0438.   \u041f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e - <code>INFO</code>.</li> </ul> <p>\u041d\u0438\u0436\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d \u043f\u0440\u0438\u043c\u0435\u0440 \u0437\u0430\u043f\u0443\u0441\u043a\u0430 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f Checkita \u0432 YARN \u0432 <code>cluster</code> \u0440\u0435\u0436\u0438\u043c\u0435. \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u044f \u043a \u0445\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0443 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u0443\u043a\u0430\u0437\u0430\u043d\u044b \u0432 \u0444\u0430\u0439\u043b\u0435 <code>application.conf</code>, \u043f\u0440\u0438 \u044d\u0442\u043e\u043c \u0440\u0435\u043a\u0432\u0438\u0437\u0438\u0442\u044b \u0434\u043b\u044f \u0432\u0445\u043e\u0434\u0430 \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u043f\u0435\u0440\u0435\u0434\u0430\u043d\u044b \u043a\u0430\u043a \u043f\u043e\u0441\u0440\u0435\u0434\u0441\u0442\u0432\u043e\u043c \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u043e\u043a\u0440\u0443\u0436\u0435\u043d\u0438\u044f, \u0442\u0430\u043a \u0438 \u0432 \u0432\u0438\u0434\u0435 \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u043f\u0440\u0438 \u0441\u0442\u0430\u0440\u0442\u0435.  \u0414\u043b\u044f \u0431\u043e\u043b\u0435\u0435 \u043f\u043e\u0434\u0440\u043e\u0431\u043d\u043e\u0439 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438, \u0441\u043c. \u0433\u043b\u0430\u0432\u0443 \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u041f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u041e\u043a\u0440\u0443\u0436\u0435\u043d\u0438\u044f \u0438 \u0414\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u041f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445.</p> <pre><code>export DQ_APPLICATION=\"&lt;\u043b\u043e\u043a\u0430\u043b\u044c\u043d\u044b\u0439 \u0438\u043b\u0438 \u0443\u0434\u0430\u043b\u0435\u043d\u043d\u044b\u0439 (HDFS, S3) \u043f\u0443\u0442\u044c \u0434\u043e uber-jar \u0441 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435\u043c&gt;\"\nexport DQ_APP_CONFIG=\"&lt;\u043b\u043e\u043a\u0430\u043b\u044c\u043d\u044b\u0439 \u0438\u043b\u0438 \u0443\u0434\u0430\u043b\u0435\u043d\u043d\u044b\u0439 (HDFS, S3) \u043f\u0443\u0442\u044c \u0434\u043e \u0444\u0430\u0439\u043b\u0430 \u0441 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430\u043c\u0438 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f&gt;\"\nexport DQ_JOB_CONFIGS=\"&lt;\u043b\u043e\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u0438\u043b\u0438 \u0443\u0434\u0430\u043b\u0435\u043d\u043d\u044b\u0435 (HDFS, S3) \u043f\u0443\u0442\u0438 \u0434\u043e \u0444\u0430\u0439\u043b\u043e\u0432 \u0441 \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u0435\u0439 \u043f\u0430\u0439\u043f\u043b\u0430\u0439\u043d\u0430 (\u0440\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u044b \u0437\u0430\u043f\u044f\u0442\u044b\u043c\u0438)&gt;\"\n\n# \u041f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 \u0443\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u0444\u0430\u0439\u043b\u044b \u0441\u043d\u0430\u0447\u0430\u043b\u0430 \u0431\u0443\u0434\u0443\u0442 \u0437\u0430\u0433\u0440\u0443\u0436\u0435\u043d\u044b \u043d\u0430 \u0434\u0440\u0430\u0439\u0432\u0435\u0440 \u0438 \u044d\u043a\u0437\u0435\u043a\u044c\u044e\u0442\u043e\u0440\u044b,\n# \u0442\u043e \u043e\u043d\u0438 \u0431\u0443\u0434\u0443\u0442 \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u044c\u0441\u044f \u0432 \u0440\u0430\u0431\u043e\u0447\u0435\u0439 \u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\u0438\u0438. \n# \u0422\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c, \u0432 \u0430\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u0430\u0445 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f \u043d\u0443\u0436\u043d\u043e \u0443\u043a\u0430\u0437\u0430\u0442\u044c \u0442\u043e\u043b\u044c\u043a\u043e \u043b\u0438\u0448\u044c \u0438\u043c\u0435\u043d\u0430 \u0444\u0430\u0439\u043b\u043e\u0432:\nexport DQ_APP_CONFIG_FILE=$(basename $DQ_APP_CONFIG)\nexport DQ_JOB_CONFIG_FILES=\"&lt;job configuration files separated by commas (only file names)&gt;\"\nexport REFERENCE_DATE=\"2023-08-01\"\n\n# \u0412\u0445\u043e\u0434\u043d\u0430\u044f \u0442\u043e\u0447\u043a\u0430 \u0434\u043b\u044f \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f (executable class): ru.raiffeisen.checkita.apps.batch.DataQualityBatchApp\n# \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 --name \u0432 spark-submit \u043a\u043e\u043c\u0430\u043d\u0434\u0435 \u0438\u043c\u0435\u0435\u0442 \u0431\u043e\u043b\u0435\u0435 \u0432\u044b\u0441\u043e\u043a\u0438\u0439 \u043f\u0440\u0438\u043e\u0440\u0438\u0442\u0435\u0442, \u0447\u0435\u043c\n# \u0438\u043c\u044f \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f \u0443\u043a\u0430\u0437\u0430\u043d\u043d\u043e\u0435 \u0432 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430\u0445 `application.conf`.\n\nspark-submit\\\n   --class ru.raiffeisen.checkita.apps.batch.DataQualityBatchApp \\\n   --name \"Checkita Data Quality\" \\\n   --master yarn \\\n   --deploy-mode cluster \\\n   --num-executors 1 \\\n   --executor-memory 2g \\\n   --executor-cores 4 \\\n   --driver-memory 2g \\\n   --files \"$DQ_APP_CONFIG,$DQ_DQ_JOB_CONFIGS\" \\\n   --conf \"spark.executor.memoryOverhead=2g\" \\\n   --conf \"spark.driver.memoryOverhead=2g\" \\\n   --conf \"spark.driver.maxResultSize=4g\" \\\n   $DQ_APPLICATION \\\n   -a $DQ_APP_CONFIG_FILE \\\n   -j $DQ_JOB_CONFIG_FILES \\\n   -d $REFERENCE_DATE \\\n   -e \"storage_db_user=some_db_user,storage_db_password=some_db_password\"\n</code></pre>"},{"location":"ru/01-application-setup/03-ResultsStorage/","title":"\u0425\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0435 \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432","text":"<p>\u0414\u043b\u044f \u0442\u043e\u0433\u043e, \u0447\u0442\u043e\u0431\u044b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0432\u0441\u0435 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u0438 \u0444\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a\u0430, \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0441\u043e\u0437\u0434\u0430\u0442\u044c \u0438 \u043d\u0430\u0441\u0442\u0440\u043e\u0438\u0442\u044c \u0445\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432. Checkita \u0444\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a \u043c\u043e\u0436\u0435\u0442 \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0441 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u043c\u0438 RDBMS \u0434\u043b\u044f \u0445\u0440\u0430\u043d\u0435\u043d\u0438\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432. \u041f\u043e\u043c\u0438\u043c\u043e \u044d\u0442\u043e\u0433\u043e, Hive \u043c\u043e\u0436\u0435\u0442  \u0431\u044b\u0442\u044c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d \u043a\u0430\u043a \u0445\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432, \u0442\u0430\u043a \u0436\u0435 \u043a\u0430\u043a \u0438 \u043e\u0431\u044b\u0447\u043d\u043e\u0435 \u0444\u0430\u0439\u043b\u043e\u0432\u043e\u0435 \u0445\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0435.</p> <p>\u041f\u043e\u043b\u043d\u044b\u0439 \u0441\u043f\u0438\u0441\u043e\u043a \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u0442\u0438\u043f\u043e\u0432 \u0445\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u0434\u0430\u043d \u043d\u0438\u0436\u0435:</p> <ul> <li><code>PostgreSQL</code> (v.9.3 \u0438 \u0432\u044b\u0448\u0435) - \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442\u0441\u044f \u0434\u043b\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0445\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432.</li> <li><code>Oracle</code></li> <li><code>MySQL</code></li> <li><code>Microsoft SQL Server</code></li> <li><code>SQLite</code></li> <li><code>H2</code></li> <li><code>Hive</code></li> <li><code>File</code> (\u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\u0438\u044f \u0432 \u043b\u043e\u043a\u0430\u043b\u044c\u043d\u043e\u0439 \u0444\u0430\u0439\u043b\u043e\u0432\u043e\u0439 \u0441\u0438\u0441\u0442\u0435\u043c\u0435 \u0438\u043b\u0438 \u0443\u0434\u0430\u043b\u0435\u043d\u043d\u043e\u0439 (HDFS, S3))</li> </ul> <p>Checkita \u0444\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u0438\u0432\u0430\u0435\u0442 \u044d\u0432\u043e\u043b\u044e\u0446\u0438\u044e \u0441\u0445\u0435\u043c\u044b \u0445\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432. \u0414\u043b\u044f \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f \u043c\u0438\u0433\u0440\u0430\u0446\u0438\u0439 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f Flyway. \u0422\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c, \u0435\u0441\u043b\u0438 \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0445\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u0432\u044b\u0431\u0440\u0430\u043d\u0430 \u043e\u0434\u043d\u0430 \u0438\u0437 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u0438\u0432\u0430\u0435\u043c\u044b\u0445  RDBMS, \u0442\u043e \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e \u043f\u0440\u043e\u0432\u0435\u0441\u0442\u0438 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0443 \u0435\u0433\u043e \u0441\u0445\u0435\u043c\u044b \u043f\u0440\u0438 \u043f\u0435\u0440\u0432\u043e\u043c \u0437\u0430\u043f\u0443\u0441\u043a\u0435 Data Quality \u043f\u0430\u0439\u043f\u043b\u0430\u0439\u043d\u0430, \u0443\u043a\u0430\u0437\u0430\u0432 \u0430\u0440\u0433\u0443\u043c\u0435\u043d\u0442 <code>-m</code> \u043f\u0440\u0438 \u0441\u0442\u0430\u0440\u0442\u0435. \u041f\u043e\u0434\u0440\u043e\u0431\u043d\u0435\u0435 \u043e \u0442\u043e\u043c, \u043a\u0430\u043a \u0437\u0430\u043f\u0443\u0441\u043a\u0430\u0442\u044c \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f Checkita, \u0441\u043c. \u0433\u043b\u0430\u0432\u0443 \u0417\u0430\u043f\u0443\u0441\u043a \u041f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0439 Data Quality.</p> <p>\u0412\u0410\u0416\u041d\u041e: \u041c\u0438\u0433\u0440\u0430\u0446\u0438\u0438 Flyway \u043e\u0431\u044b\u0447\u043d\u043e \u0437\u0430\u043f\u0443\u0441\u043a\u0430\u044e\u0442\u0441\u044f \u043b\u0438\u0431\u043e \u0432 \u043f\u0443\u0441\u0442\u043e\u0439 \u0431\u0430\u0437\u0435/\u0441\u0445\u0435\u043c\u0435, \u043b\u0438\u0431\u043e \u0432 \u0442\u043e\u0439, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0443\u0436\u0435 \u0431\u044b\u043b\u0430 \u043f\u0440\u043e\u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u0430 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e Flyway. \u0412 Checkita \u0444\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a\u0435 \u0442\u0430\u043a\u0436\u0435 \u043c\u043e\u0436\u043d\u043e \u0437\u0430\u043f\u0443\u0441\u043a\u0430\u0442\u044c \u043c\u0438\u0433\u0440\u0430\u0446\u0438\u0438 \u0432 \u043d\u0435\u043f\u0443\u0441\u0442\u043e\u0439 \u0431\u0430\u0437\u0435/\u0441\u0445\u0435\u043c\u0435. \u0412 \u044d\u0442\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0435,  \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044e \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0443\u0431\u0435\u0434\u0438\u0442\u044c\u0441\u044f, \u0447\u0442\u043e \u0432 \u0431\u0430\u0437\u0435/\u0441\u0445\u0435\u043c\u0435 \u043d\u0435\u0442 \u043a\u043e\u043d\u0444\u043b\u0438\u043a\u0442\u0443\u044e\u0449\u0438\u0445 \u0438\u043c\u0435\u043d \u0442\u0430\u0431\u043b\u0438\u0446.</p> <p>\u0415\u0441\u043b\u0438 \u0432\u044b\u0431\u0440\u0430\u043d <code>File</code> \u0442\u0438\u043f \u0445\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432, \u0442\u043e \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u0438\u0442\u044c \u043f\u0443\u0442\u044c \u0434\u043e \u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\u0438\u0438/\u0431\u0430\u043a\u0435\u0442\u0430, \u0433\u0434\u0435 \u0431\u0443\u0434\u0443\u0442 \u0445\u0440\u0430\u043d\u0438\u0442\u044c\u0441\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u044e\u0442\u0441\u044f \u043a\u0430\u043a <code>.parquet</code> \u0444\u0430\u0439\u043b\u044b \u0441 \u0442\u0430\u043a\u043e\u0439 \u0436\u0435 \u0441\u0445\u0435\u043c\u043e\u0439 \u043a\u0430\u043a \u0438 \u0432 \u0441\u043b\u0443\u0447\u0430\u0435 \u0445\u0440\u0430\u043d\u0435\u043d\u0438\u044f \u0438\u0445 \u0432 RDBMS. \u0414\u043b\u044f \u0444\u0430\u0439\u043b\u043e\u0432\u043e\u0433\u043e \u0445\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u043d\u0435 \u043f\u0440\u0435\u0434\u0443\u0441\u043c\u043e\u0442\u0440\u0435\u043d\u044b \u043c\u0435\u0445\u0430\u043d\u0438\u0437\u043c\u044b \u044d\u0432\u043e\u043b\u044e\u0446\u0438\u0438 \u0441\u0445\u0435\u043c\u044b. \u041f\u043e\u044d\u0442\u043e\u043c\u0443, \u0435\u0441\u043b\u0438 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u0438\u0437\u043c\u0435\u043d\u0438\u0442\u0441\u044f \u0432 \u0431\u0443\u0434\u0443\u0449\u0435\u043c, \u0442\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044e \u0431\u0443\u0434\u0435\u0442 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0441\u0430\u043c\u043e\u0441\u0442\u043e\u044f\u0442\u0435\u043b\u044c\u043d\u043e \u043e\u0431\u043d\u043e\u0432\u0438\u0442\u044c \u0441\u0445\u0435\u043c\u044b \u0432 \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430\u0445.</p> <p>\u0412\u0410\u0416\u041d\u041e: \u041f\u0440\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0438 \u0444\u0430\u0439\u043b\u043e\u0432\u043e\u0433\u043e \u0445\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0430, \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u043d\u0435 \u043f\u0430\u0440\u0442\u0438\u0446\u0438\u043e\u043d\u0438\u0440\u0443\u044e\u0442\u0441\u044f \u043d\u0438 \u043f\u043e \u043e\u0434\u043d\u043e\u043c\u0443 \u0438\u0437 \u043f\u043e\u043b\u0435\u0439. \u0422\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c, \u043a\u0430\u0436\u0434\u044b\u0439 Data Quality \u043f\u0430\u0439\u043f\u043b\u0430\u0439\u043d \u043f\u0440\u0438 \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u0438 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u0431\u0443\u0434\u0435\u0442 \u0447\u0438\u0442\u0430\u0442\u044c \u0444\u0430\u0439\u043b\u044b \u0446\u0435\u043b\u0438\u043a\u043e\u043c \u0438 \u0438\u0445 \u043f\u0435\u0440\u0435\u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0442\u044c. \u0412\u0432\u0438\u0434\u0443 \u044d\u0442\u0438\u0445 \u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e\u0441\u0442\u0435\u0439, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u044d\u0442\u043e\u0433\u043e \u0442\u0438\u043f\u0430 \u0445\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0430 \u0432 \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u043e\u0432\u043e\u0439 \u0441\u0440\u0435\u0434\u0435 \u043d\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442\u0441\u044f.</p> <p>\u0414\u043b\u044f <code>Hive</code> \u0442\u0438\u043f\u0430 \u0445\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432, \u043c\u0435\u0445\u0430\u043d\u0438\u0437\u043c\u044b \u044d\u0432\u043e\u043b\u044e\u0446\u0438\u0438 \u0441\u0445\u0435\u043c\u044b \u0442\u0430\u043a\u0436\u0435 \u043d\u0435\u0434\u043e\u0441\u0442\u0443\u043f\u043d\u044b. \u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044e \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0441\u0430\u043c\u043e\u0441\u0442\u043e\u044f\u0442\u0435\u043b\u044c\u043d\u043e \u0441\u043e\u0437\u0434\u0430\u0442\u044c \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0435 Hive-\u0442\u0430\u0431\u043b\u0438\u0446\u044b. DDL \u0441\u043a\u0440\u0438\u043f\u0442\u044b \u0438\u0437 \u0433\u043b\u0430\u0432\u044b  \u0421\u043a\u0440\u0438\u043f\u0442\u044b \u0434\u043b\u044f \u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438 \u0425\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0430 \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u0432 Hive.</p> <p>\u0412\u0410\u0416\u041d\u041e: \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u0434\u043e\u043b\u0436\u043d\u044b \u0431\u044b\u0442\u044c \u043f\u043e\u0437\u0438\u0446\u0438\u043e\u043d\u0438\u0440\u043e\u0432\u0430\u043d\u044b \u043f\u043e <code>jov_id</code>. \u0418\u0434\u0435\u043d\u0442\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 \u043f\u0430\u0439\u043f\u043b\u0430\u0439\u043d\u0430 \u0432\u044b\u0431\u0440\u0430\u043d \u043a\u0430\u043a \u043a\u043e\u043b\u043e\u043d\u043a\u0430 \u043f\u043e\u0437\u0438\u0446\u0438\u043e\u043d\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0434\u043b\u044f \u0442\u043e\u0433\u043e, \u0447\u0442\u043e\u0431\u044b \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0438\u0442\u044c \u0431\u043e\u043b\u0435\u0435 \u0431\u044b\u0441\u0442\u0440\u043e\u0435 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u0432\u043e \u0432\u0440\u0435\u043c\u044f \u0440\u0430\u0441\u0447\u0435\u0442\u043e\u0432 \u0442\u0440\u0435\u043d\u0434\u043e\u0432\u044b\u0445 \u043f\u0440\u043e\u0432\u0435\u0440\u043e\u043a (\u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0442\u0441\u044f \u0434\u043b\u044f \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u044f \u0430\u043d\u043e\u043c\u0430\u043b\u0438\u0439 \u0432 \u0434\u0430\u043d\u043d\u044b\u0445). <code>Hive</code> \u0445\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u0431\u044b\u0441\u0442\u0440\u0435\u0435, \u0447\u0435\u043c <code>File</code> \u0445\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0435, \u0442.\u043a. \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u0430\u0440\u0442\u0438\u0446\u0438\u044f, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u0438\u0434\u0435\u043d\u0442\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u0443 \u0442\u0435\u043a\u0443\u0449\u0435\u0433\u043e \u043f\u0430\u0439\u043f\u043b\u0430\u0439\u043d\u0430 \u0447\u0438\u0442\u0430\u0435\u0442\u0441\u044f \u0438 \u043f\u0435\u0440\u0435\u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u0442\u0441\u044f. \u0422\u0435\u043c \u043d\u0435 \u043c\u0435\u043d\u0435\u0435, \u044d\u0442\u043e\u0442 \u0442\u0438\u043f \u0445\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0430 \u0442\u0430\u043a\u0436\u0435 \u043d\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442\u0441\u044f \u0434\u043b\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0432 \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u043e\u0432\u044b\u0445 \u0441\u0440\u0435\u0434\u0430\u0445, \u0433\u0434\u0435 \u0431\u0443\u0434\u0435\u0442 \u0437\u0430\u043f\u0443\u0441\u043a\u0430\u0442\u044c\u0441\u044f \u0431\u043e\u043b\u044c\u0448\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0430\u0439\u043f\u043b\u0430\u0439\u043d\u043e\u0432.</p>"},{"location":"ru/01-application-setup/03-ResultsStorage/#_2","title":"\u0422\u0438\u043f\u044b \u0438 \u0421\u0445\u0435\u043c\u044b \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432","text":"<p>Checkita \u0444\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u0442 \u0447\u0435\u0442\u044b\u0440\u0435 \u0442\u0438\u043f\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432:</p> <ul> <li>\u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u0440\u0430\u0441\u0447\u0435\u0442\u0430 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u043d\u044b\u0445 \u043c\u0435\u0442\u0440\u0438\u043a;</li> <li>\u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u0440\u0430\u0441\u0447\u0435\u0442\u0430 \u043a\u043e\u043c\u043f\u043e\u0437\u0438\u0446\u0438\u043e\u043d\u043d\u044b\u0445 \u043c\u0435\u0442\u0440\u0438\u043a;</li> <li>\u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u0437\u0430\u0433\u0440\u0443\u0437\u043e\u0447\u043d\u044b\u0445 \u043f\u0440\u043e\u0432\u0435\u0440\u043e\u043a (\u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u043c\u0435\u0442\u0430\u0434\u0430\u043d\u043d\u044b\u0445 \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u0430 \u043f\u0435\u0440\u0435\u0434 \u0435\u0433\u043e \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u043e\u0439);</li> <li>\u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u043e\u0441\u043d\u043e\u0432\u043d\u044b\u0445 \u043f\u0440\u043e\u0432\u0435\u0440\u043e\u043a (\u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u043d\u0430\u0434 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u043d\u044b\u043c\u0438 \u043c\u0435\u0442\u0440\u0438\u043a\u0430\u043c\u0438).</li> </ul> <p>\u0421\u0445\u0435\u043c\u044b \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u0442\u0438\u043f\u043e\u0432 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u044b \u043d\u0438\u0436\u0435:</p>"},{"location":"ru/01-application-setup/03-ResultsStorage/#regular-metrics-results-schema","title":"Regular Metrics Results Schema","text":"<ul> <li>\u041f\u0435\u0440\u0432\u0438\u0447\u043d\u044b\u0439 \u043a\u043b\u044e\u0447: <code>(job_id, metric_id, reference_date)</code></li> <li><code>source_id</code> &amp; <code>column_names</code> \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442 \u0441\u0442\u0440\u043e\u043a\u043e\u0432\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0441\u043f\u0438\u0441\u043a\u043e\u0432 \u0432 \u0444\u043e\u0440\u043c\u0430\u0442\u0435: <code>'[val1,val2,val3]'</code>.</li> <li><code>params</code> \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442 \u0441\u043e\u0431\u043e\u0439 JSON \u0441\u0442\u0440\u043e\u043a\u0443.</li> </ul> Column Name Column Type Constraint job_id STRING NOT NULL metric_id STRING NOT NULL metric_name STRING NOT NULL description STRING source_id STRING NOT NULL column_names STRING params STRING result DOUBLE NOT NULL additional_result STRING reference_date TIMESTAMP NOT NULL execution_date TIMESTAMP NOT NULL"},{"location":"ru/01-application-setup/03-ResultsStorage/#composed-metrics-results-schema","title":"Composed Metrics Results Schema","text":"<ul> <li>\u041f\u0435\u0440\u0432\u0438\u0447\u043d\u044b\u0439 \u043a\u043b\u044e\u0447: <code>(job_id, metric_id, reference_date)</code></li> <li><code>source_id</code> \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u0441\u0442\u0440\u043e\u043a\u043e\u0432\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0441\u043f\u0438\u0441\u043a\u0430 \u0432 \u0444\u043e\u0440\u043c\u0430\u0442\u0435: <code>'[val1,val2,val3]'</code>.</li> </ul> Column Name Column Type Constraint job_id STRING NOT NULL metric_id STRING NOT NULL metric_name STRING NOT NULL description STRING source_id STRING NOT NULL formula STRING NOT NULL result DOUBLE NOT NULL additional_result STRING reference_date TIMESTAMP NOT NULL execution_date TIMESTAMP NOT NULL"},{"location":"ru/01-application-setup/03-ResultsStorage/#load-checks-results-schema","title":"Load Checks Results Schema","text":"<ul> <li>\u041f\u0435\u0440\u0432\u0438\u0447\u043d\u044b\u0439 \u043a\u043b\u044e\u0447: <code>(job_id, check_id, reference_date)</code></li> <li><code>source_id</code> \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u0441\u0442\u0440\u043e\u043a\u043e\u0432\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0441\u043f\u0438\u0441\u043a\u0430 \u0432 \u0444\u043e\u0440\u043c\u0430\u0442\u0435: <code>'[val1,val2,val3]'</code>.</li> </ul> Column Name Column Type Constraint job_id STRING NOT NULL check_id STRING NOT NULL check_name STRING NOT NULL source_id STRING NOT NULL expected STRING NOT NULL status STRING NOT NULL message STRING reference_date TIMESTAMP NOT NULL execution_date TIMESTAMP NOT NULL"},{"location":"ru/01-application-setup/03-ResultsStorage/#checks-results-schema","title":"Checks Results Schema","text":"<ul> <li>\u041f\u0435\u0440\u0432\u0438\u0447\u043d\u044b\u0439 \u043a\u043b\u044e\u0447: <code>(job_id, check_id, reference_date)</code></li> <li><code>source_id</code> \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u0441\u0442\u0440\u043e\u043a\u043e\u0432\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0441\u043f\u0438\u0441\u043a\u0430 \u0432 \u0444\u043e\u0440\u043c\u0430\u0442\u0435: <code>'[val1,val2,val3]'</code>.</li> </ul> Column Name Column Type Constraint job_id STRING NOT NULL check_id STRING NOT NULL check_name STRING NOT NULL description STRING source_id STRING NOT NULL base_metric STRING NOT NULL compared_metric STRING compared_threshold DOUBLE lower_bound DOUBLE upper_bound DOUBLE status STRING NOT NULL message STRING reference_date TIMESTAMP NOT NULL execution_date TIMESTAMP NOT NULL"},{"location":"ru/01-application-setup/03-ResultsStorage/#hive","title":"\u0421\u043a\u0440\u0438\u043f\u0442\u044b \u0434\u043b\u044f \u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438 \u0425\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0430 \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u0432 Hive","text":"<p>\u041d\u0438\u0436\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u044b HiveQL \u0441\u043a\u0440\u0438\u043f\u0442\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u044b \u0434\u043b\u044f \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0445\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u0432 Hive:</p> <pre><code>-- \u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0437\u0430\u043c\u0435\u043d\u0438\u0442\u044c &lt;schema_name&gt; \u0438 &lt;schema_dir&gt; \u043d\u0430 \u0444\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0438\u043c\u044f \u0441\u0445\u0435\u043c\u044b \u0438 \u043f\u0443\u0442\u044c \u0434\u043e \u043d\u0435\u0435.\nset hivevar:schema_name=&lt;schema_name&gt;;\nset hivevar:schema_dir=&lt;schema_path&gt;;\n\nCREATE SCHEMA IF NOT EXISTS ${schema_name};\n\nDROP TABLE IF EXISTS ${schema_name}.results_metric_regular;\nCREATE EXTERNAL TABLE ${schema_name}.results_metric_regular\n(\n    job_id            STRING COMMENT '',\n    metric_id         STRING COMMENT '',\n    metric_name       STRING COMMENT '',\n    description       STRING COMMENT '',\n    source_id         STRING COMMENT '',\n    column_names      STRING COMMENT '',\n    params            STRING COMMENT '',\n    result            DOUBLE COMMENT '',\n    additional_result STRING COMMENT '',\n    reference_date    TIMESTAMP COMMENT '',\n    execution_date    TIMESTAMP COMMENT ''\n)\nCOMMENT 'Data Quality Regular Metrics Results'\nPARTITIONED BY (job_id STRING)\nSTORED AS PARQUET\nLOCATION '${schema_dir}/results_metric_regular';\n\nDROP TABLE IF EXISTS ${schema_name}.results_metric_composed;\nCREATE EXTERNAL TABLE ${schema_name}.results_metric_composed\n(\n    job_id            STRING COMMENT '',\n    metric_id         STRING COMMENT '',\n    metric_name       STRING COMMENT '',\n    description       STRING COMMENT '',\n    source_id         STRING COMMENT '',\n    formula           STRING COMMENT '',\n    result            DOUBLE COMMENT '',\n    additional_result STRING COMMENT '',\n    reference_date    TIMESTAMP COMMENT '',\n    execution_date    TIMESTAMP COMMENT ''\n)\nCOMMENT 'Data Quality Composed Metrics Results'\nPARTITIONED BY (job_id STRING)\nSTORED AS PARQUET\nLOCATION '${schema_dir}/results_metric_composed';\n\nDROP TABLE IF EXISTS ${schema_name}.results_check_load;\nCREATE EXTERNAL TABLE ${schema_name}.results_check_load\n(\n    job_id         STRING COMMENT '',\n    check_id       STRING COMMENT '',\n    check_name     STRING COMMENT '',\n    source_id      STRING COMMENT '',\n    expected       STRING COMMENT '',\n    status         STRING COMMENT '',\n    message        STRING COMMENT '',\n    reference_date TIMESTAMP COMMENT '',\n    execution_date TIMESTAMP COMMENT ''\n)\nCOMMENT 'Data Quality Load Checks Results'\nPARTITIONED BY (job_id STRING)\nSTORED AS PARQUET\nLOCATION '${schema_dir}/results_check_load';\n\nDROP TABLE IF EXISTS ${schema_name}.results_check;\nCREATE EXTERNAL TABLE ${schema_name}.results_check\n(\n    job_id             STRING COMMENT '',\n    check_id           STRING COMMENT '',\n    check_name         STRING COMMENT '',\n    description        STRING COMMENT '',\n    source_id          STRING COMMENT '',\n    base_metric        STRING COMMENT '',\n    compared_metric    STRING COMMENT '',\n    compared_threshold DOUBLE COMMENT '',\n    lower_bound        DOUBLE COMMENT '',\n    upper_bound        DOUBLE COMMENT '',\n    status             STRING COMMENT '',\n    message            STRING COMMENT '',\n    reference_date     TIMESTAMP COMMENT '',\n    execution_date     TIMESTAMP COMMENT ''\n)\nCOMMENT 'Data Quality Checks Results'\nPARTITIONED BY (job_id STRING)\nSTORED AS PARQUET\nLOCATION '${schema_dir}/results_check';\n</code></pre>"},{"location":"ru/02-general-concepts/","title":"\u041e\u0441\u043d\u043e\u0432\u043d\u044b\u0435 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u044b","text":"<p>\u0412 \u0434\u0430\u043d\u043d\u043e\u043c \u0440\u0430\u0437\u0434\u0435\u043b\u0435 \u043e\u0431\u044a\u044f\u0441\u043d\u044f\u044e\u0442\u0441\u044f \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0435 \u0430\u0441\u043f\u0435\u043a\u0442\u044b \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u0444\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a\u043e\u043c Checkita Data Quality.</p>"},{"location":"ru/02-general-concepts/01-WorkingWithDateTime/","title":"\u0420\u0430\u0431\u043e\u0442\u0430 \u0441 \u0414\u0430\u0442\u0430\u043c\u0438","text":"<p>\u0417\u0434\u0435\u0441\u044c \u0438 \u0434\u0430\u043b\u0435\u0435 \u043f\u043e\u0434 \u0434\u0430\u0442\u043e\u0439 \u043f\u043e\u043d\u0438\u043c\u0430\u0435\u0442\u0441\u044f DateTime \u043e\u0431\u044a\u0435\u043a\u0442, \u0445\u0440\u0430\u043d\u044f\u0449\u0438\u0439 \u043a\u0430\u043a \u0434\u0430\u0442\u0443, \u0442\u0430\u043a \u0438 \u0432\u0440\u0435\u043c\u044f</p> <p>\u0412 \u0444\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a\u0435 Checkita \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u0435\u0442 \u0434\u0432\u0430 \u043e\u0441\u043d\u043e\u0432\u043d\u044b\u0445 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0434\u0430\u0442\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0442\u0441\u044f \u0434\u043b\u044f \u0438\u0434\u0435\u043d\u0442\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u0437\u0430\u043f\u0443\u0441\u043a\u043e\u0432 Data Quality \u043f\u0430\u0439\u043f\u043b\u0430\u0439\u043d\u043e\u0432:</p> <ul> <li><code>referenceDate</code> - \u0434\u0430\u0442\u0430, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442 \u043d\u0430 \u0442\u043e, \u0437\u0430 \u043a\u0430\u043a\u043e\u0439 \u043f\u0435\u0440\u0438\u043e\u0434 \u0437\u0430\u043f\u0443\u0441\u043a\u0430\u0435\u0442\u0441\u044f \u0438 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442\u0441\u044f \u0443\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 Data Quality \u043f\u0430\u0439\u043f\u043b\u0430\u0439\u043d.</li> <li><code>executionDate</code> - \u0434\u0430\u0442\u0430, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0445\u0440\u0430\u043d\u0438\u0442 \u0444\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0432\u0440\u0435\u043c\u044f \u0437\u0430\u043f\u0443\u0441\u043a\u0430 Data Quality \u043f\u0430\u0439\u043f\u043b\u0430\u0439\u043d\u0430.</li> </ul> <p>\u0422\u0438\u043f\u043e\u0432\u043e\u0439 \u043f\u0440\u0438\u043c\u0435\u0440: \u043c\u044b \u0437\u0430\u043f\u0443\u0441\u043a\u0430\u0435\u043c \u043a\u0430\u043a\u043e\u0439-\u043b\u0438\u0431\u043e ETL \u043f\u0430\u0439\u043f\u043b\u0430\u0439\u043d (\u0442\u0430\u043a\u0436\u0435 \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u0437\u0430\u0434\u0430\u0447\u0443 \u043f\u043e \u0440\u0430\u0441\u0447\u0435\u0442\u0443 \u043c\u0435\u0442\u0440\u0438\u043a \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u0434\u0430\u043d\u043d\u044b\u0445) \u043f\u043e\u0441\u043b\u0435 \u0437\u0430\u043a\u0440\u044b\u0442\u0438\u044f \u0431\u0438\u0437\u043d\u0435\u0441 \u0434\u043d\u044f, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 \u0432 \u043f\u043e\u043b\u043d\u043e\u0447\u044c. \u0422\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c, <code>referenceDate</code> \u0431\u0443\u0434\u0435\u0442 \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u043d\u0430 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0438\u0439 \u0434\u0435\u043d\u044c, \u0437\u0430 \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043c\u044b \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u043c ETL, \u0430 <code>executionDate</code> \u0431\u0443\u0434\u0435\u0442 \u0445\u0440\u0430\u043d\u0438\u0442\u044c \u0442\u0435\u043a\u0443\u0449\u0443\u044e \u0434\u0430\u0442\u0443 - \u0434\u0430\u0442\u0443 \u0444\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0437\u0430\u043f\u0443\u0441\u043a\u0430 Data Quality \u043f\u0430\u0439\u043f\u043b\u0430\u0439\u043d\u0430. \u0412\u0435\u0440\u043e\u044f\u0442\u043d\u043e, \u0443 \u043d\u0430\u0441 \u043f\u043e\u044f\u0432\u0438\u0442\u0441\u044f \u043f\u043e\u0442\u0440\u0435\u0431\u043d\u043e\u0441\u0442\u044c \u0432 \u0442\u043e\u043c, \u0447\u0442\u043e\u0431\u044b \u0441\u0442\u0440\u043e\u043a\u043e\u0432\u044b\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u044d\u0442\u0438\u0445 \u0434\u0430\u0442 \u043e\u0442\u043b\u0438\u0447\u0430\u043b\u0438\u0441\u044c. \u0418 \u0444\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a Checkita \u0434\u0430\u0435\u0442 \u043d\u0430\u043c \u0442\u0430\u043a\u0443\u044e \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c, \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u044f \u043d\u0430\u0441\u0442\u0440\u0430\u0438\u0432\u0430\u0442\u044c \u0438\u043d\u0434\u0438\u0432\u0438\u0434\u0443\u0430\u043b\u044c\u043d\u044b\u0435 \u0444\u043e\u0440\u043c\u0430\u0442\u044b \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u0438\u0437 \u044d\u0442\u0438\u0445 \u0434\u0430\u0442 \u0432 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430\u0445 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f.</p> <p>\u0422\u0430\u043a \u043a\u0430\u043a <code>referenceDate</code> \u043c\u043e\u0436\u0435\u0442 \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u043d\u0430 \u043f\u0440\u043e\u0448\u0435\u0434\u0448\u0438\u0435 \u0434\u0430\u0442\u044b, \u0442\u043e \u0435\u0435 \u043c\u043e\u0436\u043d\u043e \u044f\u0432\u043d\u043e \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u0432 \u043f\u0440\u0438 \u0441\u0442\u0430\u0440\u0442\u0435 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f. \u0415\u0441\u043b\u0438 \u044d\u0442\u0430 \u0434\u0430\u043d\u043d\u0430 \u043d\u0435 \u0443\u043a\u0430\u0437\u0430\u043d\u0430 \u044f\u0432\u043d\u043e, \u0442\u043e \u043e\u043d\u0430 \u0431\u0443\u0434\u0435\u0442 \u0442\u0430\u043a\u0436\u0435 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c \u0444\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0434\u0430\u0442\u0435 \u0441\u0442\u0430\u0440\u0442\u0430 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f. \u0421\u043c. \u0433\u043b\u0430\u0432\u0443 \u0417\u0430\u043f\u0443\u0441\u043a \u041f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0439 Data Quality \u0434\u043b\u044f \u0431\u043e\u043b\u0435\u0435 \u043f\u043e\u0434\u0440\u043e\u0431\u043d\u043e\u0439 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u043e\u0431 \u0430\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u0430\u0445, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u044b\u0445 \u043f\u0440\u0438 \u0437\u0430\u043f\u0443\u0441\u043a\u0435 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0439 Data Quality.</p> <p>\u041e\u0431\u0435 \u044d\u0442\u0438 \u0434\u0430\u0442\u044b \u0448\u0438\u0440\u043e\u043a\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0442\u0441\u044f \u0432\u043d\u0443\u0442\u0440\u0438 \u0444\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a\u0430. \u041f\u043e\u044d\u0442\u043e\u043c\u0443, \u0432 \u043b\u044e\u0431\u044b\u0445 \u0441\u0438\u0442\u0443\u0430\u0446\u0438\u044f\u0445, \u043a\u043e\u0433\u0434\u0430 \u043d\u0430\u043c \u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f \u0438\u0445 \u0441\u0442\u0440\u043e\u043a\u043e\u0432\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435, \u043e\u043d\u043e \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u0442\u0441\u044f \u0432 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0438\u0438 \u0441 \u0442\u0435\u043c \u0444\u043e\u0440\u043c\u0430\u0442\u043e\u043c, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0443\u043a\u0430\u0437\u0430\u043d \u0432 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430\u0445 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f.</p> <p>\u0422\u0430\u043a\u0436\u0435 \u043d\u0443\u0436\u043d\u043e \u043e\u0442\u043c\u0435\u0442\u0438\u0442\u044c, \u0447\u0442\u043e \u0441\u0442\u0440\u043e\u043a\u043e\u0432\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0444\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u0441 \u0443\u0447\u0435\u0442\u043e\u043c \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 \u0437\u043e\u043d\u044b, \u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u0437\u0430\u043f\u0443\u0441\u043a\u0430\u0435\u0442\u0441\u044f \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435. \u0412\u0440\u0435\u043c\u0435\u043d\u043d\u0430 \u0437\u043e\u043d\u0430 \u0442\u0430\u043a\u0436\u0435 \u0437\u0430\u0434\u0430\u0435\u0442\u0441\u044f \u0432 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430\u0445 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f. \u041f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0437\u043e\u043d\u0430 <code>UTC</code>.</p> <p>\u0412 \u0434\u043e\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u0435, \u043d\u043e \u043d\u0435 \u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u043e: \u043c\u044b \u0441\u043e\u0437\u043d\u0430\u0442\u0435\u043b\u044c\u043d\u043e \u0438\u0437\u0431\u0435\u0433\u0430\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0441\u0442\u0440\u043e\u043a\u043e\u0432\u043e\u0433\u043e \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0434\u0430\u0442 \u043f\u0440\u0438 \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u0438 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u0432 \u0431\u0430\u0437\u0443 \u0434\u0430\u043d\u043d\u044b\u0445. \u0412 \u044d\u0442\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0435 \u0434\u0430\u0442\u044b \u043a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u0443\u044e\u0442\u0441\u044f \u0432 \u0442\u0438\u043f Timestamp \u0438 \u043f\u0440\u0438\u0432\u043e\u0434\u044f\u0442\u0441\u044f \u043a \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 \u0437\u043e\u043d\u0435 <code>UTC</code>. \u0422\u0430\u043a\u043e\u0439 \u043f\u043e\u0434\u0445\u043e\u0434 \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u043d\u0430\u0434\u0435\u0436\u043d\u043e \u0441\u0442\u0440\u043e\u0438\u0442\u044c \u0437\u0430\u043f\u0440\u043e\u0441\u044b \u043a \u0445\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0443 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0435 \u0431\u0443\u0434\u0443\u0442 \u0437\u0430\u0432\u0438\u0441\u0435\u0442\u044c \u043e\u0442 \u043d\u0430\u0441\u0442\u0440\u043e\u0435\u043a \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0434\u0430\u0442. \u0421\u043c. \u0433\u043b\u0430\u0432\u0443 \u0425\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0435 \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u0434\u043b\u044f \u0431\u043e\u043b\u0435\u0435 \u043f\u043e\u0434\u0440\u043e\u0431\u043d\u043e\u0439 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u043e \u0445\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432.</p> <p>\u0412\u0410\u0416\u041d\u041e: \u0424\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0441\u0442\u0440\u043e\u043a\u043e\u0432\u044b\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f <code>referenceDate</code> \u0438 <code>exectionDate</code> \u0432\u0441\u0435\u0433\u0434\u0430 \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0435 \u0444\u0430\u0439\u043b\u044b \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b. \u0414\u043b\u044f \u0431\u043e\u043b\u0435\u0435 \u043f\u043e\u0434\u0440\u043e\u0431\u043d\u043e\u0439 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u043e\u0431 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0438 \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0445  \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445, \u0441\u043c. \u0433\u043b\u0430\u0432\u0443  \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u041f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u041e\u043a\u0440\u0443\u0436\u0435\u043d\u0438\u044f \u0438 \u0414\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u041f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445.</p>"},{"location":"ru/02-general-concepts/02-EnvironmentAndExtraVariables/","title":"\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u041f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u041e\u043a\u0440\u0443\u0436\u0435\u043d\u0438\u044f \u0438 \u0414\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u041f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445","text":"<p>Hocon \u0444\u043e\u0440\u043c\u0430\u0442 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u0438\u0432\u0430\u0435\u0442\u0441\u044f \u043f\u043e\u0434\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0443 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445. \u042d\u0442\u043e\u0442 \u043c\u0435\u0445\u0430\u043d\u0438\u0437\u043c \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0431\u043e\u043b\u0435\u0435 \u0433\u0438\u0431\u043a\u043e \u0443\u043f\u0440\u0430\u0432\u043b\u044f\u0442\u044c \u043a\u0430\u043a \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430\u043c\u0438 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f, \u0442\u0430\u043a \u0438 \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u044f\u043c\u0438 Data Quality \u043f\u0430\u0439\u043f\u043b\u0430\u0439\u043d\u043e\u0432.</p> <p>\u0422\u0430\u043a, \u043a \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u043c \u0444\u0430\u0439\u043b\u0430\u043c \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0441\u0447\u0438\u0442\u044b\u0432\u0430\u044e\u0442\u0441\u044f \u0438\u0437 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u043e\u043a\u0440\u0443\u0436\u0435\u043d\u0438\u044f \u0438\u043b\u0438 \u0436\u0435 \u0437\u0430\u0434\u0430\u044e\u0442\u0441\u044f \u0432 \u044f\u0432\u043d\u043e \u0432\u0438\u0434\u0435 \u043f\u0440\u0438 \u0441\u0442\u0430\u0440\u0442\u0435 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f.</p> <p>\u0414\u043b\u044f \u0431\u043e\u043b\u0435\u0435 \u043f\u043e\u0434\u0440\u043e\u0431\u043d\u043e\u0439 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u043e \u0437\u0430\u0434\u0430\u043d\u0438\u0438 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u0432 \u044f\u0432\u043d\u043e\u043c \u0432\u0438\u0434\u0435 \u043f\u0440\u0438 \u0441\u0442\u0430\u0440\u0442\u0435 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f, \u0441\u043c. \u0433\u043b\u0430\u0432\u0443 \u0417\u0430\u043f\u0443\u0441\u043a \u041f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0439 Data Quality.</p> <p>\u0414\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u043e\u043a\u0440\u0443\u0436\u0435\u043d\u0438\u0435 (\u043c\u043e\u0436\u043d\u043e \u0442\u0430\u043a\u0436\u0435 \u0437\u0430\u0434\u0430\u0432\u0430\u0442\u044c \u0438 JVM-\u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435), \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e, \u0447\u0442\u043e\u0431\u044b \u0438\u0445 \u0438\u043c\u0435\u043d\u0430 \u0441\u043e\u0432\u043f\u0430\u0434\u0430\u043b\u0438 \u0441\u043e \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u043d\u044b\u043c \u0432\u044b\u0440\u0430\u0436\u0435\u043d\u0438\u0435\u043c: <code>^(?i)(DQ)[a-z0-9_-]+$</code>, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 <code>DQ_STORAGE_PASSOWRD</code> \u0438\u043b\u0438 <code>dqMattermostToken</code>. \u0412\u0441\u0435 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u043e\u043a\u0440\u0443\u0436\u0435\u043d\u0438\u044f, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0441\u043e\u0432\u043f\u0430\u0434\u0430\u044e\u0442 \u0441 \u044d\u0442\u0438\u043c \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u043d\u044b\u043c \u0432\u044b\u0440\u0430\u0436\u0435\u043d\u0438\u0435\u043c \u0431\u0443\u0434\u0443\u0442 \u0441\u0447\u0438\u0442\u0430\u043d\u044b \u0438 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u044b \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0435 \u0444\u0430\u0439\u043b\u044b \u0434\u043b\u044f \u043f\u043e\u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u0439 \u043f\u043e\u0434\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0438 \u0432 \u043d\u0443\u0436\u043d\u044b\u0435 \u0440\u0430\u0437\u0434\u0435\u043b\u044b. \u041f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u043a\u0430\u043a \u0432 \u0444\u0430\u0439\u043b \u0441 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430\u043c\u0438 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f, \u0442\u0430\u043a \u0438 \u0432 \u0444\u0430\u0439\u043b/\u044b \u0441 \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u0435\u0439 Data Quality \u043f\u0430\u0439\u043f\u043b\u0430\u0439\u043d\u043e\u0432.</p> <p>\u0422\u0438\u043f\u043e\u0432\u043e\u0439 \u043f\u0440\u0438\u043c\u0435\u0440 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u043e\u043a\u0440\u0443\u0436\u0435\u043d\u0438\u044f - \u044d\u0442\u043e \u043f\u043e\u0434\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430 \u0441\u0435\u043a\u0440\u0435\u0442\u043e\u0432 \u0434\u043b\u044f \u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u044f \u043a \u0432\u043d\u0435\u0448\u043d\u0438\u043c \u0441\u0438\u0441\u0442\u0435\u043c\u0430\u043c. \u0425\u0440\u0430\u043d\u0435\u043d\u0438\u0435 \u0442\u0430\u043a\u043e\u0439 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0445 \u0444\u0430\u0439\u043b\u0430\u0445 - \u044d\u0442\u043e \u043d\u0435 \u043e\u0447\u0435\u043d\u044c \u0445\u043e\u0440\u043e\u0448\u0430\u044f \u0438\u0434\u0435\u044f. \u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u0432 \u0444\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a\u0435 Checkita \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d \u043c\u0435\u0445\u0430\u043d\u0438\u0437\u043c \u0434\u043b\u044f \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0442\u0430\u043a\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0432\u043e \u0432\u0440\u0435\u043c\u044f \u0438\u0441\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f.</p> <p>\u0412\u0410\u0416\u041d\u041e \u041f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u0432 \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0435 \u0444\u0430\u0439\u043b\u044b \u0432\u043e \u0432\u0440\u0435\u043c\u044f \u0438\u0441\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f \u0438 \u043d\u0435 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u044e\u0442\u0441\u044f \u043d\u0438 \u0432 \u043a\u0430\u043a\u043e\u043c \u0432\u0438\u0434\u0435.</p>"},{"location":"ru/03-job-configuration/","title":"Job Configuration","text":"<p>tbd.</p>"},{"location":"ru/03-job-configuration/01-Connections/","title":"Connections Configuration","text":"<p>tbd</p>"},{"location":"ru/03-job-configuration/02-Schemas/","title":"Schemas Configuration","text":"<p>tbd</p>"},{"location":"ru/03-job-configuration/03-Sources/","title":"Source Configuration","text":"<p>tbd</p>"},{"location":"ru/03-job-configuration/04-VirtualSources/","title":"Virtual Sources Configuration","text":"<p>tbd</p>"},{"location":"ru/03-job-configuration/05-LoadChecks/","title":"Load Checks Configuration","text":"<p>tbd</p>"},{"location":"ru/03-job-configuration/06-Metrics/","title":"Metrics Configuration","text":"<p>tbd</p>"},{"location":"ru/03-job-configuration/07-Checks/","title":"Checks Configurations","text":"<p>tbd</p>"},{"location":"ru/03-job-configuration/08-Targets/","title":"Targets Configuration","text":"<p>tbd</p>"}]}